{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-09-20-Entity Extraction Transformers - Part-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPB3Cma8Q4xQYMDBfk6bF9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/crazycloud/dl-blog/blob/master/_notebooks/2020_09_20_Entity_Extraction_Transformers_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wLAJmgEzsWP",
        "colab_type": "text"
      },
      "source": [
        "# \"Entity Extraction (NER) - Training and Inference using Transformers - Part 2\"\n",
        "> Learn to train a NER model using Transformers Trainer Class, and to run Inference using Pipeline function\n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [nlp, token classification, ]\n",
        "- image: images/transformers-ner-part2.png\n",
        "- hide: false"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D5ZJzd5f3Q-",
        "colab_type": "text"
      },
      "source": [
        "In the Part-1, we talked about how to use the pretrained language model, Tokenizer and TokenClassification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc0pqYHCcppc",
        "colab_type": "text"
      },
      "source": [
        "## Fine Tuning Token Classification Model\n",
        "\n",
        "Steps for Finetuning TC Model\n",
        "\n",
        "**Step 1.** Dataset - Get labelled dataset for training and testing. For token classification, we need word level labels like following\n",
        "\n",
        "- `The` O  \n",
        "- `battery` Aspect\n",
        "- `of` O \n",
        "- `the` O  \n",
        "- `speaker` O \n",
        "- `is` O \n",
        "- `very` Sentiment\n",
        "- `poor` Sentiment\n",
        "\n",
        "  \n",
        "  We need train.txt, test.txt and labels.txt file to finetune the model, and to verify the performance of the model.\n",
        "\n",
        "  train.txt/test.txt - word and corresponding label per line and a blank line between each example.\n",
        "\n",
        "  labels.txt - list of unique tags that we have labelled in the train.txt and test.txt\n",
        "\n",
        "**Step 2.** If the data is in any other format, we have to either convert it into this format, or write a new Task class and implement the methods of [TokenClassificationTask](https://github.com/huggingface/transformers/blob/4f6e52574248636352a746cfe6cc0b13cf3eb7f9/examples/token-classification/tasks.py#L13)\n",
        "\n",
        "For this example, we will use the Task class NER in the [tasks.py](https://github.com/huggingface/transformers/blob/4f6e52574248636352a746cfe6cc0b13cf3eb7f9/examples/token-classification/tasks.py#L13) file. This will prepare the list of InputExample\n",
        "\n",
        "```python\n",
        "class InputExample:\n",
        "    \"\"\"\n",
        "    A single training/test example for token classification.\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        words: list. The words of the sequence.\n",
        "        labels: (Optional) list. The labels for each word of the sequence. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    \n",
        "    guid: str\n",
        "    words: List[str]\n",
        "    labels: Optional[List[str]]\n",
        "```\n",
        "\n",
        "**Step 3.** NER class extends the [TokenClassificationTask](https://github.com/huggingface/transformers/blob/4f6e52574248636352a746cfe6cc0b13cf3eb7f9/examples/token-classification/utils_ner.py#L68) class which has a method `convert_examples_to_features` to convert list of example in to input features \n",
        "\n",
        "```python\n",
        "class InputFeatures:\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    Property names are the same names as the corresponding inputs to a model.\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids: List[int]\n",
        "    attention_mask: List[int]\n",
        "    token_type_ids: Optional[List[int]] = None\n",
        "    label_ids: Optional[List[int]] = None\n",
        "\n",
        "```\n",
        "\n",
        "This method convert_examples_to_features uses the Tokenizer class to convert InputExample into InputFeature. In the Part-1 we discussed about the Tokenizer and how to prepate input for the model.\n",
        "\n",
        "**Step 4.** Convert InputFeatures into Pytorch Dataset. The utils_ner.py has a function to convert InputFeatures into Dataset required for training.\n",
        "\n",
        "**Step 5.** Call the Trainer class in run_ner.py which trains the model and evaluates the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl_4hFZqqV3k",
        "colab_type": "text"
      },
      "source": [
        "## Installation\n",
        "Install the latest transformers library "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIJca7NMqgP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install seqeval\n",
        "!pip install conllu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxVKGw5Qq0Oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e9a65ff-cfab-4c6c-944d-dad6ad37686f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXCdFgg7qQSu",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the Training Data\n",
        "\n",
        "Download the dataset from https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n",
        "\n",
        "download the file `new_dataset.csv`\n",
        "\n",
        "Let's load the dataset in pandas dataframe and look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSySLQpXcqbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "878c750f-e4b1-404a-fe8e-4e0af84a09b4"
      },
      "source": [
        "import pandas as pd\n",
        "data_path = '/content/drive/My Drive/transformers-ner/ner_dataset.csv'\n",
        "df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
        "\n",
        "# fille the empty Sentence# with the previous available value\n",
        "df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1  Sentence: 1             of   IN   O\n",
              "2  Sentence: 1  demonstrators  NNS   O\n",
              "3  Sentence: 1           have  VBP   O\n",
              "4  Sentence: 1        marched  VBN   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWU_l6_Urvmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
        "tags = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OMAT_Ngr2Y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "476016dc-dd11-4a86-e466-699a8a9eae5d"
      },
      "source": [
        "print(sentences[0])\n",
        "print(tags[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', 'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O', 'O']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAv4SRj3sLKZ",
        "colab_type": "text"
      },
      "source": [
        "### Split the dataset into Train and Test using sklearn "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMGzqCecrkvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#split into 80% train and 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, tags, test_size=0.2, random_state=42)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVq0q4ggsVwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_FILE_PATH = '/content/drive/My Drive/transformers-ner/data/train.txt'\n",
        "TEST_FILE_PATH = '/content/drive/My Drive/transformers-ner/data/test.txt'\n",
        "LABELS_FILE_PATH = '/content/drive/My Drive/transformers-ner/data/labels.txt'\n",
        "with open(TRAIN_FILE_PATH,'w') as ftrain:\n",
        "  for (k,v) in zip(X_train, y_train):\n",
        "    [ftrain.write(s+' '+t+'\\n') for s,t in zip(k,v)]\n",
        "    ftrain.write('\\n')\n",
        "\n",
        "with open(TEST_FILE_PATH,'w') as ftest:\n",
        "  for (k,v) in zip(X_test, y_test):\n",
        "    [ftest.write(s+' '+str(t)+'\\n') for s,t in zip(k,v)]\n",
        "    ftest.write('\\n')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_f9FNw8s7Gc",
        "colab_type": "text"
      },
      "source": [
        "Prepare the labels file with list of unique labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_SDFniUrRCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "85a6bcb9-3af3-424b-d1c3-462fa57d0d3e"
      },
      "source": [
        "with open(LABELS_FILE_PATH,'w') as f:\n",
        "  for tag in df['Tag'].unique():\n",
        "    print(tag)\n",
        "    f.write(str(tag)+'\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "O\n",
            "B-geo\n",
            "B-gpe\n",
            "B-per\n",
            "I-geo\n",
            "B-org\n",
            "I-org\n",
            "B-tim\n",
            "B-art\n",
            "I-art\n",
            "I-per\n",
            "I-gpe\n",
            "I-tim\n",
            "B-nat\n",
            "B-eve\n",
            "I-eve\n",
            "I-nat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElBa2Py0tJlt",
        "colab_type": "text"
      },
      "source": [
        "### Download Finetuning Code from Transformers Package\n",
        "\n",
        "Download following files from transformers github repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3vQlsZXrf_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/utils_ner.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/run_ner.py\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/tasks.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRQ4WxSYuHzm",
        "colab_type": "text"
      },
      "source": [
        "## Training Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnMmNGRmulfS",
        "colab_type": "text"
      },
      "source": [
        "The datafiles are converted into Input Example and and input features using the tasks.py NER class. The run_ner.py takes care of converting \n",
        "\n",
        "\n",
        ">Datafile -> InputExample -> InputFeature -> Dataset -> DataLoader\n",
        "\n",
        "which is required for Training Loop. We can simply run the run_ner.py file with all the required parameters. \n",
        "\n",
        "We will perform training using run_ner.py but before that let us look at how the InputExample and InputFeature looks like "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_gWNRQjtcg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tasks import NER\n",
        "task = NER()\n",
        "\n",
        "examples = task.read_examples_from_file('/content/drive/My Drive/transformers-ner/data',mode= 'train')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddcIicH_uYoU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9f64904f-18fb-4527-94f9-4ad3fee80885"
      },
      "source": [
        "examples[:2]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[InputExample(guid='train-1', words=['South', 'Korea', \"'s\", 'government', 'Tuesday', 'also', 'unveiled', 'a', 'so-called', 'Green', 'New', 'Job', 'Creation', 'Plan', ',', 'expected', 'to', 'create', '9,60,000', 'new', 'jobs', '.'], labels=['B-geo', 'I-geo', 'O', 'O', 'B-tim', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']),\n",
              " InputExample(guid='train-2', words=['When', 'the', 'Lion', 'found', 'that', 'he', 'could', 'not', 'escape', ',', 'he', 'flew', 'upon', 'the', 'sheep', 'and', 'killed', 'them', ',', 'and', 'then', 'attacked', 'the', 'oxen', '.'], labels=['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBBt9N1AvnHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "9b8a70cb-258f-4081-c81b-e66d789440cb"
      },
      "source": [
        "labels = []\n",
        "for t in open(LABELS_FILE_PATH).readlines():\n",
        "  labels.append(t.replace('\\n',''))\n",
        "labels "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'B-geo',\n",
              " 'B-gpe',\n",
              " 'B-per',\n",
              " 'I-geo',\n",
              " 'B-org',\n",
              " 'I-org',\n",
              " 'B-tim',\n",
              " 'B-art',\n",
              " 'I-art',\n",
              " 'I-per',\n",
              " 'I-gpe',\n",
              " 'I-tim',\n",
              " 'B-nat',\n",
              " 'B-eve',\n",
              " 'I-eve',\n",
              " 'I-nat']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ2kNmDywcS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer= AutoTokenizer.from_pretrained('roberta-base')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-pYv0gfuas0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = task.convert_examples_to_features(examples,label_list=labels, max_seq_length=128, tokenizer=tokenizer)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3nsUjYvxd4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b5db8ae8-2ffd-467c-cb86-69f47543d67e"
      },
      "source": [
        "features[:2]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[InputFeatures(input_ids=[3, 10050, 530, 33594, 18, 11455, 25464, 19726, 879, 548, 6691, 102, 2527, 12, 4155, 19247, 4030, 43128, 40008, 1258, 35351, 6, 10162, 560, 32845, 466, 6, 2466, 6, 151, 4651, 41207, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label_ids=[-100, 1, 4, -100, 0, 0, 7, 0, 0, -100, -100, 0, 0, -100, -100, 0, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, -100, -100, -100, -100, 0, 0, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]),\n",
              " InputFeatures(input_ids=[3, 1779, 627, 574, 1499, 19256, 6025, 700, 17304, 3654, 46230, 6, 700, 40307, 605, 32630, 627, 8877, 2462, 463, 39132, 35369, 6, 463, 13040, 2611, 10074, 627, 4325, 225, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label_ids=[-100, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, 0, -100, 0, 0, 0, 0, 0, 0, 0, -100, 0, 0, -100, 0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCbXxiskuvMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_ner.py  --model_name_or_path 'roberta-base'\\\n",
        " --labels '/content/drive/My Drive/transformers-ner/data/labels.txt' \\\n",
        "--data_dir '/content/drive/My Drive/transformers-ner/data' \\\n",
        "--output_dir 'model' \\\n",
        "--max_seq_length  '128' \\\n",
        "--num_train_epochs 3 \\\n",
        "--per_device_train_batch_size 8 \\\n",
        "--save_steps 1000000 \\\n",
        "--seed 16 \\\n",
        "--do_train \\\n",
        "--do_predict \\\n",
        "--overwrite_output_dir \\\n",
        "--fp16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBpSQH-UyIH0",
        "colab_type": "text"
      },
      "source": [
        "Change the per_device_train_batch_size if you are facing out of memory issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CIRPZ9LyPx6",
        "colab_type": "text"
      },
      "source": [
        "## Model Prediction \n",
        "\n",
        "Once the model is trained, the final model weights, configuration, tokenizer will be avilable in the output_dir. We will use the Pipeline module to do the model predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QruG_cOix5qC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import pipeline\n",
        "model_name = '/content/drive/My Drive/transformers-ner/model'\n",
        "nlp = pipeline(task=\"ner\", model=model_name, tokenizer=model_name, framework=\"pt\",grouped_entities=False)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFvgiqLCynOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4c3cd44-2254-44da-edf9-a11dce79333c"
      },
      "source": [
        "sequence = \"\"\"\n",
        "Remind me to do those 11 things at 10 pm.\n",
        "\"\"\"\n",
        "result = nlp(sequence)\n",
        "result"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'B-tim', 'score': 0.9684648811817169, 'word': ' 10 pm'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuA0PCfbyw2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "2a4d2868-09bb-40b3-c0c8-d360d792d102"
      },
      "source": [
        "nlp = pipeline(task=\"ner\", model=model_name, tokenizer=model_name, framework=\"pt\",grouped_entities=True)\n",
        "result = nlp(sequence)\n",
        "result"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'B-tim', 'score': 0.9991254210472107, 'word': 'Today'},\n",
              " {'entity_group': 'B-geo', 'score': 0.7912563681602478, 'word': ' India'},\n",
              " {'entity_group': 'B-geo', 'score': 0.6456469893455505, 'word': ' Pakistan'},\n",
              " {'entity_group': 'B-tim', 'score': 0.981935441493988, 'word': ' 2pm'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuOxoj99zRkl",
        "colab_type": "text"
      },
      "source": [
        "We will try Training Token Classification for one more dataset in Part-3 to see how easy it is do the prediction and training for any dataset."
      ]
    }
  ]
}