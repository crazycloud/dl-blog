{
  
    
        "post0": {
            "title": "Detecting Handwritten text in Documents",
            "content": "Problem Statement . We wish to detect the handwritten text in the scanned/pdf document. It could be for number of reasons like . to identify if the document has been signed | to process handwritten text in the document in a different way | to mask the handwritten text | . Take following document image for an example. We wish to detect the text highlighted in the red bounding boxes. . Training Dataset . We will use the annotated dataset available in the following github repo https://github.com/CatalystCode/Handwriting/tree/master/Data/labelledcontracttrainingdata/trainingjpg_output_99/ . The dataset is part of the Microsoft blog available here https://devblogs.microsoft.com/cse/2018/05/07/handwriting-detection-and-recognition-in-scanned-documents-using-azure-ml-package-computer-vision-azure-cognitive-services-ocr/ . Each image is annotated in Pascal VOC Annotation format using Microsoft Vott Annotation tool. The directory structure of the annotated dataset looks like this . data/Annotations_99 data/JPEGImages_99 . There are 99 annotated images in the dataset. The images are present in JPEGImages_99 folder and corresponding xml annotations are available under Annotations_99. . An XML annotation file looks like . &lt;annotation verified=&quot;yes&quot;&gt; &lt;folder&gt;Annotation&lt;/folder&gt; &lt;filename&gt;07653e58-24d1-4b3f-9b4a-76057efe5c09-1&lt;/filename&gt; &lt;path&gt;C: data JPEGImages 07653e58-24d1-4b3f-9b4a-76057efe5c09-1.jpg&lt;/path&gt; &lt;source&gt; &lt;database&gt;Unknown&lt;/database&gt; &lt;/source&gt; &lt;size&gt; &lt;width&gt;1700&lt;/width&gt; &lt;height&gt;2200&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; &lt;object&gt; &lt;name&gt;signature&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;bndbox&gt; &lt;xmin&gt;192&lt;/xmin&gt; &lt;ymin&gt;1188&lt;/ymin&gt; &lt;xmax&gt;738&lt;/xmax&gt; &lt;ymax&gt;1320&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; ... . In Pascal VOC annotation, there is a seperate annotation file for each image. The data we are interested in the xml file is . image filename - 07653e58-24d1-4b3f-9b4a-76057efe5c09-1 | object attribute for each annotation in the image category/class of the marked annotation | bounding box coordinates of top left and right bottom position | . | . About Detectron2 Framework . We will use pytorch detectron2 framework because it is simple and easy to extend. There are simple Training, Visualization, and Prediction modules available in the detectron2 which handles most of the stuff and we can use it as is, or if required, we can extend the functionality. . Simple steps to train a vision model in Detectron2 . Convert dataset in the detectron2 format | Register the dataset and metadata information like class labels | Update the config with registered dataset (DATASETS.{TRAIN,TEST}), model weight (MODEL.WEIGHT), learning rate, Number of output classes (MODEL.ROI_HEADS.NUM_CLASSES), and other training and test parameters | Train the model using DefaultTrainer class | Dataset Preparation(step 1 &amp; 2) . Detectron2 expects the dataset as list[dict] in the following format. So for training with detectron2 we will have to convert our dataset in the following format. . [{&#39;file_name&#39;: &#39;datasets/JPEGImages/1.jpg&#39;, &#39;image_id&#39;: &#39;1&#39;, &#39;height&#39;: 3300, &#39;width&#39;: 2550, &#39;annotations&#39;: [{&#39;category_id&#39;: 1, &#39;bbox&#39;: [1050.1000264270613, 457.33333333333337, 1406.9139799154334, 587.7450980392157], &#39;bbox_mode&#39;: &lt;BoxMode.XYXY_ABS: 0&gt;}, {&#39;category_id&#39;: 1, &#39;bbox&#39;: [1529.9097515856238, 473.5098039215687, 1617.167679704017, 555.3921568627452], &#39;bbox_mode&#39;: &lt;BoxMode.XYXY_ABS: 0&gt;}]}] . Detectron registers this list of dict as torch dataset and uses the default dataloader and datasampler for training. We can register the list[dict] with detectron2 using following code . def get_dicts(): ... return list[dict] in the above format from detectron2.data import DatasetCatalog DatasetCatalog.register(&quot;my_dataset&quot;, get_dicts) . And to register the metadata information related to dataset like category mapping to id&#39;s, the type of dataset, we have to set the keyvalue pair using . MetadataCatalog.get(&quot;my_dataset&quot;).thing_classes = [&quot;person&quot;, &quot;dog&quot;] . Choosing a Model and Initializing Configuration (step 3) . Detectron2 has lot of pretrained model available in the model zoo. For handwritten text detection, we will choose Faster RCNN with FPN backbone. . We have to initialize the parameters and weights for model we want to train. . cfg = get_cfg() cfg.merge_from_file(&#39;&lt;pretrained model config&#39;&gt;) cfg.MODEL.WEIGHTS = &#39;&lt;path to pretrained model weight&gt; #custom config for training cfg.DATASETS.TRAIN = (&quot;&lt;registered training dataset name&gt;&quot;,) cfg.SOLVER.MAX_ITER = &#39;&lt;number of training iterations&gt;&#39; cfg.MODEL.ROI_HEADS.NUM_CLASSES = &#39;&lt;number of classes&gt;&#39; . All the model configs are available in cfg object. If we want to replicate the training later, we can save the cfg object and load it back to resume training. . Model Training (step 4) . We will use the DefaultTrainer for now. There are simple modules available which only accept the minimal parameters and make assumptions about lot of things. . The DefaultTrainer Module . builds the model | builds the optimizer | builds the dataloader | loads the model weights, and | register common hooks | . trainer = DefaultTrainer(cfg) trainer.resume_or_load(resume=False) trainer.train() . Now, we can train our Instance Detection model using Detectron2. We will try FasterRCNN-FPN-50 Model and see how it performs . 1. Prepare &amp; Visualize the Dataset . To visualize the labeled dataset in detectron2, we need to convert the xml annotations in the detectron2 dataset format as explained above. . We will use the custom function register_pascal_voc() which will convert the dataset into detectron2 format and register it with DatasetCatalog. It expects the directory structure like . Annotations JPEGImages train.txt . train.txt and test.txt have a filename(without extension) per row . Visualizer Class . To draw the annotations on the images, we will use the Detectron2 Visualizer class which takes the image in rgb format, the metadata which has ordered label names and the scale parameter. . Visualizer.draw_instance_predictions() function to visualize prediction results Visualizer.draw_dataset_dict() function to draw the annotated dataset . %matplotlib inline . import numpy as np import os import xml.etree.ElementTree as ET from detectron2.data import DatasetCatalog, MetadataCatalog from detectron2.structures import BoxMode from fvcore.common.file_io import PathManager import random import cv2 from detectron2.utils.visualizer import Visualizer from matplotlib.pyplot import figure from matplotlib import pyplot as plt def load_voc_instances(dirname, split, CLASS_NAMES): &quot;&quot;&quot; Load Pascal VOC detection annotations to Detectron2 format. Args: dirname: Contain &quot;Annotations&quot;, &quot;JPEGImages&quot; split (str): one of &quot;train&quot;, &quot;test&quot;, &quot;val&quot;, &quot;trainval&quot; &quot;&quot;&quot; with PathManager.open(os.path.join(dirname, split+&quot;.txt&quot;)) as f: fileids = np.loadtxt(f, dtype=np.str) dicts = [] for fileid in fileids: anno_file = os.path.join(dirname, &quot;Annotations&quot;, fileid + &quot;.xml&quot;) jpeg_file = os.path.join(dirname, &quot;JPEGImages&quot;, fileid + &quot;.jpg&quot;) tree = ET.parse(anno_file) r = { &quot;file_name&quot;: jpeg_file, &quot;image_id&quot;: fileid, &quot;height&quot;: int(tree.findall(&quot;./size/height&quot;)[0].text), &quot;width&quot;: int(tree.findall(&quot;./size/width&quot;)[0].text), } instances = [] for obj in tree.findall(&quot;object&quot;): cls = obj.find(&quot;name&quot;).text # We include &quot;difficult&quot; samples in training. # Based on limited experiments, they don&#39;t hurt accuracy. # difficult = int(obj.find(&quot;difficult&quot;).text) # if difficult == 1: # continue bbox = obj.find(&quot;bndbox&quot;) bbox = [float(bbox.find(x).text) for x in [&quot;xmin&quot;, &quot;ymin&quot;, &quot;xmax&quot;, &quot;ymax&quot;]] # Original annotations are integers in the range [1, W or H] # Assuming they mean 1-based pixel indices (inclusive), # a box with annotation (xmin=1, xmax=W) covers the whole image. # In coordinate space this is represented by (xmin=0, xmax=W) bbox[0] -= 1.0 bbox[1] -= 1.0 instances.append( {&quot;category_id&quot;: CLASS_NAMES.index(cls), &quot;bbox&quot;: bbox, &quot;bbox_mode&quot;: BoxMode.XYXY_ABS} ) r[&quot;annotations&quot;] = instances dicts.append(r) return dicts def visualize_dataset(datasetname, n_samples=10): dataset_dicts = DatasetCatalog.get(datasetname) metadata = MetadataCatalog.get(datasetname) for d in random.sample(dataset_dicts,n_samples): print(d[&#39;file_name&#39;]) img = cv2.imread(d[&quot;file_name&quot;]) visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5) vis = visualizer.draw_dataset_dict(d) figure(num=None, figsize=(15, 15), dpi=100, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.axis(&quot;off&quot;) plt.imshow(vis.get_image()[:, :, ::-1]) plt.show() def register_pascal_voc(name, dirname, split, CLASS_NAMES): if name not in DatasetCatalog.list(): DatasetCatalog.register(name, lambda: load_voc_instances(dirname, split, CLASS_NAMES)) MetadataCatalog.get(name).set( thing_classes=CLASS_NAMES, split=split, dirname= dirname, year=2012 ) . #register pascal voc dataset in detectron2 register_pascal_voc(&#39;signature_dataset_train&#39;, dirname=&#39;datasets&#39;, split=&#39;train&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) . visualize_dataset(&#39;signature_dataset_train&#39;,n_samples = 4) . datasets/JPEGImages/07653e58-24d1-4b3f-9b4a-76057efe5c09-3.jpg . datasets/JPEGImages/7674b81e-aa42-4891-856d-8938620d6fa0-1.jpg . datasets/JPEGImages/84cce561-1ee5-4201-9dfe-13da1711ca75-2.jpg . datasets/JPEGImages/9854ded9-3bd7-437c-83c4-b05d409c5872-2.jpg . 2. Model Training . from detectron2.engine import default_argument_parser from detectron2.engine import DefaultTrainer from detectron2.engine import default_setup from detectron2.config import get_cfg def setup_cfg(args): &quot;&quot;&quot; Create configs and perform basic setups. &quot;&quot;&quot; cfg = get_cfg() cfg.merge_from_file(args.config_file) cfg.merge_from_list(args.opts) cfg.freeze() default_setup(cfg, args) return cfg parser = default_argument_parser() args = parser.parse_args(&quot;--config-file sign_config/sign_faster_rcnn_R_50_FPN_3x.yaml OUTPUT_DIR sign_model &quot;.split()) . We have copied the config file for Faster RCNN R50 FPN from the model zoo as sign_faster_rcnn_R_50_FPN_3x.yaml and updated the configuration parameters. We have set the MODEL.ROI_HEADS(classes) to 2, Max Number of iterations to 4000, and training dataset name to the one we registered earlier. . config.setup_cfg function will load the configuration from the --config-file path, and will update the configration with other parameters passed as arguments . Here, we have passed the OUTPUT_DIR parameter to update the cfg.OUTPUT_DIR parameter value . cfg = setup_cfg(args) . Now that we have all the configurations, we can start training the model. . As I explained earlier, DefaultTrainer will build the model(without weights), optimizer, learning rate scheduler and then load weights from the checkpoint file specified in the cfg.MODEL.WEIGHTS parameter. . trainer = DefaultTrainer(cfg) trainer.resume_or_load(resume=False) trainer.train() . 3. Model Prediction . Now that the model has been trained and saved in the output directory. The config saved during the model training has all the parameters except model weight. We pass the model weight path as paramter to load the trained model weight. . The DefaultPredictor does image translation and takes only single image for prediction. But we can easily modify the DefaultPredictor class to accept batch of input images for prediction . from detectron2.engine import default_argument_parser from detectron2.engine import DefaultPredictor import config parser = default_argument_parser() args = parser.parse_args(&quot;--config-file sign_model/config.yaml MODEL.WEIGHTS sign_model/model_final.pth&quot;.split()) cfg = config.setup_cfg(args) predictor = DefaultPredictor(cfg) . import glob import time import os from matplotlib.pyplot import figure from matplotlib import pyplot as plt import cv2 from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog files = glob.glob(&quot;test_images/*.jpg&quot;) sample_size = 5 for file,_ in zip(files,range(sample_size)): im = cv2.imread(file) MetadataCatalog.get(&quot;signature_dataset_train&quot;).thing_classes = [&quot;signature&quot;,&quot;others&quot;] start_time = time.time() outputs = predictor(im) print(time.time()- start_time) v = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(&quot;signature_dataset_train&quot;), scale=0.5) v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) print(file) figure(num=None, figsize=(15, 15), dpi=100, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.axis(&quot;off&quot;) plt.imshow(v.get_image()[:, :, ::-1]) plt.show() . 0.4223031997680664 test_images/image_11.jpg . 0.18352723121643066 test_images/0d0eddfc-731b-44de-b84d-d265afc7d996-1.jpg . 0.17967772483825684 test_images/07653e58-24d1-4b3f-9b4a-76057efe5c09-6.jpg . 0.14139199256896973 test_images/0d0eddfc-731b-44de-b84d-d265afc7d996-2.jpg . 0.15801262855529785 test_images/image_10.jpg . 4. Evaluate . The DefaultTrainer class doesn&#39;t have a evaluator method implemented. I have created a new Trainer class and added the build_evaluator method. We could have used this new Trainer class in the first step instead of DefaultTrainer but I wanted to show how easy it is to train the model without writing more code. . from detectron2.engine import default_argument_parser import config import trainer import dataset_utils dataset_utils.register_pascal_voc(&#39;signature_dataset_train&#39;, dirname=&#39;datasets&#39;, split=&#39;train&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) #dataset_utils.register_pascal_voc(&#39;signature_dataset_test&#39;, dirname=&#39;datasets&#39;, split=&#39;test&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) parser = default_argument_parser() args = parser.parse_args(&quot;--config-file sign_model/config.yaml MODEL.WEIGHTS sign_model/model_final.pth&quot;.split()) trainer.eval(args) . OrderedDict([(&#39;bbox&#39;, {&#39;AP&#39;: 69.67298193268152, &#39;AP50&#39;: 98.10585383880746, &#39;AP75&#39;: 88.13255308840152})]) . . Training on a Custom Dataset . Let us say we have got some more annotated dataset which is not in PASCAL VOC xml format. To train the above model, we have write a custom function get_dicts() which returns data in detectron2 format. . To improve the accuracy of handwriting detection, I found one more dataset which is of annotated french documents. The annotations are in json format for each image. The dataset is available in the following github repo https://github.com/hyperlex/Signature-detection-Practical-guide/tree/master/data/dataset. Download and save it in french_dataset directory . I have added all the functions in the library files dataset_utils.py and trainer.py. We will use these abstractions to quickly train and evaluate new models . import json import glob import os import cv2 from detectron2.structures import BoxMode def get_french_dicts(annot_dir): json_files = glob.glob(os.path.join(annot_dir,&#39;*.json&#39;)) dataset_dicts = [] for f in json_files: record={} img_ann = json.load(open(f)) filename = img_ann[&#39;asset&#39;][&#39;name&#39;] height, width = cv2.imread(os.path.join(annot_dir,&#39;..&#39;,filename)).shape[:2] record[&quot;file_name&quot;] = os.path.join(annot_dir,&#39;..&#39;,filename) record[&quot;image_id&quot;] = img_ann[&#39;asset&#39;][&#39;id&#39;] record[&quot;height&quot;] = height record[&quot;width&quot;] = width annos = img_ann[&quot;regions&quot;] objs =[] for ann in annos: px = ann[&#39;boundingBox&#39;][&#39;left&#39;] py = ann[&#39;boundingBox&#39;][&#39;top&#39;] px1 = ann[&#39;boundingBox&#39;][&#39;left&#39;] + ann[&#39;boundingBox&#39;][&#39;width&#39;] py1 = ann[&#39;boundingBox&#39;][&#39;top&#39;] + ann[&#39;boundingBox&#39;][&#39;height&#39;] obj = { &quot;bbox&quot;: [px, py, px1, py1], &quot;bbox_mode&quot;: BoxMode.XYXY_ABS, &quot;category_id&quot;: {&#39;signature&#39;:0,&#39;paraphe&#39;:1,&#39;date&#39;:1}[ann[&#39;tags&#39;][0]], &quot;iscrowd&quot;: 0 } objs.append(obj) record[&quot;annotations&quot;] = objs dataset_dicts.append(record) return dataset_dicts . from detectron2.utils.visualizer import Visualizer from detectron2.data import DatasetCatalog, MetadataCatalog import dataset_utils def get_img_dicts(): ann1 = dataset_utils.load_voc_instances(dirname = &#39;datasets&#39;, split = &#39;train&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) ann2 = get_french_dicts(&#39;french_dataset/per_img_labels&#39;) return ann1 + ann2 dataset_name = &#39;signature_dataset_train&#39; DatasetCatalog.register(dataset_name, lambda: get_img_dicts()) MetadataCatalog.get(dataset_name).set(thing_classes=[&quot;signature&quot;,&quot;others&quot;], split=&#39;train&#39;, dirname= dirname, year=2012) . Metadata(name=&#39;signature_dataset_train&#39;, thing_classes=[&#39;signature&#39;, &#39;others&#39;]) . len(DatasetCatalog.get(dataset_name)) . 139 . dataset_utils.visualize_dataset(&#39;signature_dataset_train&#39;, n_samples=2) . datasets/JPEGImages/63348ad3-b0cc-45d0-bc85-bf2c865744ec-2.jpg . datasets/JPEGImages/4c85bb9b-1d8d-45b0-8f4a-664d77ee4b83-4.jpg . We can run the remaining steps as we did before for training the model and prediction . from detectron2.engine import default_argument_parser from detectron2.engine import DefaultTrainer import trainer parser = default_argument_parser() args = parser.parse_args(&quot;--config-file sign_config/chk_faster_rcnn_R_50_FPN_3x.yaml --num-gpus 3 OUTPUT_DIR french_sign_model SOLVER.MAX_ITER 4000&quot;.split()) trainer.train(args) . from detectron2.engine import default_argument_parser import config import trainer import dataset_utils import dataset_utils dataset_utils.register_pascal_voc(&#39;signature_dataset_test&#39;, dirname=&#39;datasets&#39;, split=&#39;train&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) parser = default_argument_parser() args = parser.parse_args(&#39;--config-file french_sign_model/config.yaml MODEL.WEIGHTS french_sign_model/model_final.pth DATASETS.TEST (&quot;signature_dataset_test&quot;,)&#39;.split()) trainer.eval(args) . OrderedDict([(&#39;bbox&#39;, {&#39;AP&#39;: 67.82590895115814, &#39;AP50&#39;: 96.77629667360196, &#39;AP75&#39;: 84.18387912626437})]) . The Average Precision has reduced compared to the previous model. Let us check the prediction results. . from detectron2.engine import default_argument_parser from detectron2.engine import DefaultPredictor import config parser = default_argument_parser() args = parser.parse_args(&quot;--config-file french_sign_model/config.yaml MODEL.WEIGHTS french_sign_model/model_final.pth&quot;.split()) cfg = config.setup_cfg(args) predictor = DefaultPredictor(cfg) . import glob import time import os from matplotlib.pyplot import figure from matplotlib import pyplot as plt import cv2 from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog files = glob.glob(&quot;test_images/*.jpg&quot;) sample_size = 5 for file,_ in zip(files,range(sample_size)): im = cv2.imread(file) MetadataCatalog.get(&quot;signature_dataset_train&quot;).thing_classes = [&quot;signature&quot;,&quot;others&quot;] start_time = time.time() outputs = predictor(im) print(time.time()- start_time) v = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(&quot;signature_dataset_train&quot;), scale=0.5) v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) print(file) figure(num=None, figsize=(15, 15), dpi=100, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.imshow(v.get_image()[:, :, ::-1]) plt.show() . 0.4847722053527832 test_images/image_11.jpg . 0.18241333961486816 test_images/0d0eddfc-731b-44de-b84d-d265afc7d996-1.jpg . 0.17200803756713867 test_images/07653e58-24d1-4b3f-9b4a-76057efe5c09-6.jpg . 0.18814849853515625 test_images/0d0eddfc-731b-44de-b84d-d265afc7d996-2.jpg . 0.19216012954711914 test_images/image_10.jpg .",
            "url": "https://crazycloud.github.io/dl-blog/detectron2/fasterrcnn/vision/2020/04/09/Handwritten-Text-Detection-in-Detectron2.html",
            "relUrl": "/detectron2/fasterrcnn/vision/2020/04/09/Handwritten-Text-Detection-in-Detectron2.html",
            "date": " • Apr 9, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://crazycloud.github.io/dl-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://crazycloud.github.io/dl-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://crazycloud.github.io/dl-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://crazycloud.github.io/dl-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}