{
  
    
        "post0": {
            "title": "Entity Extraction (NER) - Training and Inference using HuggingFace Transformers Library - Part 1",
            "content": ". In Part-1, we will discuss how to use the Pretrained language model, the Token Classification Head, and Tokenizer. You can skip the Part-1 if you want to go ahead and check the finetuning demo and model prediction. . In Part-2, we will learn to finetune the TokenClassification head of RoBerta model. . Token Classification Task . The token classification task identifies the entities of interest in the text. For NER task, we identify the Named entities like Person, Location, Organization and etc. . . Fig 0. NER task . Pretrained Language Model . A pretrained language model like BERT, ALBERT provides the contextual embedding for a token. The contextual embedding is the representation based on the sentence/context a token is used in. So it will understand difference between a River Bank and a Financial Banks. . What is a token? . A token could be a whole word or a substring of a word. If a word is not part of the model vocabulary it would be broken down into subwords based on some algorithm. . There are 3 important parts of a pretrained language model . Model - Neural Network model Architecture Tokenizer - Preprocessor for converting the text into the input representation that the model can understand. Model Weights - The weights of the model are saved in a seperate file. . How to use a pretrained model for a specific task? . For any task like NER or Classification, we add a Task head(another Neural Network layer on top of base model) to map word embedding into output labels. During the Training process we learn weights of the head to classify each token properly. . . Fig 1. Pretrained Bert Model without any task specific head. Each sentence is converted into token embedding Tok1, Tok2 using a Tokenizer and the fed into the Model which outputs new representation for each token Tok1, Tok2 .. along with additional token [CLS] and [SEP] . Installation . Install the latest package of transformers from the github or follow the installation instructions here https://huggingface.co/transformers/installation.html . #install the latest transformers library !pip install git+https://github.com/huggingface/transformers.git . Collecting git+https://github.com/huggingface/transformers.git Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-s1xpfl5a Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-s1xpfl5a Requirement already satisfied (use --upgrade to upgrade): transformers==3.1.0 from git+https://github.com/huggingface/transformers.git in /usr/local/lib/python3.6/dist-packages Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (1.18.5) Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8.1rc2) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.4) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20) Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.1.91) Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.0.43) Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.7) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers==3.1.0) (2.4.7) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers==3.1.0) (1.15.0) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (2020.6.20) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (1.24.3) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.1.0) (0.16.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.1.0) (7.1.2) Building wheels for collected packages: transformers Building wheel for transformers (setup.py) ... done Created wheel for transformers: filename=transformers-3.1.0-cp36-none-any.whl size=996650 sha256=5252d2ca62c92b2555c75fc9def4499ed11fc40d8078891199f7ee3f52654d90 Stored in directory: /tmp/pip-ephem-wheel-cache-o28ockr8/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f Successfully built transformers . import transformers transformers.__version__ . &#39;3.1.0&#39; . Base Language Model . Roberta Pretrained embeddings, or any other Language Model embeddings, provides a vector representation for each token. A word could be broken down into multiple tokens by the tokenizer. . from transformers import AutoModel, AutoTokenizer import torch # Load the Ro pretrained tokenizer and model tokenizer = AutoTokenizer.from_pretrained(&#39;roberta-base&#39;) model = AutoModel.from_pretrained(&#39;roberta-base&#39;) . Tokenizer . A tokenizer . split the sentence into words and then further into subwords if the word is not part of the Tokenizer Vocabulary. | assign id to each token based on the vocabulary dictionary (word to integer mapping) | . Since the pretrained models are not trained for any specific domains, they consider Top N high frequency tokens. It is not possible to add all the words to the model vocabulary because that would drastically increase the model Embedding layer weights and also, lot of new words are added to dictionary every now and then. . . Fig 2. The above picture is taked from the Huggingface Tokenizer example. It clearly illustrates the character level, the token level and the word level tokenizer. . The middle one is the Token level Tokenizer which is used frequently now a days. . #The Roberta tokenizer vocabulary size tokenizer.vocab_size . 50265 . #list all the vocabulary words in the tokenizer tokenizer.get_vocab() . {&#39;&lt;s&gt;&#39;: 0, &#39;&lt;pad&gt;&#39;: 1, &#39;&lt;/s&gt;&#39;: 2, &#39;&lt;unk&gt;&#39;: 3, &#39;.&#39;: 4, &#39;Ġthe&#39;: 5, &#39;,&#39;: 6, &#39;Ġto&#39;: 7, &#39;Ġand&#39;: 8, &#39;Ġof&#39;: 9, &#39;Ġa&#39;: 10, &#39;Ġin&#39;: 11, &#39;-&#39;: 12, &#39;Ġfor&#39;: 13, &#39;Ġthat&#39;: 14, &#39;Ġon&#39;: 15, &#39;Ġis&#39;: 16, &#39;âĢ&#39;: 17, &#34;&#39;s&#34;: 18, &#39;Ġwith&#39;: 19, &#39;ĠThe&#39;: 20, &#39;Ġwas&#39;: 21, &#39;Ġ&#34;&#39;: 22, &#39;Ġat&#39;: 23, &#39;Ġit&#39;: 24, &#39;Ġas&#39;: 25, &#39;Ġsaid&#39;: 26, &#39;Ļ&#39;: 27, &#39;Ġbe&#39;: 28, &#39;s&#39;: 29, &#39;Ġby&#39;: 30, &#39;Ġfrom&#39;: 31, &#39;Ġare&#39;: 32, &#39;Ġhave&#39;: 33, &#39;Ġhas&#39;: 34, &#39;:&#39;: 35, &#39;Ġ(&#39;: 36, &#39;Ġhe&#39;: 37, &#39;ĠI&#39;: 38, &#39;Ġhis&#39;: 39, &#39;Ġwill&#39;: 40, &#39;Ġan&#39;: 41, &#39;Ġthis&#39;: 42, &#39;)&#39;: 43, &#39;ĠâĢ&#39;: 44, &#39;Ġnot&#39;: 45, &#39;Ŀ&#39;: 46, &#39;Ġyou&#39;: 47, &#39;ľ&#39;: 48, &#39;Ġtheir&#39;: 49, &#39;Ġor&#39;: 50, &#39;Ġthey&#39;: 51, &#39;Ġwe&#39;: 52, &#39;Ġbut&#39;: 53, &#39;Ġwho&#39;: 54, &#39;Ġmore&#39;: 55, &#39;Ġhad&#39;: 56, &#39;Ġbeen&#39;: 57, &#39;Ġwere&#39;: 58, &#39;Ġabout&#39;: 59, &#39;,&#34;&#39;: 60, &#39;Ġwhich&#39;: 61, &#39;Ġup&#39;: 62, &#39;Ġits&#39;: 63, &#39;Ġcan&#39;: 64, &#39;Ġone&#39;: 65, &#39;Ġout&#39;: 66, &#39;Ġalso&#39;: 67, &#39;Ġ$&#39;: 68, &#39;Ġher&#39;: 69, &#39;Ġall&#39;: 70, &#39;Ġafter&#39;: 71, &#39;.&#34;&#39;: 72, &#39;/&#39;: 73, &#39;Ġwould&#39;: 74, &#34;&#39;t&#34;: 75, &#39;Ġyear&#39;: 76, &#39;Ġwhen&#39;: 77, &#39;Ġfirst&#39;: 78, &#39;Ġshe&#39;: 79, &#39;Ġtwo&#39;: 80, &#39;Ġover&#39;: 81, &#39;Ġpeople&#39;: 82, &#39;ĠA&#39;: 83, &#39;Ġour&#39;: 84, &#39;ĠIt&#39;: 85, &#39;Ġtime&#39;: 86, &#39;Ġthan&#39;: 87, &#39;Ġinto&#39;: 88, &#39;Ġthere&#39;: 89, &#39;t&#39;: 90, &#39;ĠHe&#39;: 91, &#39;Ġnew&#39;: 92, &#39;ĠâĢĶ&#39;: 93, &#39;Ġlast&#39;: 94, &#39;Ġjust&#39;: 95, &#39;ĠIn&#39;: 96, &#39;Ġother&#39;: 97, &#39;Ġso&#39;: 98, &#39;Ġwhat&#39;: 99, &#39;I&#39;: 100, &#39;Ġlike&#39;: 101, &#39;a&#39;: 102, &#39;Ġsome&#39;: 103, &#39;S&#39;: 104, &#39;Ã«&#39;: 105, &#39;Ġthem&#39;: 106, &#39;Ġyears&#39;: 107, &#34;&#39;&#34;: 108, &#39;Ġdo&#39;: 109, &#39;Ġyour&#39;: 110, &#39;Ġ-&#39;: 111, &#39;Ġ1&#39;: 112, &#39;&#34;&#39;: 113, &#39;Ġif&#39;: 114, &#39;Ġcould&#39;: 115, &#39;?&#39;: 116, &#39;Ġno&#39;: 117, &#39;i&#39;: 118, &#39;m&#39;: 119, &#39;Ġget&#39;: 120, &#39;ĠU&#39;: 121, &#39;Ġnow&#39;: 122, &#39;Ġhim&#39;: 123, &#39;Ġback&#39;: 124, &#39;ĠBut&#39;: 125, &#39;ĠâĢĵ&#39;: 126, &#39;Ġmy&#39;: 127, &#34;Ġ&#39;&#34;: 128, &#39;Ġonly&#39;: 129, &#39;Ġthree&#39;: 130, &#39;;&#39;: 131, &#39;Ġ2&#39;: 132, &#39;The&#39;: 133, &#39;1&#39;: 134, &#39;Ġpercent&#39;: 135, &#39;Ġagainst&#39;: 136, &#39;Ġbefore&#39;: 137, &#39;Ġcompany&#39;: 138, &#39;o&#39;: 139, &#39;ĠTrump&#39;: 140, &#39;Ġhow&#39;: 141, &#39;Ġbecause&#39;: 142, &#39;Ġany&#39;: 143, &#39;Ġmost&#39;: 144, &#39;Ġbeing&#39;: 145, &#39;Ġmake&#39;: 146, &#39;Ġwhere&#39;: 147, &#39;Ġduring&#39;: 148, &#39;Ġthrough&#39;: 149, &#39;Ġwhile&#39;: 150, &#39;000&#39;: 151, &#39;ĠThis&#39;: 152, &#39;Ġmillion&#39;: 153, &#39;ing&#39;: 154, &#39;Ġ3&#39;: 155, &#39;Ġmade&#39;: 156, &#39;Ġwell&#39;: 157, &#39;Ġ10&#39;: 158, &#39;Ġdown&#39;: 159, &#39;Ġoff&#39;: 160, &#39;Ġsays&#39;: 161, &#39;Ġme&#39;: 162, &#39;ĠB&#39;: 163, &#39;Ġgoing&#39;: 164, &#39;Ġteam&#39;: 165, &#39;ĠWe&#39;: 166, &#39;Ġthose&#39;: 167, &#39;Ġgovernment&#39;: 168, &#39;Ġway&#39;: 169, &#39;We&#39;: 170, &#39;Ġmany&#39;: 171, &#39;Ġthen&#39;: 172, &#39;Ġwork&#39;: 173, &#39;Ġtold&#39;: 174, &#39;com&#39;: 175, &#39;2&#39;: 176, &#39;Ġgame&#39;: 177, &#39;ĠAnd&#39;: 178, &#39;in&#39;: 179, &#39;year&#39;: 180, &#39;Ġp&#39;: 181, &#39;Ġvery&#39;: 182, &#39;Ġday&#39;: 183, &#39;Ġhome&#39;: 184, &#39;Ġtake&#39;: 185, &#39;Ġweek&#39;: 186, &#39;Ġsince&#39;: 187, &#39;ĠNew&#39;: 188, &#39;Ġmay&#39;: 189, &#39;Ġeven&#39;: 190, &#39;Ġseason&#39;: 191, &#39;Ġsee&#39;: 192, &#39;Ġ2017&#39;: 193, &#39;Ġstate&#39;: 194, &#39;Ġ5&#39;: 195, &#39;ed&#39;: 196, &#39;Ġshould&#39;: 197, &#39;Ġaround&#39;: 198, &#39;Ġ2018&#39;: 199, &#39;Ġsecond&#39;: 200, &#39;Ġus&#39;: 201, &#39;Ġstill&#39;: 202, &#39;Ġmuch&#39;: 203, &#39;Ġ4&#39;: 204, &#39;Ġgood&#39;: 205, &#39;Ġthink&#39;: 206, &#39;%&#39;: 207, &#39;ĠS&#39;: 208, &#39;Ġthese&#39;: 209, &#39;Ġmarket&#39;: 210, &#39;ĠD&#39;: 211, &#39;th&#39;: 212, &#39;Ġgo&#39;: 213, &#34;&#39;re&#34;: 214, &#39;Ġsuch&#39;: 215, &#39;Ġknow&#39;: 216, &#39;Ġincluding&#39;: 217, &#39;Ġdon&#39;: 218, &#39;y&#39;: 219, &#39;Ġnext&#39;: 220, &#39;ĠP&#39;: 221, &#39;Ġdid&#39;: 222, &#39;Ġunder&#39;: 223, &#39;Ġsay&#39;: 224, &#39;en&#39;: 225, &#39;ĠL&#39;: 226, &#39;Ġbetween&#39;: 227, &#39;Ġper&#39;: 228, &#39;ĠK&#39;: 229, &#39;ĠC&#39;: 230, &#39;Ġ6&#39;: 231, &#39;Ġworld&#39;: 232, &#39;Ġpart&#39;: 233, &#39;ĠN&#39;: 234, &#39;Ġright&#39;: 235, &#39;Ġwant&#39;: 236, &#39;Ġfour&#39;: 237, &#39;),&#39;: 238, &#39;Ġhigh&#39;: 239, &#39;Ġneed&#39;: 240, &#39;re&#39;: 241, &#39;e&#39;: 242, &#39;It&#39;: 243, &#39;Ġhelp&#39;: 244, &#39;5&#39;: 245, &#39;3&#39;: 246, &#39;Ġcountry&#39;: 247, &#39;ĠR&#39;: 248, &#39;Ġpolice&#39;: 249, &#39;A&#39;: 250, &#39;Ġlong&#39;: 251, &#39;ĠThey&#39;: 252, &#39;Ġend&#39;: 253, &#39;er&#39;: 254, &#39;ĠT&#39;: 255, &#39;ĠM&#39;: 256, &#39;u&#39;: 257, &#39;Ġboth&#39;: 258, &#39;Ġhere&#39;: 259, &#39;an&#39;: 260, &#39;on&#39;: 261, &#39;Ġ7&#39;: 262, &#39;Ġde&#39;: 263, &#39;ĠShe&#39;: 264, &#39;Ġbusiness&#39;: 265, &#39;Ġreport&#39;: 266, &#39;j&#39;: 267, &#39;ers&#39;: 268, &#39;Ġreally&#39;: 269, &#39;ĠPresident&#39;: 270, &#39;ar&#39;: 271, &#39;ĠG&#39;: 272, &#39;ĠFriday&#39;: 273, &#39;ĠF&#39;: 274, &#39;Ġbest&#39;: 275, &#39;Ġsame&#39;: 276, &#39;Ġanother&#39;: 277, &#39;Ġset&#39;: 278, &#39;old&#39;: 279, &#39;ĠThat&#39;: 280, &#39;as&#39;: 281, &#39;n&#39;: 282, &#39;Ġcome&#39;: 283, &#39;Ġfamily&#39;: 284, &#39;Ġpublic&#39;: 285, &#39;ĠFor&#39;: 286, &#39;ĠAs&#39;: 287, &#39;0&#39;: 288, &#39;ĠH&#39;: 289, &#39;Ġ8&#39;: 290, &#39;Ġ20&#39;: 291, &#39;Ġfive&#39;: 292, &#39;es&#39;: 293, &#39;ĠTuesday&#39;: 294, &#39;Ġn&#39;: 295, &#39;ĠThursday&#39;: 296, &#39;Ġquarter&#39;: 297, &#39;h&#39;: 298, &#39;Ġtop&#39;: 299, &#39;Ġgot&#39;: 300, &#39;Ġlife&#39;: 301, &#39;ĠMonday&#39;: 302, &#39;Ġfound&#39;: 303, &#39;Ġuse&#39;: 304, &#39;ĠW&#39;: 305, &#39;4&#39;: 306, &#39;ĠWednesday&#39;: 307, &#39;Ġown&#39;: 308, &#39;Ġaccording&#39;: 309, &#39;Ġplay&#39;: 310, &#39;Ġshow&#39;: 311, &#39;ĠSt&#39;: 312, &#39;Ġman&#39;: 313, &#39;Ġleft&#39;: 314, &#39;ĠUnited&#39;: 315, &#39;Ġ12&#39;: 316, &#39;Ġplace&#39;: 317, &#39;ĠIf&#39;: 318, &#39;Ġlot&#39;: 319, &#39;Ġformer&#39;: 320, &#39;Ġ0&#39;: 321, &#39;).&#39;: 322, &#39;Ġsupport&#39;: 323, &#39;ie&#39;: 324, &#39;Ġbillion&#39;: 325, &#39;Ġt&#39;: 326, &#39;Ġshares&#39;: 327, &#39;!&#39;: 328, &#39;z&#39;: 329, &#39;k&#39;: 330, &#39;ĠState&#39;: 331, &#39;Ġpoints&#39;: 332, &#39;Ġgroup&#39;: 333, &#39;Ġschool&#39;: 334, &#39;Ġinformation&#39;: 335, &#39;Ġ2016&#39;: 336, &#39;al&#39;: 337, &#39;r&#39;: 338, &#39;Ġwin&#39;: 339, &#39;Ġnews&#39;: 340, &#39;Ġused&#39;: 341, &#39;Ġput&#39;: 342, &#39;Ġcity&#39;: 343, &#39;ĠJ&#39;: 344, &#39;ĠThere&#39;: 345, &#39;Ġnumber&#39;: 346, &#39;C&#39;: 347, &#34;&#39;ve&#34;: 348, &#39;Ġeach&#39;: 349, &#39;Ġtoo&#39;: 350, &#39;Ġwon&#39;: 351, &#39;ly&#39;: 352, &#39;Ġmonth&#39;: 353, &#39;is&#39;: 354, &#39;Ġadded&#39;: 355, &#39;Ġlook&#39;: 356, &#39;Ġbetter&#39;: 357, &#39;Ġevery&#39;: 358, &#39;Ġ&amp;&#39;: 359, &#39;Ġdays&#39;: 360, &#39;Ġ9&#39;: 361, &#39;Ġtook&#39;: 362, &#39;Ġnight&#39;: 363, &#39;Ġe&#39;: 364, &#39;Ġ11&#39;: 365, &#39;os&#39;: 366, &#39;Ġfew&#39;: 367, &#39;or&#39;: 368, &#39;ĠNorth&#39;: 369, &#39;ĠYou&#39;: 370, &#39;Ġthird&#39;: 371, &#39;Ġgreat&#39;: 372, &#39;Ġcalled&#39;: 373, &#39;ĠOn&#39;: 374, &#39;Ġpast&#39;: 375, &#39;Ġcame&#39;: 376, &#39;Ġmonths&#39;: 377, &#39;ĠSaturday&#39;: 378, &#39;Ġ15&#39;: 379, &#39;Ġbig&#39;: 380, &#39;ĠE&#39;: 381, &#39;ĠUS&#39;: 382, &#39;Ġthings&#39;: 383, &#39;ĠO&#39;: 384, &#39;Ġd&#39;: 385, &#39;Ġstart&#39;: 386, &#39;B&#39;: 387, &#39;Ġstock&#39;: 388, &#39;Ġ30&#39;: 389, &#39;Ġwomen&#39;: 390, &#39;ĠSouth&#39;: 391, &#39;ĠMay&#39;: 392, &#39;Ġnever&#39;: 393, &#39;Ġpresident&#39;: 394, &#39;ĠSunday&#39;: 395, &#39;Ġwithout&#39;: 396, &#39;man&#39;: 397, &#39;8&#39;: 398, &#39;Ġdidn&#39;: 399, &#39;Ġlocal&#39;: 400, &#39;6&#39;: 401, &#39;Ġsomething&#39;: 402, &#39;Ġcase&#39;: 403, &#39;ĠAll&#39;: 404, &#39;it&#39;: 405, &#39;7&#39;: 406, &#39;ĠSo&#39;: 407, &#39;Ġchildren&#39;: 408, &#39;Ġaway&#39;: 409, &#39;Ġlittle&#39;: 410, &#39;Ġsix&#39;: 411, &#39;ĠCity&#39;: 412, &#39;ĠCounty&#39;: 413, &#39;Ġdata&#39;: 414, &#39;at&#39;: 415, &#39;Ġalready&#39;: 416, &#39;d&#39;: 417, &#39;Ġmoney&#39;: 418, &#39;Ġearly&#39;: 419, &#39;Ġacross&#39;: 420, &#39;Ġexpected&#39;: 421, &#39;Ġrun&#39;: 422, &#39;Ġlater&#39;: 423, &#39;am&#39;: 424, &#39;Ġprice&#39;: 425, &#39;Ġgames&#39;: 426, &#39;ĠMr&#39;: 427, &#39;b&#39;: 428, &#39;Ġmight&#39;: 429, &#39;Ġdifferent&#39;: 430, &#39;Ġreported&#39;: 431, &#39;Ġdeal&#39;: 432, &#39;Ġmedia&#39;: 433, &#39;Ġgrowth&#39;: 434, &#39;Ġcommunity&#39;: 435, &#39;ĠChina&#39;: 436, &#34;&#39;m&#34;: 437, &#39;c&#39;: 438, &#39;Ġwent&#39;: 439, &#39;ĠNo&#39;: 440, &#39;Ġable&#39;: 441, &#39;Ġmaking&#39;: 442, &#39;Ġarea&#39;: 443, &#39;Ġfar&#39;: 444, &#39;Ġstatement&#39;: 445, &#39;ĠHouse&#39;: 446, &#39;Ġworking&#39;: 447, &#39;M&#39;: 448, &#39;Ġk&#39;: 449, &#39;Ġseen&#39;: 450, &#39;Ġcompanies&#39;: 451, &#39;Ġtoday&#39;: 452, &#39;Ġmembers&#39;: 453, &#39;Ġuntil&#39;: 454, &#39;Ġfull&#39;: 455, &#39;Ġagain&#39;: 456, &#39;Ġhalf&#39;: 457, &#39;Ġshare&#39;: 458, &#39;le&#39;: 459, &#39;Ġalways&#39;: 460, &#39;Ġcourt&#39;: 461, &#39;l&#39;: 462, &#39;and&#39;: 463, &#39;Ġchange&#39;: 464, &#39;Ġfind&#39;: 465, &#39;9&#39;: 466, &#39;Ġsystem&#39;: 467, &#39;ĠV&#39;: 468, &#39;ĠYork&#39;: 469, &#39;ĠAmerican&#39;: 470, &#39;Ġhead&#39;: 471, &#39;Ġplayers&#39;: 472, &#39;Ġdoes&#39;: 473, &#39;Ġhealth&#39;: 474, &#39;Ġm&#39;: 475, &#39;Ġpower&#39;: 476, &#39;Ġpoint&#39;: 477, &#39;Ġhit&#39;: 478, &#39;Ġ.&#39;: 479, &#39;Ġ--&#39;: 480, &#39;Ġfree&#39;: 481, &#39;.,&#39;: 482, &#39;Ġlead&#39;: 483, &#39;Ġseveral&#39;: 484, &#39;Ġrecent&#39;: 485, &#39;Ġcall&#39;: 486, &#39;N&#39;: 487, &#39;Ġlaw&#39;: 488, &#39;Ġkeep&#39;: 489, &#39;Ġopen&#39;: 490, &#39;ĠNews&#39;: 491, &#39;Ġgive&#39;: 492, &#39;ia&#39;: 493, &#39;ĠMarch&#39;: 494, &#39;D&#39;: 495, &#39;ĠNational&#39;: 496, &#39;ĠAt&#39;: 497, &#39;Ġtimes&#39;: 498, &#39;Ġfuture&#39;: 499, &#39;R&#39;: 500, &#39;Ġ14&#39;: 501, &#39;ĠJune&#39;: 502, &#39;Ġofficials&#39;: 503, &#39;Ġ18&#39;: 504, &#39;Ġimportant&#39;: 505, &#39;f&#39;: 506, &#39;Ġfinal&#39;: 507, &#39;Ġ13&#39;: 508, &#39;ĠOne&#39;: 509, &#39;P&#39;: 510, &#39;Ġfollowing&#39;: 511, &#39;Ġcar&#39;: 512, &#39;Ġleast&#39;: 513, &#39;Ġwater&#39;: 514, &#39;Ġevent&#39;: 515, &#39;Ġline&#39;: 516, &#39;Ġmove&#39;: 517, &#39;Ġservices&#39;: 518, &#39;Ġhaving&#39;: 519, &#39;ĠWhen&#39;: 520, &#39;Ġstudents&#39;: 521, &#39;ĠPolice&#39;: 522, &#39;el&#39;: 523, &#39;Ġam&#39;: 524, &#39;ĠZ&#39;: 525, &#39;Ġside&#39;: 526, &#39;Ġstory&#39;: 527, &#39;Ġdue&#39;: 528, &#39;Ġmeeting&#39;: 529, &#39;K&#39;: 530, &#39;Ġmust&#39;: 531, &#39;ĠStates&#39;: 532, &#39;Ġlikely&#39;: 533, &#39;G&#39;: 534, &#39;Ġcontinue&#39;: 535, &#39;Ġago&#39;: 536, &#39;Ġparty&#39;: 537, &#39;Ġmajor&#39;: 538, &#39;Ġindustry&#39;: 539, &#39;Ġless&#39;: 540, &#39;30&#39;: 541, &#39;Ġun&#39;: 542, &#39;Ġhard&#39;: 543, &#39;Ġservice&#39;: 544, &#39;Ġ16&#39;: 545, &#39;Ġlooking&#39;: 546, &#39;Ġheld&#39;: 547, &#39;ve&#39;: 548, &#39;Ġwhether&#39;: 549, &#39;ĠJuly&#39;: 550, &#39;Ġtaken&#39;: 551, &#39;Ġalong&#39;: 552, &#39;Ġasked&#39;: 553, &#39;Ġstarted&#39;: 554, &#39;Ġbecome&#39;: 555, &#39;Ġforward&#39;: 556, &#39;Ġresearch&#39;: 557, &#39;Ġoffice&#39;: 558, &#39;Ġpolitical&#39;: 559, &#39;to&#39;: 560, &#39;Ġtogether&#39;: 561, &#39;Ġgetting&#39;: 562, &#39;Ġplan&#39;: 563, &#39;Ġ25&#39;: 564, &#39;T&#39;: 565, &#39;Ġamong&#39;: 566, &#39;Ġcoming&#39;: 567, &#39;Ġdecision&#39;: 568, &#39;Ġvideo&#39;: 569, &#39;Ġ2015&#39;: 570, &#39;g&#39;: 571, &#39;ĠAfter&#39;: 572, &#39;Ġsecurity&#39;: 573, &#39;L&#39;: 574, &#39;Ġcare&#39;: 575, &#39;Ġgiven&#39;: 576, &#39;Ġavailable&#39;: 577, &#39;âĢĶ&#39;: 578, &#39;Ġs&#39;: 579, &#39;ĠWest&#39;: 580, &#34;&#39;ll&#34;: 581, &#39;Ġpay&#39;: 582, &#39;Ġnear&#39;: 583, &#39;Ġsaying&#39;: 584, &#39;Ġannounced&#39;: 585, &#39;Ġprogram&#39;: 586, &#39;ĠApril&#39;: 587, &#39;Ġreal&#39;: 588, &#39;ĠUniversity&#39;: 589, &#39;ĠWith&#39;: 590, &#39;AP&#39;: 591, &#39;Ġsocial&#39;: 592, &#39;Ġclose&#39;: 593, &#39;et&#39;: 594, &#39;Ġcurrent&#39;: 595, &#39;Ġwhy&#39;: 596, &#39;F&#39;: 597, &#39;ĠTo&#39;: 598, &#39;ĠTwitter&#39;: 599, &#39;Ġthough&#39;: 600, &#39;Ġ17&#39;: 601, &#39;Ġtaking&#39;: 602, &#39;ĠInc&#39;: 603, &#39;Ġmen&#39;: 604, &#39;w&#39;: 605, &#39;Ġcomes&#39;: 606, &#39;ley&#39;: 607, &#39;Ġdoing&#39;: 608, &#39;Ġprocess&#39;: 609, &#39;ĠJohn&#39;: 610, &#39;ch&#39;: 611, &#39;00&#39;: 612, &#39;Ġfinancial&#39;: 613, &#39;Ġlow&#39;: 614, &#39;Ġenough&#39;: 615, &#39;ĠWhile&#39;: 616, &#39;Ġfurther&#39;: 617, &#39;Ġpost&#39;: 618, &#39;Ġfeel&#39;: 619, &#39;st&#39;: 620, &#39;Ġperson&#39;: 621, &#39;ĠFacebook&#39;: 622, &#39;ĠWorld&#39;: 623, &#39;Ġwithin&#39;: 624, &#39;ad&#39;: 625, &#39;Ġdone&#39;: 626, &#39;the&#39;: 627, &#39;Ġlate&#39;: 628, &#39;Ġtax&#39;: 629, &#39;Ġdoesn&#39;: 630, &#39;Ġthing&#39;: 631, &#39;Ġnational&#39;: 632, &#39;Ġjob&#39;: 633, &#39;Ġusing&#39;: 634, &#39;ĠHowever&#39;: 635, &#39;ic&#39;: 636, &#39;Ġcampaign&#39;: 637, &#39;Ġrecord&#39;: 638, &#39;Ġbehind&#39;: 639, &#39;://&#39;: 640, &#39;ĠDepartment&#39;: 641, &#39;p&#39;: 642, &#39;Ġothers&#39;: 643, &#39;ĠJanuary&#39;: 644, &#39;Ġorder&#39;: 645, &#39;Ġ[&#39;: 646, &#39;Ġsales&#39;: 647, &#39;Ġyet&#39;: 648, &#39;Ä&#39;: 649, &#39;Ġsmall&#39;: 650, &#39;Ġseries&#39;: 651, &#39;Ġface&#39;: 652, &#39;ĠWhat&#39;: 653, &#39;Ġ50&#39;: 654, &#39;Ġever&#39;: 655, &#39;Ġearlier&#39;: 656, &#39;Ġlove&#39;: 657, &#39;up&#39;: 658, &#39;Ġrights&#39;: 659, &#39;ĠAn&#39;: 660, &#39;ist&#39;: 661, &#39;Ġmorning&#39;: 662, &#39;ĠWashington&#39;: 663, &#39;Ġyoung&#39;: 664, &#39;Ġlatest&#39;: 665, &#39;ĠIndia&#39;: 666, &#39;Ġtrying&#39;: 667, &#39;Ġfire&#39;: 668, &#39;Ġled&#39;: 669, &#39;Ġstrong&#39;: 670, &#39;Ġreturn&#39;: 671, &#39;Ġlevel&#39;: 672, &#39;O&#39;: 673, &#39;Ġaverage&#39;: 674, &#39;Ġperiod&#39;: 675, &#39;Ġexperience&#39;: 676, &#39;ak&#39;: 677, &#39;Ġpossible&#39;: 678, &#39;Ġbelieve&#39;: 679, &#39;Ġinclude&#39;: 680, &#39;Ġoil&#39;: 681, &#39;Ġrecently&#39;: 682, &#39;Ġonce&#39;: 683, &#39;Ġknown&#39;: 684, &#39;Ġlost&#39;: 685, &#39;Ġsure&#39;: 686, &#39;us&#39;: 687, &#39;Ġweeks&#39;: 688, &#39;Ġfood&#39;: 689, &#39;Ġreports&#39;: 690, &#39;Ġrating&#39;: 691, &#39;ĠMinister&#39;: 692, &#39;Ġwoman&#39;: 693, &#39;Ġprovide&#39;: 694, &#39;Ġproject&#39;: 695, &#39;Ġissue&#39;: 696, &#39;Ġlive&#39;: 697, &#39;10&#39;: 698, &#39;Ġclear&#39;: 699, &#39;he&#39;: 700, &#39;Ġcost&#39;: 701, &#39;Ġplayed&#39;: 702, &#39;Ġreleased&#39;: 703, &#39;Ġcoach&#39;: 704, &#39;v&#39;: 705, &#39;Ġ24&#39;: 706, &#39;Ġseven&#39;: 707, &#39;Ġplans&#39;: 708, &#39;Ġdevelopment&#39;: 709, &#39;ur&#39;: 710, &#39;ĺ&#39;: 711, &#39;Ġincrease&#39;: 712, &#39;This&#39;: 713, &#39;Ġpolicy&#39;: 714, &#39;Ġcent&#39;: 715, &#39;Ġbased&#39;: 716, &#39;E&#39;: 717, &#39;il&#39;: 718, &#39;ĠDecember&#39;: 719, &#39;Ġglobal&#39;: 720, &#39;Ġtrade&#39;: 721, &#39;Ġhours&#39;: 722, &#39;Ġhigher&#39;: 723, &#39;Ġgoal&#39;: 724, &#39;H&#39;: 725, &#39;ĠAl&#39;: 726, &#39;Ġ100&#39;: 727, &#39;Ġminutes&#39;: 728, &#39;Ġelection&#39;: 729, &#39;ĠAmerica&#39;: 730, &#39;Ġrate&#39;: 731, &#39;ĠCh&#39;: 732, &#39;Ġ21&#39;: 733, &#39;...&#39;: 734, &#39;ĠWhite&#39;: 735, &#39;Ġdirector&#39;: 736, &#39;Ġposition&#39;: 737, &#39;Ġshot&#39;: 738, &#39;Ġlarge&#39;: 739, &#39;Ġc&#39;: 740, &#39;Ġb&#39;: 741, &#39;]&#39;: 742, &#39;Ġissues&#39;: 743, &#39;Ġdeath&#39;: 744, &#39;Ġbuilding&#39;: 745, &#39;Ġtotal&#39;: 746, &#39;Ġoften&#39;: 747, &#39;Ġv&#39;: 748, &#39;Ġcountries&#39;: 749, &#39;Ġhistory&#39;: 750, &#39;Ġoutside&#39;: 751, &#39;Ġfederal&#39;: 752, &#39;Ġ19&#39;: 753, &#39;Ġfact&#39;: 754, &#39;ĠHigh&#39;: 755, &#39;Ġcareer&#39;: 756, &#39;im&#39;: 757, &#39;Ġinternational&#39;: 758, &#39;ĠNovember&#39;: 759, &#39;Ġfront&#39;: 760, &#39;Ġkind&#39;: 761, &#39;Ġkey&#39;: 762, &#39;ra&#39;: 763, &#39;ĠSan&#39;: 764, &#39;Ġshort&#39;: 765, &#39;Ġname&#39;: 766, &#39;ĠAccording&#39;: 767, &#39;Ġcourse&#39;: 768, &#39;Ġre&#39;: 769, &#39;Ġwanted&#39;: 770, &#39;W&#39;: 771, &#39;ĠSeptember&#39;: 772, &#39;Ġinterest&#39;: 773, &#39;Ġrole&#39;: 774, &#39;Ġresults&#39;: 775, &#39;Ġeconomic&#39;: 776, &#39;Ġ2014&#39;: 777, &#39;Ġchance&#39;: 778, &#39;ĠOctober&#39;: 779, &#39;Ġspecial&#39;: 780, &#39;Ġofficial&#39;: 781, &#39;Ġneeds&#39;: 782, &#39;um&#39;: 783, &#39;Ġl&#39;: 784, &#39;Ġproducts&#39;: 785, &#39;Ġnon&#39;: 786, &#39;Ġ@&#39;: 787, &#39;ĠBank&#39;: 788, &#39;Ġahead&#39;: 789, &#39;Ġhouse&#39;: 790, &#39;U&#39;: 791, &#39;Ġboard&#39;: 792, &#39;Ġold&#39;: 793, &#39;Ġsaw&#39;: 794, &#39;Ġlower&#39;: 795, &#39;ĠEuropean&#39;: 796, &#39;Ġcontrol&#39;: 797, &#39;ĠRussia&#39;: 798, &#39;Ġeight&#39;: 799, &#39;Ġrelease&#39;: 800, &#39;Ġpotential&#39;: 801, &#39;Ġthought&#39;: 802, &#39;Ġinvestigation&#39;: 803, &#39;Ġonline&#39;: 804, &#39;based&#39;: 805, &#39;Ġtechnology&#39;: 806, &#39;ĠDonald&#39;: 807, &#39;id&#39;: 808, &#39;Ġbody&#39;: 809, &#39;Ġrisk&#39;: 810, &#39;ian&#39;: 811, &#39;Ġcapital&#39;: 812, &#39;Ġstaff&#39;: 813, &#39;Ġaction&#39;: 814, &#39;ĠLeague&#39;: 815, &#39;Ġplaying&#39;: 816, &#39;Ġmakes&#39;: 817, &#39;Ġalmost&#39;: 818, &#39;Ġperformance&#39;: 819, &#39;Ġ22&#39;: 820, &#39;Ġg&#39;: 821, &#39;Ġfilm&#39;: 822, &#39;Ġnearly&#39;: 823, &#39;ĠCenter&#39;: 824, &#39;Ġvisit&#39;: 825, &#39;ĠGroup&#39;: 826, &#39;Ġbank&#39;: 827, &#39;Ġbit&#39;: 828, &#39;Ġreceived&#39;: 829, &#39;ĠAugust&#39;: 830, &#39;Ġmilitary&#39;: 831, &#39;ĠHis&#39;: 832, &#39;ine&#39;: 833, &#39;Ġchief&#39;: 834, &#39;ĠSchool&#39;: 835, &#39;Ġbring&#39;: 836, &#39;ĠCourt&#39;: 837, &#39;Ġ(@&#39;: 838, &#39;Ġmeans&#39;: 839, &#39;ĠSh&#39;: 840, &#39;Ġfans&#39;: 841, &#39;Ġse&#39;: 842, &#39;Ġ40&#39;: 843, &#39;20&#39;: 844, &#39;&#34;.&#39;: 845, &#39;V&#39;: 846, &#39;Ġcut&#39;: 847, &#39;Ġkilled&#39;: 848, &#39;Ġ#&#39;: 849, &#39;Ġprices&#39;: 850, &#39;Ġgave&#39;: 851, &#39;ĠStreet&#39;: 852, &#39;ir&#39;: 853, &#39;ĠY&#39;: 854, &#39;Ġcurrently&#39;: 855, &#39;Ġf&#39;: 856, &#39;ay&#39;: 857, &#39;ne&#39;: 858, &#39;te&#39;: 859, &#39;Ġtry&#39;: 860, &#39;ĠPark&#39;: 861, &#39;ĥ&#39;: 862, &#39;J&#39;: 863, &#39;Ġquestion&#39;: 864, &#39;Ġhand&#39;: 865, &#39;Ġeconomy&#39;: 866, &#39;Ġinvestors&#39;: 867, &#39;able&#39;: 868, &#39;Ġplayer&#39;: 869, &#39;ĠBy&#39;: 870, &#39;ĠDavid&#39;: 871, &#39;Ġloss&#39;: 872, &#39;ab&#39;: 873, &#39;Ġbelow&#39;: 874, &#39;Ġwrote&#39;: 875, &#39;co&#39;: 876, &#39;ate&#39;: 877, &#39;Ġrunning&#39;: 878, &#39;un&#39;: 879, &#39;Ġbegan&#39;: 880, &#39;Ġsingle&#39;: 881, &#39;Ġfield&#39;: 882, &#39;Ġ23&#39;: 883, &#39;Ġleader&#39;: 884, &#39;Ġw&#39;: 885, &#39;ĠCalifornia&#39;: 886, &#39;Ġfourth&#39;: 887, &#39;Ġactually&#39;: 888, &#39;Ġlist&#39;: 889, &#39;ll&#39;: 890, &#39;Ġcouple&#39;: 891, &#39;Ġstudy&#39;: 892, &#39;Ġteams&#39;: 893, &#39;He&#39;: 894, &#39;ah&#39;: 895, &#39;ĠCanada&#39;: 896, &#39;Ġla&#39;: 897, &#39;Ġresult&#39;: 898, &#39;Ġaccess&#39;: 899, &#39;Ġvote&#39;: 900, &#39;ĠMore&#39;: 901, &#39;ĠFebruary&#39;: 902, &#39;Ġrevenue&#39;: 903, &#39;Ġoffer&#39;: 904, &#39;Ġlet&#39;: 905, &#39;ier&#39;: 906, &#39;Ġbuy&#39;: 907, &#39;Ġattack&#39;: 908, &#39;Ġblack&#39;: 909, &#39;Ġr&#39;: 910, &#39;Ġareas&#39;: 911, &#39;Ġstop&#39;: 912, &#39;Ġimpact&#39;: 913, &#39;Ġmatch&#39;: 914, &#39;Ġinvestment&#39;: 915, &#39;Ġcustomers&#39;: 916, &#39;Ġleaders&#39;: 917, &#39;ies&#39;: 918, &#39;Ġmember&#39;: 919, &#39;Ġchild&#39;: 920, &#39;Ġroad&#39;: 921, &#39;ul&#39;: 922, &#39;Ġvalue&#39;: 923, &#39;Ġshows&#39;: 924, &#39;ĠDr&#39;: 925, &#39;ĠDe&#39;: 926, &#39;ant&#39;: 927, &#39;ĠLondon&#39;: 928, &#39;Ġroom&#39;: 929, &#39;Ġmusic&#39;: 930, &#39;Ġproduction&#39;: 931, &#39;Ġanything&#39;: 932, &#39;Ġfirm&#39;: 933, &#39;Ġbiggest&#39;: 934, &#39;Ġair&#39;: 935, &#39;Ġproblem&#39;: 936, &#39;Ġgeneral&#39;: 937, &#39;Ġwasn&#39;: 938, &#39;Ġi&#39;: 939, &#39;Ġprivate&#39;: 940, &#39;Ġespecially&#39;: 941, &#39;Ġadministration&#39;: 942, &#39;Ġadditional&#39;: 943, &#39;ĠCo&#39;: 944, &#39;Ġopportunity&#39;: 945, &#39;Ġhold&#39;: 946, &#39;&amp;&#39;: 947, &#39;Ġmatter&#39;: 948, &#39;Ġsenior&#39;: 949, &#39;Ġclub&#39;: 950, &#39;Ġsomeone&#39;: 951, &#39;ĠÃ&#39;: 952, &#39;ĠEast&#39;: 953, &#39;Ġ2019&#39;: 954, &#34;.&#39;&#34;: 955, &#39;Ġneeded&#39;: 956, &#39;ĠJames&#39;: 957, &#39;time&#39;: 958, &#39;Ġhowever&#39;: 959, &#39;Ġeverything&#39;: 960, &#39;Ġeveryone&#39;: 961, &#39;Ġdied&#39;: 962, &#39;Ġinvolved&#39;: 963, &#39;Ġfriends&#39;: 964, &#39;Ġisn&#39;: 965, &#39;Ġworth&#39;: 966, &#39;ik&#39;: 967, &#39;ĠCup&#39;: 968, &#39;Ġshowed&#39;: 969, &#39;There&#39;: 970, &#39;Ġ28&#39;: 971, &#39;Ġmeet&#39;: 972, &#39;Ġ26&#39;: 973, &#39;Ġ27&#39;: 974, &#39;Y&#39;: 975, &#39;Ġregion&#39;: 976, &#39;ĠPress&#39;: 977, &#39;ĠNow&#39;: 978, &#39;Ġson&#39;: 979, &#39;Ġspace&#39;: 980, &#39;Ġleading&#39;: 981, &#39;Ġstates&#39;: 982, &#39;Ġweekend&#39;: 983, &#39;ĠÂ£&#39;: 984, &#39;Ġmother&#39;: 985, &#39;Ġprevious&#39;: 986, &#39;ĠUK&#39;: 987, &#39;ĠMichael&#39;: 988, &#39;Ġleave&#39;: 989, &#39;est&#39;: 990, &#39;em&#39;: 991, &#39;Ġz&#39;: 992, &#39;ĠSome&#39;: 993, &#39;ors&#39;: 994, &#39;out&#39;: 995, &#39;15&#39;: 996, &#39;Ġwar&#39;: 997, &#39;Ġwebsite&#39;: 998, &#39;Ġstar&#39;: 999, ...} . Any word not present in the vocabulary will be split into tokens. . #__call__ method Returns a dictionary containing the encoded sequence or sequence pair and additional informationReturns a dictionary containing the encoded sequence or sequence pair and additional information #tokenize the sentence using __call__ method of the tokenizer inputs = tokenizer(&quot;&quot;&quot;Today, Sachin won&#39;t be playing in the cricket match between India and Pakistan.&quot;&quot;&quot;, return_tensors=&#39;pt&#39;) inputs . {&#39;input_ids&#39;: tensor([[ 0, 5625, 6, 19686, 179, 351, 75, 28, 816, 11, 5, 5630, 914, 227, 666, 8, 1752, 4, 2]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} . Let&#39;s decode the token id&#39;s to see the token text . [tokenizer.decode(i) for i in inputs[&#39;input_ids&#39;][0].tolist()] . [&#39;&lt;s&gt;&#39;, &#39;Today&#39;, &#39;,&#39;, &#39; Sach&#39;, &#39;in&#39;, &#39; won&#39;, &#34;&#39;t&#34;, &#39; be&#39;, &#39; playing&#39;, &#39; in&#39;, &#39; the&#39;, &#39; cricket&#39;, &#39; match&#39;, &#39; between&#39;, &#39; India&#39;, &#39; and&#39;, &#39; Pakistan&#39;, &#39;.&#39;, &#39; Sach&#39;, &#39;in&#39;, &#39; wil&#39;, &#39;&lt;/s&gt;&#39;] . The word Sachin is split into [&#39;Sach,&#39;in&#39;] tokens. This method of splitting a word into tokens helps in handling out of vocabulary words. . Special Tokens . Roberta uses special token &lt;s&gt; to mark sentence boundaries. While BERT has special tokens like [CLS] [SEP] . [CLS] provides the vector representation for the whole sentence and [SEP] token is useful when we have to encode two sentences. . Tokenize a sentence . There are a lot of tokenizer functions available in the transformers library. It can get really confusing as to which function to use and when. . __call__ method is a wrapper around the tokenizer.tokenize method. It adds special tokens specific to the model and returns additional fields like token_type_ids and attention_masks . tokenize method splits the sentence into tokens without adding any special tokens. . convert_tokens_to_ids is called after converting a string into tokens, to convert the tokens into vocabulary token ids . tokens = tokenizer.tokenize(&quot;I don&#39;t like it here&quot;) tokens . [&#39;I&#39;, &#39;Ġdon&#39;, &#34;&#39;t&#34;, &#39;Ġlike&#39;, &#39;Ġit&#39;, &#39;Ġhere&#39;] . tokenizer.convert_tokens_to_ids(tokens) . [100, 218, 75, 101, 24, 259] . tokenizer.build_inputs_with_special_tokens(tokens) . [0, &#39;I&#39;, &#39;Ġdon&#39;, &#34;&#39;t&#34;, &#39;Ġlike&#39;, &#39;Ġit&#39;, &#39;Ġhere&#39;, 2] . use the __call__ method to get the token_ids with extra inputs required by the specific model. . tokenizer(&quot;I don&#39;t like it here&quot;) . {&#39;input_ids&#39;: [2, 31, 221, 22, 38, 101, 32, 235, 3], &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]} . Before trying to understand the attention_mask and token_type_ids return values. Let us first look at the inputs required by a transformers model. . Model Inputs . The base RoBERTa Model provides the raw hidden-states without any specific head on top. . The forward method of the Model has following input parameters . def forward( self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None, return_dict=None, ):input_ids - The input ids are often the only required parameters to be passed to the model as input. These are numerical representation of the tokens the one which we got in the previous step using tokenizer call method. attention_mask - A binary value representing whether to attend to the corresponding input_id token or not. It is useful when we have different length sentences in a batch and we need to pad all sentences to the same length. We ignore the padding index input_id using attention mask. . token_type_id - Useful when encoding multiple sentences. . The tokenizers __call__ method handles everything to prepare the input for the model. It does more than just encoding the sentence into numeric representation. . Model Inference / Output . Run the model for the input sentence and get the embedding for each token . output = model(**inputs) . output[0].shape #(batch_size, #tokens, embedding_dim) . torch.Size([1, 19, 768]) . We provided a single sentence as input to the model, hence the batch_size of 1. . The tokenizer converted the sentence into 19 tokens, including the special tokens then the model converted each token into a vector representation of 768 dimension, hence [19,768] . output[1].shape #pooled output - the vector representation of a whole sentence . torch.Size([1, 768]) . output = model(input_ids= inputs[&#39;input_ids&#39;], attention_mask=inputs[&#39;attention_mask&#39;] , return_dict=True) . if not return_dict:return (sequence_output, pooled_output) + encoder_outputs[1:] return BaseModelOutputWithPooling( last_hidden_state=sequence_output, pooler_output=pooled_output, hidden_states=encoder_outputs.hidden_states, attentions=encoder_outputs.attentions, ) . To return hiddden states and attentions, we have to set output_attentions and output_hidden_states in the model call . output = model(input_ids= inputs[&#39;input_ids&#39;], attention_mask=inputs[&#39;attention_mask&#39;] , return_dict=True, output_hidden_states=True, output_attentions= True) . len(output.attentions) . 12 . output.attentions[0].shape . torch.Size([1, 12, 19, 19]) . Token Classification(TC) Head for Entity Extraction . What we have learned so far - . How to tokenize a sentence using model specific tokenizer | How to get contextual embeddings using the Pretrained Model | . The Token classification task assigns a label to each token in the sentence. . For example in the sentence . &quot;The battery of the speaker is very poor.&quot; TC head will map each token embedding to a label like . The O | battery Aspect | of O | the O | speaker O | is O | very Sentiment | poor Sentiment. | . Here, we wanted to extract the Aspect and the Sentiment in the text. O is Others. . RoBerta Token Classification Head . Since we have the embedding vector for each token, if we add a linear head on top of the embedding to map each embedding to one of the labels, we will get a label for each token. . hidden_size of Roberta is 786, and depending on the num_labels(the entities of interest) it will do the token classification . From RobertaTokenClassification code . self.classifier = nn.Linear(config.hidden_size, self.config.num_labels) self.dropout = nn.Dropout(config.hidden_dropout_prob) sequence_output = self.dropout(sequence_output) logits = self.classifier(sequence_output) . (N, T, 786) -&gt; (N, T, num_labels) . N - batch_size T - Number of tokens . . Token Classification Output . During Training, we will provide labels along with the input_ids to the model. The model calculates the CrossEntropy Loss of the predicted labels and the ground truth and the weights are updated with the gradient. . Task Head will return the loss if labels are provided as part of the model input . output = (logits,) + outputs[2:]return ((loss,) + output) if loss is not None else output . loss is the cross entropy loss between the labels and the logits . loss_fct = CrossEntropyLoss() loss_fct(logits.view(-1, self.num_labels), labels.view(-1)) . from transformers import AutoModelForTokenClassification,RobertaForTokenClassification . Let us run Token Classification model Prediction without any finetuning to see the output of the model . from transformers import AutoConfig config = AutoConfig.from_pretrained( &#39;roberta-base&#39;, num_labels=3, id2label={0:&#39;ASPECT&#39;,1:&#39;OTHERS&#39;,2:&#39;EXTRA&#39;}, label2id={label: i for i, label in enumerate([&#39;ASPECT&#39;,&#39;OTHERS&#39;,&#39;EXTRA&#39;])} ) . config . RobertaConfig { &#34;architectures&#34;: [ &#34;RobertaForMaskedLM&#34; ], &#34;attention_probs_dropout_prob&#34;: 0.1, &#34;bos_token_id&#34;: 0, &#34;eos_token_id&#34;: 2, &#34;gradient_checkpointing&#34;: false, &#34;hidden_act&#34;: &#34;gelu&#34;, &#34;hidden_dropout_prob&#34;: 0.1, &#34;hidden_size&#34;: 768, &#34;id2label&#34;: { &#34;0&#34;: &#34;ASPECT&#34;, &#34;1&#34;: &#34;OTHERS&#34;, &#34;2&#34;: &#34;EXTRA&#34; }, &#34;initializer_range&#34;: 0.02, &#34;intermediate_size&#34;: 3072, &#34;label2id&#34;: { &#34;ASPECT&#34;: 0, &#34;EXTRA&#34;: 2, &#34;OTHERS&#34;: 1 }, &#34;layer_norm_eps&#34;: 1e-05, &#34;max_position_embeddings&#34;: 514, &#34;model_type&#34;: &#34;roberta&#34;, &#34;num_attention_heads&#34;: 12, &#34;num_hidden_layers&#34;: 12, &#34;pad_token_id&#34;: 1, &#34;type_vocab_size&#34;: 1, &#34;vocab_size&#34;: 50265 } . model = RobertaForTokenClassification.from_pretrained(&#39;albert-base-v2&#39;, config = config) . Some weights of the model checkpoint at albert-base-v2 were not used when initializing RobertaForTokenClassification: [&#39;albert.embeddings.word_embeddings.weight&#39;, &#39;albert.embeddings.position_embeddings.weight&#39;, &#39;albert.embeddings.token_type_embeddings.weight&#39;, &#39;albert.embeddings.LayerNorm.weight&#39;, &#39;albert.embeddings.LayerNorm.bias&#39;, &#39;albert.encoder.embedding_hidden_mapping_in.weight&#39;, &#39;albert.encoder.embedding_hidden_mapping_in.bias&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight&#39;, &#39;albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias&#39;, &#39;albert.pooler.weight&#39;, &#39;albert.pooler.bias&#39;, &#39;predictions.bias&#39;, &#39;predictions.LayerNorm.weight&#39;, &#39;predictions.LayerNorm.bias&#39;, &#39;predictions.dense.weight&#39;, &#39;predictions.dense.bias&#39;, &#39;predictions.decoder.weight&#39;, &#39;predictions.decoder.bias&#39;] - This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: [&#39;embeddings.word_embeddings.weight&#39;, &#39;embeddings.position_embeddings.weight&#39;, &#39;embeddings.token_type_embeddings.weight&#39;, &#39;embeddings.LayerNorm.weight&#39;, &#39;embeddings.LayerNorm.bias&#39;, &#39;encoder.layer.0.attention.self.query.weight&#39;, &#39;encoder.layer.0.attention.self.query.bias&#39;, &#39;encoder.layer.0.attention.self.key.weight&#39;, &#39;encoder.layer.0.attention.self.key.bias&#39;, &#39;encoder.layer.0.attention.self.value.weight&#39;, &#39;encoder.layer.0.attention.self.value.bias&#39;, &#39;encoder.layer.0.attention.output.dense.weight&#39;, &#39;encoder.layer.0.attention.output.dense.bias&#39;, &#39;encoder.layer.0.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.0.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.0.intermediate.dense.weight&#39;, &#39;encoder.layer.0.intermediate.dense.bias&#39;, &#39;encoder.layer.0.output.dense.weight&#39;, &#39;encoder.layer.0.output.dense.bias&#39;, &#39;encoder.layer.0.output.LayerNorm.weight&#39;, &#39;encoder.layer.0.output.LayerNorm.bias&#39;, &#39;encoder.layer.1.attention.self.query.weight&#39;, &#39;encoder.layer.1.attention.self.query.bias&#39;, &#39;encoder.layer.1.attention.self.key.weight&#39;, &#39;encoder.layer.1.attention.self.key.bias&#39;, &#39;encoder.layer.1.attention.self.value.weight&#39;, &#39;encoder.layer.1.attention.self.value.bias&#39;, &#39;encoder.layer.1.attention.output.dense.weight&#39;, &#39;encoder.layer.1.attention.output.dense.bias&#39;, &#39;encoder.layer.1.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.1.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.1.intermediate.dense.weight&#39;, &#39;encoder.layer.1.intermediate.dense.bias&#39;, &#39;encoder.layer.1.output.dense.weight&#39;, &#39;encoder.layer.1.output.dense.bias&#39;, &#39;encoder.layer.1.output.LayerNorm.weight&#39;, &#39;encoder.layer.1.output.LayerNorm.bias&#39;, &#39;encoder.layer.2.attention.self.query.weight&#39;, &#39;encoder.layer.2.attention.self.query.bias&#39;, &#39;encoder.layer.2.attention.self.key.weight&#39;, &#39;encoder.layer.2.attention.self.key.bias&#39;, &#39;encoder.layer.2.attention.self.value.weight&#39;, &#39;encoder.layer.2.attention.self.value.bias&#39;, &#39;encoder.layer.2.attention.output.dense.weight&#39;, &#39;encoder.layer.2.attention.output.dense.bias&#39;, &#39;encoder.layer.2.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.2.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.2.intermediate.dense.weight&#39;, &#39;encoder.layer.2.intermediate.dense.bias&#39;, &#39;encoder.layer.2.output.dense.weight&#39;, &#39;encoder.layer.2.output.dense.bias&#39;, &#39;encoder.layer.2.output.LayerNorm.weight&#39;, &#39;encoder.layer.2.output.LayerNorm.bias&#39;, &#39;encoder.layer.3.attention.self.query.weight&#39;, &#39;encoder.layer.3.attention.self.query.bias&#39;, &#39;encoder.layer.3.attention.self.key.weight&#39;, &#39;encoder.layer.3.attention.self.key.bias&#39;, &#39;encoder.layer.3.attention.self.value.weight&#39;, &#39;encoder.layer.3.attention.self.value.bias&#39;, &#39;encoder.layer.3.attention.output.dense.weight&#39;, &#39;encoder.layer.3.attention.output.dense.bias&#39;, &#39;encoder.layer.3.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.3.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.3.intermediate.dense.weight&#39;, &#39;encoder.layer.3.intermediate.dense.bias&#39;, &#39;encoder.layer.3.output.dense.weight&#39;, &#39;encoder.layer.3.output.dense.bias&#39;, &#39;encoder.layer.3.output.LayerNorm.weight&#39;, &#39;encoder.layer.3.output.LayerNorm.bias&#39;, &#39;encoder.layer.4.attention.self.query.weight&#39;, &#39;encoder.layer.4.attention.self.query.bias&#39;, &#39;encoder.layer.4.attention.self.key.weight&#39;, &#39;encoder.layer.4.attention.self.key.bias&#39;, &#39;encoder.layer.4.attention.self.value.weight&#39;, &#39;encoder.layer.4.attention.self.value.bias&#39;, &#39;encoder.layer.4.attention.output.dense.weight&#39;, &#39;encoder.layer.4.attention.output.dense.bias&#39;, &#39;encoder.layer.4.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.4.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.4.intermediate.dense.weight&#39;, &#39;encoder.layer.4.intermediate.dense.bias&#39;, &#39;encoder.layer.4.output.dense.weight&#39;, &#39;encoder.layer.4.output.dense.bias&#39;, &#39;encoder.layer.4.output.LayerNorm.weight&#39;, &#39;encoder.layer.4.output.LayerNorm.bias&#39;, &#39;encoder.layer.5.attention.self.query.weight&#39;, &#39;encoder.layer.5.attention.self.query.bias&#39;, &#39;encoder.layer.5.attention.self.key.weight&#39;, &#39;encoder.layer.5.attention.self.key.bias&#39;, &#39;encoder.layer.5.attention.self.value.weight&#39;, &#39;encoder.layer.5.attention.self.value.bias&#39;, &#39;encoder.layer.5.attention.output.dense.weight&#39;, &#39;encoder.layer.5.attention.output.dense.bias&#39;, &#39;encoder.layer.5.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.5.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.5.intermediate.dense.weight&#39;, &#39;encoder.layer.5.intermediate.dense.bias&#39;, &#39;encoder.layer.5.output.dense.weight&#39;, &#39;encoder.layer.5.output.dense.bias&#39;, &#39;encoder.layer.5.output.LayerNorm.weight&#39;, &#39;encoder.layer.5.output.LayerNorm.bias&#39;, &#39;encoder.layer.6.attention.self.query.weight&#39;, &#39;encoder.layer.6.attention.self.query.bias&#39;, &#39;encoder.layer.6.attention.self.key.weight&#39;, &#39;encoder.layer.6.attention.self.key.bias&#39;, &#39;encoder.layer.6.attention.self.value.weight&#39;, &#39;encoder.layer.6.attention.self.value.bias&#39;, &#39;encoder.layer.6.attention.output.dense.weight&#39;, &#39;encoder.layer.6.attention.output.dense.bias&#39;, &#39;encoder.layer.6.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.6.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.6.intermediate.dense.weight&#39;, &#39;encoder.layer.6.intermediate.dense.bias&#39;, &#39;encoder.layer.6.output.dense.weight&#39;, &#39;encoder.layer.6.output.dense.bias&#39;, &#39;encoder.layer.6.output.LayerNorm.weight&#39;, &#39;encoder.layer.6.output.LayerNorm.bias&#39;, &#39;encoder.layer.7.attention.self.query.weight&#39;, &#39;encoder.layer.7.attention.self.query.bias&#39;, &#39;encoder.layer.7.attention.self.key.weight&#39;, &#39;encoder.layer.7.attention.self.key.bias&#39;, &#39;encoder.layer.7.attention.self.value.weight&#39;, &#39;encoder.layer.7.attention.self.value.bias&#39;, &#39;encoder.layer.7.attention.output.dense.weight&#39;, &#39;encoder.layer.7.attention.output.dense.bias&#39;, &#39;encoder.layer.7.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.7.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.7.intermediate.dense.weight&#39;, &#39;encoder.layer.7.intermediate.dense.bias&#39;, &#39;encoder.layer.7.output.dense.weight&#39;, &#39;encoder.layer.7.output.dense.bias&#39;, &#39;encoder.layer.7.output.LayerNorm.weight&#39;, &#39;encoder.layer.7.output.LayerNorm.bias&#39;, &#39;encoder.layer.8.attention.self.query.weight&#39;, &#39;encoder.layer.8.attention.self.query.bias&#39;, &#39;encoder.layer.8.attention.self.key.weight&#39;, &#39;encoder.layer.8.attention.self.key.bias&#39;, &#39;encoder.layer.8.attention.self.value.weight&#39;, &#39;encoder.layer.8.attention.self.value.bias&#39;, &#39;encoder.layer.8.attention.output.dense.weight&#39;, &#39;encoder.layer.8.attention.output.dense.bias&#39;, &#39;encoder.layer.8.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.8.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.8.intermediate.dense.weight&#39;, &#39;encoder.layer.8.intermediate.dense.bias&#39;, &#39;encoder.layer.8.output.dense.weight&#39;, &#39;encoder.layer.8.output.dense.bias&#39;, &#39;encoder.layer.8.output.LayerNorm.weight&#39;, &#39;encoder.layer.8.output.LayerNorm.bias&#39;, &#39;encoder.layer.9.attention.self.query.weight&#39;, &#39;encoder.layer.9.attention.self.query.bias&#39;, &#39;encoder.layer.9.attention.self.key.weight&#39;, &#39;encoder.layer.9.attention.self.key.bias&#39;, &#39;encoder.layer.9.attention.self.value.weight&#39;, &#39;encoder.layer.9.attention.self.value.bias&#39;, &#39;encoder.layer.9.attention.output.dense.weight&#39;, &#39;encoder.layer.9.attention.output.dense.bias&#39;, &#39;encoder.layer.9.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.9.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.9.intermediate.dense.weight&#39;, &#39;encoder.layer.9.intermediate.dense.bias&#39;, &#39;encoder.layer.9.output.dense.weight&#39;, &#39;encoder.layer.9.output.dense.bias&#39;, &#39;encoder.layer.9.output.LayerNorm.weight&#39;, &#39;encoder.layer.9.output.LayerNorm.bias&#39;, &#39;encoder.layer.10.attention.self.query.weight&#39;, &#39;encoder.layer.10.attention.self.query.bias&#39;, &#39;encoder.layer.10.attention.self.key.weight&#39;, &#39;encoder.layer.10.attention.self.key.bias&#39;, &#39;encoder.layer.10.attention.self.value.weight&#39;, &#39;encoder.layer.10.attention.self.value.bias&#39;, &#39;encoder.layer.10.attention.output.dense.weight&#39;, &#39;encoder.layer.10.attention.output.dense.bias&#39;, &#39;encoder.layer.10.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.10.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.10.intermediate.dense.weight&#39;, &#39;encoder.layer.10.intermediate.dense.bias&#39;, &#39;encoder.layer.10.output.dense.weight&#39;, &#39;encoder.layer.10.output.dense.bias&#39;, &#39;encoder.layer.10.output.LayerNorm.weight&#39;, &#39;encoder.layer.10.output.LayerNorm.bias&#39;, &#39;encoder.layer.11.attention.self.query.weight&#39;, &#39;encoder.layer.11.attention.self.query.bias&#39;, &#39;encoder.layer.11.attention.self.key.weight&#39;, &#39;encoder.layer.11.attention.self.key.bias&#39;, &#39;encoder.layer.11.attention.self.value.weight&#39;, &#39;encoder.layer.11.attention.self.value.bias&#39;, &#39;encoder.layer.11.attention.output.dense.weight&#39;, &#39;encoder.layer.11.attention.output.dense.bias&#39;, &#39;encoder.layer.11.attention.output.LayerNorm.weight&#39;, &#39;encoder.layer.11.attention.output.LayerNorm.bias&#39;, &#39;encoder.layer.11.intermediate.dense.weight&#39;, &#39;encoder.layer.11.intermediate.dense.bias&#39;, &#39;encoder.layer.11.output.dense.weight&#39;, &#39;encoder.layer.11.output.dense.bias&#39;, &#39;encoder.layer.11.output.LayerNorm.weight&#39;, &#39;encoder.layer.11.output.LayerNorm.bias&#39;, &#39;pooler.dense.weight&#39;, &#39;pooler.dense.bias&#39;, &#39;classifier.bias&#39;, &#39;classifier.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . output = model(**inputs) output . (tensor([[[ 8.0052e-01, -7.5425e-02, 4.5114e-01], [ 8.4571e-01, -2.9972e-01, 1.7145e-01], [ 9.3446e-01, -7.4490e-01, 1.6924e-01], [ 2.6570e-01, 1.0174e-01, 5.2277e-01], [ 4.5587e-01, -1.6770e-01, 8.7134e-01], [ 1.2516e+00, -6.7618e-01, 7.1735e-01], [ 7.4633e-01, 4.1005e-01, 4.7558e-01], [ 5.2609e-01, -9.0839e-02, 5.9321e-01], [ 8.0308e-01, -2.6667e-01, 1.1635e+00], [ 7.8067e-01, 9.4608e-02, 4.2717e-02], [ 6.4042e-01, -1.1400e-01, 1.8376e-01], [ 2.9740e-01, -3.3065e-01, 7.9212e-01], [ 8.3010e-01, -6.5126e-01, 5.6326e-01], [ 4.0874e-01, -1.6919e-01, 1.2946e+00], [ 9.3214e-01, 2.3070e-01, 3.4809e-01], [ 9.2671e-01, -4.8473e-01, 3.4551e-01], [ 5.1119e-01, -2.8516e-01, 2.6419e-01], [ 6.8518e-01, -6.7281e-01, 6.1023e-01], [ 6.1454e-01, -1.0960e-03, 2.6023e-01]]], grad_fn=&lt;AddBackward0&gt;),) . output[0].shape . torch.Size([1, 19, 3]) . This outputs a 3-label probabilities for each token. . During training, we will provide the labels also as input to the model which will calculate the CorssEntropy loss. . labels = torch.randint(0,3,(19,)) . outputs = model(input_ids = inputs[&#39;input_ids&#39;],attention_mask= inputs[&#39;attention_mask&#39;], labels = labels) loss = outputs[0] . In the next Part, we will discuss about finetuning the model for Entity Extraction Task . Fine Tuning for Token Classification . Steps for Finetuning TC Model . Dataset - Get labelled dataset - We need word level labels for training the TC model. | Implement TokenClassificationTask class to read external data files and to convert it into Features(model input_ids, token_type_ids, attention_mask) | create a pytorch dataset from list of features - TokenClassificationDataset | | We will follow the huggingface run_new.py script for finetuning the model . token-classification/utils_ner.py . An Example is a sentence(words) and corresponding labels. InputExample represents single example . We can&#39;t directly feed the words and labels into model as we have seen earlier we have to convert it into - (input_ids, token_type_ids, attention_mask, label). For this purpose we have InputFeatures class which holds the numeric repsentation of each Example. . Implement TokenClassificationTask class to read external data files and to convert it into Features. . Convert Input Data to Features . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . #install the latest transformers library !pip install git+https://github.com/huggingface/transformers.git !pip install seqeval !pip install conllu . Collecting git+https://github.com/huggingface/transformers.git Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-mbrpt47v Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-mbrpt47v Requirement already satisfied (use --upgrade to upgrade): transformers==3.1.0 from git+https://github.com/huggingface/transformers.git in /usr/local/lib/python3.6/dist-packages Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (1.18.5) Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.8.1rc2) Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (20.4) Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (3.0.12) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2.23.0) Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (4.41.1) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (2019.12.20) Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.1.91) Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.0.43) Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==3.1.0) (0.7) Requirement already satisfied: pyparsing&gt;=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers==3.1.0) (2.4.7) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging-&gt;transformers==3.1.0) (1.15.0) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (2020.6.20) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;transformers==3.1.0) (3.0.4) Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.1.0) (0.16.0) Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-&gt;transformers==3.1.0) (7.1.2) Building wheels for collected packages: transformers Building wheel for transformers (setup.py) ... done Created wheel for transformers: filename=transformers-3.1.0-cp36-none-any.whl size=996650 sha256=80fc5ec20593b8261c9e813530103d1288401a0c17a16272ed8be70fb5ebb61c Stored in directory: /tmp/pip-ephem-wheel-cache-k909pec1/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f Successfully built transformers Collecting seqeval Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz Requirement already satisfied: numpy&gt;=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5) Requirement already satisfied: Keras&gt;=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3) Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras&gt;=2.2.4-&gt;seqeval) (2.10.0) Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras&gt;=2.2.4-&gt;seqeval) (3.13) Requirement already satisfied: scipy&gt;=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras&gt;=2.2.4-&gt;seqeval) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py-&gt;Keras&gt;=2.2.4-&gt;seqeval) (1.15.0) Building wheels for collected packages: seqeval Building wheel for seqeval (setup.py) ... done Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=7cef3c723aa1ae19e26a16657fdc8dfaa9b33f406ab2334b97284b009aa29f2c Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68 Successfully built seqeval Installing collected packages: seqeval Successfully installed seqeval-0.0.12 Collecting conllu Downloading https://files.pythonhosted.org/packages/89/02/e5a55ac2c88166996d13871217206ef0f022479354b39a3ab47aad538a5d/conllu-4.2-py2.py3-none-any.whl Installing collected packages: conllu Successfully installed conllu-4.2 . Prepare Training Data . # This is formatted as code . Download the dataset from https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus . import pandas as pd data_path = &#39;/content/drive/My Drive/transformers-ner/ner_dataset.csv&#39; df = pd.read_csv(data_path, encoding=&quot;latin-1&quot;) df.loc[:, &quot;Sentence #&quot;] = df[&quot;Sentence #&quot;].fillna(method=&quot;ffill&quot;) . df.head() . Sentence # Word POS Tag . 0 Sentence: 1 | Thousands | NNS | O | . 1 Sentence: 1 | of | IN | O | . 2 Sentence: 1 | demonstrators | NNS | O | . 3 Sentence: 1 | have | VBP | O | . 4 Sentence: 1 | marched | VBN | O | . with open(&#39;/content/drive/My Drive/transformers-ner/data/labels.txt&#39;,&#39;w&#39;) as f: for tag in df[&#39;Tag&#39;].unique(): print(tag) f.write(str(tag)+&#39; n&#39;) . O B-geo B-gpe B-per I-geo B-org I-org B-tim B-art I-art I-per I-gpe I-tim B-nat B-eve I-eve I-nat . sentences = df.groupby(&quot;Sentence #&quot;)[&quot;Word&quot;].apply(list).values tags = df.groupby(&quot;Sentence #&quot;)[&quot;Tag&quot;].apply(list).values . Train Test Split . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(sentences, tags, test_size=0.2, random_state=42) . with open(&#39;/content/drive/My Drive/transformers-ner/data/train.txt&#39;,&#39;w&#39;) as ftrain: for (k,v) in zip(X_train, y_train): [ftrain.write(s+&#39; &#39;+t+&#39; n&#39;) for s,t in zip(k,v)] ftrain.write(&#39; n&#39;) with open(&#39;/content/drive/My Drive/transformers-ner/data/test.txt&#39;,&#39;w&#39;) as ftest: for (k,v) in zip(X_test, y_test): [ftest.write(s+&#39; &#39;+str(t)+&#39; n&#39;) for s,t in zip(k,v)] ftest.write(&#39; n&#39;) . Training Code . !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/utils_ner.py !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/run_ner.py !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/tasks.py . --2020-09-19 18:40:23-- https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/utils_ner.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 15629 (15K) [text/plain] Saving to: ‘utils_ner.py’ utils_ner.py 100%[===================&gt;] 15.26K --.-KB/s in 0.004s 2020-09-19 18:40:23 (3.98 MB/s) - ‘utils_ner.py’ saved [15629/15629] --2020-09-19 18:40:24-- https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/run_ner.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 11596 (11K) [text/plain] Saving to: ‘run_ner.py’ run_ner.py 100%[===================&gt;] 11.32K --.-KB/s in 0s 2020-09-19 18:40:24 (84.2 MB/s) - ‘run_ner.py’ saved [11596/11596] --2020-09-19 18:40:24-- https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/tasks.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 5509 (5.4K) [text/plain] Saving to: ‘tasks.py’ tasks.py 100%[===================&gt;] 5.38K --.-KB/s in 0s 2020-09-19 18:40:24 (81.6 MB/s) - ‘tasks.py’ saved [5509/5509] . from tasks import NER task = NER() . examples = task.read_examples_from_file(&#39;/content/drive/My Drive/transformers-ner/data&#39;,mode= &#39;train&#39;) . examples[:3] . [InputExample(guid=&#39;train-1&#39;, words=[&#39;South&#39;, &#39;Korea&#39;, &#34;&#39;s&#34;, &#39;government&#39;, &#39;Tuesday&#39;, &#39;also&#39;, &#39;unveiled&#39;, &#39;a&#39;, &#39;so-called&#39;, &#39;Green&#39;, &#39;New&#39;, &#39;Job&#39;, &#39;Creation&#39;, &#39;Plan&#39;, &#39;,&#39;, &#39;expected&#39;, &#39;to&#39;, &#39;create&#39;, &#39;9,60,000&#39;, &#39;new&#39;, &#39;jobs&#39;, &#39;.&#39;], labels=[&#39;B-geo&#39;, &#39;I-geo&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-tim&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;]), InputExample(guid=&#39;train-2&#39;, words=[&#39;When&#39;, &#39;the&#39;, &#39;Lion&#39;, &#39;found&#39;, &#39;that&#39;, &#39;he&#39;, &#39;could&#39;, &#39;not&#39;, &#39;escape&#39;, &#39;,&#39;, &#39;he&#39;, &#39;flew&#39;, &#39;upon&#39;, &#39;the&#39;, &#39;sheep&#39;, &#39;and&#39;, &#39;killed&#39;, &#39;them&#39;, &#39;,&#39;, &#39;and&#39;, &#39;then&#39;, &#39;attacked&#39;, &#39;the&#39;, &#39;oxen&#39;, &#39;.&#39;], labels=[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;]), InputExample(guid=&#39;train-3&#39;, words=[&#39;The&#39;, &#39;cost&#39;, &#39;of&#39;, &#39;major&#39;, &#39;food&#39;, &#39;commodities&#39;, &#39;has&#39;, &#39;doubled&#39;, &#39;over&#39;, &#39;the&#39;, &#39;past&#39;, &#39;two&#39;, &#39;years&#39;, &#39;with&#39;, &#39;rice&#39;, &#39;,&#39;, &#39;corn&#39;, &#39;and&#39;, &#39;wheat&#39;, &#39;at&#39;, &#39;record&#39;, &#39;highs&#39;, &#39;.&#39;], labels=[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-tim&#39;, &#39;I-tim&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;])] . !python run_ner.py --model_name_or_path &#39;roberta-base&#39; --labels &#39;/content/drive/My Drive/transformers-ner/data/labels.txt&#39; --data_dir &#39;/content/drive/My Drive/transformers-ner/data&#39; --output_dir &#39;model&#39; --max_seq_length &#39;128&#39; --num_train_epochs 3 --per_device_train_batch_size 8 --save_steps 1000000 --seed 16 --do_train --do_predict --overwrite_output_dir --fp16 . 2020-09-19 21:08:45.932334: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1 09/19/2020 21:08:47 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True 09/19/2020 21:08:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(output_dir=&#39;model&#39;, overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=True, evaluate_during_training=False, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=&#39;runs/Sep19_21-08-47_b791624ba0ff&#39;, logging_first_step=False, logging_steps=500, save_steps=1000000, save_total_limit=None, no_cuda=False, seed=16, fp16=True, fp16_opt_level=&#39;O1&#39;, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None) Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;] - This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. 09/19/2020 21:08:55 - INFO - filelock - Lock 139766042433912 acquired on /content/drive/My Drive/transformers-ner/data/cached_train_RobertaTokenizer_128.lock 09/19/2020 21:08:55 - INFO - utils_ner - Loading features from cached file /content/drive/My Drive/transformers-ner/data/cached_train_RobertaTokenizer_128 09/19/2020 21:08:58 - INFO - filelock - Lock 139766042433912 released on /content/drive/My Drive/transformers-ner/data/cached_train_RobertaTokenizer_128.lock ^C . from transformers import pipeline model_name = &#39;/content/drive/My Drive/transformers-ner/model&#39; nlp = pipeline(task=&quot;ner&quot;, model=model_name, tokenizer=model_name, framework=&quot;pt&quot;,grouped_entities=False) . sequence = &quot;&quot;&quot;Today there is a cricket match between India and Pakistan. It will be over by 10pm. I am excited to watch Sachin.&quot;&quot;&quot; result = nlp(sequence) result . [{&#39;entity&#39;: &#39;B-tim&#39;, &#39;index&#39;: 1, &#39;score&#39;: 0.9975540041923523, &#39;word&#39;: &#39;Today&#39;}, {&#39;entity&#39;: &#39;B-geo&#39;, &#39;index&#39;: 3, &#39;score&#39;: 0.6129258871078491, &#39;word&#39;: &#39;ĠIndia&#39;}, {&#39;entity&#39;: &#39;B-geo&#39;, &#39;index&#39;: 5, &#39;score&#39;: 0.7825925946235657, &#39;word&#39;: &#39;ĠPakistan&#39;}, {&#39;entity&#39;: &#39;B-tim&#39;, &#39;index&#39;: 13, &#39;score&#39;: 0.994357168674469, &#39;word&#39;: &#39;Ġ10&#39;}, {&#39;entity&#39;: &#39;I-tim&#39;, &#39;index&#39;: 14, &#39;score&#39;: 0.9675320386886597, &#39;word&#39;: &#39;pm&#39;}, {&#39;entity&#39;: &#39;B-per&#39;, &#39;index&#39;: 21, &#39;score&#39;: 0.830727219581604, &#39;word&#39;: &#39;ĠSach&#39;}, {&#39;entity&#39;: &#39;I-per&#39;, &#39;index&#39;: 22, &#39;score&#39;: 0.8053500652313232, &#39;word&#39;: &#39;in&#39;}] . result . tok = AutoTokenizer.from_pretrained(model_name) . toks = [t.replace(&#39;Ġ&#39;,&#39;&#39;) for t in tok.convert_ids_to_tokens(tok(sequence)[&#39;input_ids&#39;])] toks . [&#39;&lt;s&gt;&#39;, &#39;Today&#39;, &#39;is&#39;, &#39;India&#39;, &#39;and&#39;, &#39;Pakistan&#39;, &#39;match&#39;, &#39;.&#39;, &#39;It&#39;, &#39;will&#39;, &#39;be&#39;, &#39;over&#39;, &#39;by&#39;, &#39;10&#39;, &#39;pm&#39;, &#39;.&#39;, &#39;I&#39;, &#39;am&#39;, &#39;excited&#39;, &#39;to&#39;, &#39;watch&#39;, &#39;Sach&#39;, &#39;in&#39;, &#39;.&#39;, &#39;&lt;/s&gt;&#39;] . previous_tag = result[0][&#39;entity&#39;].split(&#39;-&#39;)[1] entity_list = [] start_index = result[0][&#39;index&#39;] current_len = len(toks[start_index]) start_pos = 1 print(result[0]) for i, res in enumerate(result[1:]): current_tag = res[&#39;entity&#39;].split(&#39;-&#39;)[1] print(res) if previous_tag == current_tag: current_len+=len(toks[res[&#39;index&#39;]]) else: print(previous_tag, start_index, current_len) entity_list.append({&#39;start_index&#39;:start_index, &#39;len&#39;:current_len, &#39;entity&#39;: previous_tag}) # previous_tag = current_tag start_index = res[&#39;index&#39;] current_len = len(toks[start_index]) print(previous_tag, start_index, current_len) . {&#39;word&#39;: &#39;Today&#39;, &#39;score&#39;: 0.9975540041923523, &#39;entity&#39;: &#39;B-tim&#39;, &#39;index&#39;: 1} {&#39;word&#39;: &#39;ĠIndia&#39;, &#39;score&#39;: 0.6129258871078491, &#39;entity&#39;: &#39;B-geo&#39;, &#39;index&#39;: 3} tim 1 5 geo 3 5 {&#39;word&#39;: &#39;ĠPakistan&#39;, &#39;score&#39;: 0.7825925946235657, &#39;entity&#39;: &#39;B-geo&#39;, &#39;index&#39;: 5} geo 3 13 {&#39;word&#39;: &#39;Ġ10&#39;, &#39;score&#39;: 0.994357168674469, &#39;entity&#39;: &#39;B-tim&#39;, &#39;index&#39;: 13} geo 3 13 tim 13 2 {&#39;word&#39;: &#39;pm&#39;, &#39;score&#39;: 0.9675320386886597, &#39;entity&#39;: &#39;I-tim&#39;, &#39;index&#39;: 14} tim 13 4 {&#39;word&#39;: &#39;ĠSach&#39;, &#39;score&#39;: 0.830727219581604, &#39;entity&#39;: &#39;B-per&#39;, &#39;index&#39;: 21} tim 13 4 per 21 4 {&#39;word&#39;: &#39;in&#39;, &#39;score&#39;: 0.8053500652313232, &#39;entity&#39;: &#39;I-per&#39;, &#39;index&#39;: 22} per 21 6 . start_pos=0 for i, t in enumerate(toks): start_pos += len(t) if i in result: current_tag = res[&#39;entity&#39;].split(&#39;-&#39;)[1] print(res) . entity_list . [{&#39;entity&#39;: &#39;tim&#39;, &#39;len&#39;: 5, &#39;start_index&#39;: 1}, {&#39;entity&#39;: &#39;geo&#39;, &#39;len&#39;: 13, &#39;start_index&#39;: 3}, {&#39;entity&#39;: &#39;tim&#39;, &#39;len&#39;: 4, &#39;start_index&#39;: 13}] . len(&#39;Ġis&#39;) . 3 . inputs = tokenizer(&#39;this is it&#39;, return_tensors=&#39;pt&#39;) . inputs . {&#39;input_ids&#39;: tensor([[ 0, 9226, 16, 24, 2]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1]])} . inputs.data . {&#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1]]), &#39;input_ids&#39;: tensor([[ 0, 9226, 16, 24, 2]])} . model(**inputs) . (tensor([[[ 0.9070, 0.6633, 1.2796, ..., -0.1246, -2.4989, -0.3228], [ 0.5650, 0.5028, 0.7081, ..., -0.1855, -1.6958, 0.4517], [ 0.3656, 0.4841, 0.5750, ..., -0.3846, -1.9121, -0.1240], [ 0.5685, 0.4180, 0.6799, ..., 0.0297, -1.5358, -0.1548], [ 0.1183, 0.5644, 0.9009, ..., -0.0645, -2.2902, 0.1540]]], grad_fn=&lt;AddBackward0&gt;),) . !wget https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttrain.bio . --2020-09-19 20:44:33-- https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttrain.bio Resolving groups.csail.mit.edu (groups.csail.mit.edu)... 128.30.2.44 Connecting to groups.csail.mit.edu (groups.csail.mit.edu)|128.30.2.44|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 759757 (742K) Saving to: ‘restauranttrain.bio’ restauranttrain.bio 100%[===================&gt;] 741.95K 1.66MB/s in 0.4s 2020-09-19 20:44:34 (1.66 MB/s) - ‘restauranttrain.bio’ saved [759757/759757] . !wget https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttest.bio . --2020-09-19 20:44:48-- https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttest.bio Resolving groups.csail.mit.edu (groups.csail.mit.edu)... 128.30.2.44 Connecting to groups.csail.mit.edu (groups.csail.mit.edu)|128.30.2.44|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 155722 (152K) Saving to: ‘restauranttest.bio’ restauranttest.bio 100%[===================&gt;] 152.07K 596KB/s in 0.3s 2020-09-19 20:44:49 (596 KB/s) - ‘restauranttest.bio’ saved [155722/155722] . !awk &#39;{print $2,$1}&#39; restauranttest.bio &gt; &quot;/content/drive/My Drive/transformers-ner/mit-restaurant-data/test.txt&quot; !awk &#39;{print $2,$1}&#39; restauranttrain.bio &gt; &quot;/content/drive/My Drive/transformers-ner/mit-restaurant-data/train.txt&quot; . !sed -i &#39;s/ t/ /g&#39; &#39;/content/drive/My Drive/transformers-ner/mit-restaurant-data/test.txt&#39; !sed -i &#39;s/^ $/ n/g&#39; &#39;/content/drive/My Drive/transformers-ner/mit-restaurant-data/test.txt&#39; . !sed -i &#39;s/ t/ /g&#39; &#39;/content/drive/My Drive/transformers-ner/mit-restaurant-data/train.txt&#39; !sed -i &#39;s/^ $/ n/g&#39; &#39;/content/drive/My Drive/transformers-ner/mit-restaurant-data/train.txt&#39; . df = pd.read_csv(&#39;/content/drive/My Drive/transformers-ner/mit-restaurant-data/train.txt&#39;,sep=&#39; &#39;, header=None) . with open(&#39;/content/drive/My Drive/transformers-ner/mit-restaurant-data/labels.txt&#39;,&#39;w&#39;) as fout: for tag in df[1].unique(): fout.write(tag+&#39; n&#39;) . from tasks import NER task = NER() examples = task.read_examples_from_file(&#39;/content/drive/My Drive/transformers-ner/mit-restaurant-data&#39;,&#39;train&#39;) examples[:3] . %env MODEL_NAME_OR_PATH=&#39;roberta-base&#39; %env LABELS=&#39;roberta-base&#39; %env OUTPUT_DIR= . !python run_ner.py --model_name_or_path &#39;roberta-base&#39; --data_dir &#39;/content/drive/My Drive/transformers-ner/mit-restaurant-data/&#39; --output_dir &#39;restaurant-model&#39; --max_seq_length &#39;128&#39; --num_train_epochs 3 --per_device_train_batch_size 8 --save_steps 1000000 --seed 16 --do_train --do_predict --overwrite_output_dir --labels &quot;/content/drive/My Drive/transformers-ner/mit-restaurant-data/labels.txt&quot; --fp16 . 2020-09-19 21:15:34.101825: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1 09/19/2020 21:15:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: True 09/19/2020 21:15:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(output_dir=&#39;restaurant-model&#39;, overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=True, evaluate_during_training=False, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=&#39;runs/Sep19_21-15-36_b791624ba0ff&#39;, logging_first_step=False, logging_steps=500, save_steps=1000000, save_total_limit=None, no_cuda=False, seed=16, fp16=True, fp16_opt_level=&#39;O1&#39;, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True, label_names=None) Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: [&#39;lm_head.bias&#39;, &#39;lm_head.dense.weight&#39;, &#39;lm_head.dense.bias&#39;, &#39;lm_head.layer_norm.weight&#39;, &#39;lm_head.layer_norm.bias&#39;, &#39;lm_head.decoder.weight&#39;] - This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. 09/19/2020 21:15:44 - INFO - filelock - Lock 140460101683352 acquired on /content/drive/My Drive/transformers-ner/mit-restaurant-data/cached_train_RobertaTokenizer_128.lock 09/19/2020 21:15:44 - INFO - utils_ner - Creating features from dataset file at /content/drive/My Drive/transformers-ner/mit-restaurant-data/ 09/19/2020 21:15:44 - INFO - utils_ner - Writing example 0 of 7660 09/19/2020 21:15:44 - INFO - utils_ner - *** Example *** 09/19/2020 21:15:44 - INFO - utils_ner - guid: train-1 09/19/2020 21:15:44 - INFO - utils_ner - tokens: &lt;s&gt; 2 start rest aur ants with inside d ining &lt;/s&gt; 09/19/2020 21:15:44 - INFO - utils_ner - input_ids: 0 176 13124 7110 8616 3277 5632 36615 417 6074 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:15:44 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - label_ids: -100 0 1 2 -100 -100 2 3 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:15:44 - INFO - utils_ner - *** Example *** 09/19/2020 21:15:44 - INFO - utils_ner - guid: train-2 09/19/2020 21:15:44 - INFO - utils_ner - tokens: &lt;s&gt; 34 &lt;/s&gt; 09/19/2020 21:15:44 - INFO - utils_ner - input_ids: 0 3079 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:15:44 - INFO - utils_ner - input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - label_ids: -100 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:15:44 - INFO - utils_ner - *** Example *** 09/19/2020 21:15:44 - INFO - utils_ner - guid: train-3 09/19/2020 21:15:44 - INFO - utils_ner - tokens: &lt;s&gt; 5 star rest ur ants in my town &lt;/s&gt; 09/19/2020 21:15:44 - INFO - utils_ner - input_ids: 0 245 3641 7110 710 3277 179 4783 7171 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:15:44 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - label_ids: -100 0 1 2 -100 -100 5 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:15:44 - INFO - utils_ner - *** Example *** 09/19/2020 21:15:44 - INFO - utils_ner - guid: train-4 09/19/2020 21:15:44 - INFO - utils_ner - tokens: &lt;s&gt; 98 h ong k ong rest aur ant reasonable pr ices &lt;/s&gt; 09/19/2020 21:15:44 - INFO - utils_ner - input_ids: 0 5208 298 1657 330 1657 7110 8616 927 33739 4862 6355 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:15:44 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - label_ids: -100 2 7 -100 8 -100 2 -100 -100 9 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:15:44 - INFO - utils_ner - *** Example *** 09/19/2020 21:15:44 - INFO - utils_ner - guid: train-5 09/19/2020 21:15:44 - INFO - utils_ner - tokens: &lt;s&gt; a great l unch spot but open t ill 2 a m pass ims kit chen &lt;/s&gt; 09/19/2020 21:15:44 - INFO - utils_ner - input_ids: 0 102 12338 462 11842 27840 4297 12592 90 1873 176 102 119 10212 13776 23199 8224 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:15:44 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:15:44 - INFO - utils_ner - label_ids: -100 2 2 2 -100 2 2 10 11 -100 11 11 11 7 -100 8 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:15:48 - INFO - utils_ner - Saving features into cached file /content/drive/My Drive/transformers-ner/mit-restaurant-data/cached_train_RobertaTokenizer_128 09/19/2020 21:15:49 - INFO - filelock - Lock 140460101683352 released on /content/drive/My Drive/transformers-ner/mit-restaurant-data/cached_train_RobertaTokenizer_128.lock Epoch: 0% 0/3 [00:00&lt;?, ?it/s] Iteration: 0% 0/958 [00:00&lt;?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`. Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate &#34;https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate&#34;, UserWarning) Iteration: 0% 1/958 [00:00&lt;01:49, 8.73it/s] Iteration: 0% 3/958 [00:00&lt;01:35, 10.04it/s] Iteration: 0% 4/958 [00:00&lt;01:37, 9.80it/s] Iteration: 1% 5/958 [00:00&lt;01:37, 9.80it/s] Iteration: 1% 6/958 [00:00&lt;01:37, 9.75it/s] Iteration: 1% 8/958 [00:00&lt;01:37, 9.79it/s] Iteration: 1% 10/958 [00:00&lt;01:31, 10.31it/s] Iteration: 1% 12/958 [00:01&lt;01:24, 11.18it/s] Iteration: 1% 14/958 [00:01&lt;01:19, 11.86it/s] Iteration: 2% 16/958 [00:01&lt;01:14, 12.57it/s] Iteration: 2% 18/958 [00:01&lt;01:13, 12.87it/s] Iteration: 2% 20/958 [00:01&lt;01:11, 13.18it/s] Iteration: 2% 22/958 [00:01&lt;01:13, 12.73it/s] Iteration: 3% 24/958 [00:01&lt;01:11, 13.12it/s] Iteration: 3% 26/958 [00:02&lt;01:10, 13.19it/s] Iteration: 3% 28/958 [00:02&lt;01:08, 13.56it/s] Iteration: 3% 30/958 [00:02&lt;01:07, 13.79it/s] Iteration: 3% 32/958 [00:02&lt;01:13, 12.68it/s] Iteration: 4% 34/958 [00:02&lt;01:11, 12.91it/s] Iteration: 4% 36/958 [00:02&lt;01:11, 12.93it/s] Iteration: 4% 38/958 [00:03&lt;01:08, 13.36it/s] Iteration: 4% 40/958 [00:03&lt;01:08, 13.41it/s] Iteration: 4% 42/958 [00:03&lt;01:06, 13.72it/s] Iteration: 5% 44/958 [00:03&lt;01:05, 13.98it/s] Iteration: 5% 46/958 [00:03&lt;01:05, 13.86it/s] Iteration: 5% 48/958 [00:03&lt;01:05, 13.93it/s] Iteration: 5% 50/958 [00:03&lt;01:08, 13.31it/s] Iteration: 5% 52/958 [00:04&lt;01:07, 13.42it/s] Iteration: 6% 54/958 [00:04&lt;01:06, 13.66it/s] Iteration: 6% 56/958 [00:04&lt;01:05, 13.75it/s] Iteration: 6% 58/958 [00:04&lt;01:05, 13.81it/s] Iteration: 6% 60/958 [00:04&lt;01:04, 13.88it/s] Iteration: 6% 62/958 [00:04&lt;01:04, 13.91it/s] Iteration: 7% 64/958 [00:04&lt;01:07, 13.32it/s] Iteration: 7% 66/958 [00:05&lt;01:05, 13.53it/s] Iteration: 7% 68/958 [00:05&lt;01:04, 13.82it/s] Iteration: 7% 70/958 [00:05&lt;01:04, 13.86it/s] Iteration: 8% 72/958 [00:05&lt;01:03, 13.89it/s] Iteration: 8% 74/958 [00:05&lt;01:04, 13.77it/s] Iteration: 8% 76/958 [00:05&lt;01:06, 13.30it/s] Iteration: 8% 78/958 [00:05&lt;01:11, 12.23it/s] Iteration: 8% 80/958 [00:06&lt;01:11, 12.31it/s] Iteration: 9% 82/958 [00:06&lt;01:09, 12.66it/s] Iteration: 9% 84/958 [00:06&lt;01:07, 13.04it/s] Iteration: 9% 86/958 [00:06&lt;01:06, 13.15it/s] Iteration: 9% 88/958 [00:06&lt;01:08, 12.78it/s] Iteration: 9% 90/958 [00:06&lt;01:11, 12.11it/s] Iteration: 10% 92/958 [00:07&lt;01:13, 11.86it/s] Iteration: 10% 94/958 [00:07&lt;01:09, 12.46it/s] Iteration: 10% 96/958 [00:07&lt;01:06, 12.87it/s] Iteration: 10% 98/958 [00:07&lt;01:06, 12.95it/s] Iteration: 10% 100/958 [00:07&lt;01:05, 13.02it/s] Iteration: 11% 102/958 [00:07&lt;01:05, 13.15it/s] Iteration: 11% 104/958 [00:08&lt;01:05, 13.05it/s] Iteration: 11% 106/958 [00:08&lt;01:03, 13.37it/s] Iteration: 11% 108/958 [00:08&lt;01:02, 13.49it/s] Iteration: 11% 110/958 [00:08&lt;01:01, 13.78it/s] Iteration: 12% 112/958 [00:08&lt;01:00, 13.87it/s] Iteration: 12% 114/958 [00:08&lt;01:01, 13.63it/s] Iteration: 12% 116/958 [00:08&lt;01:03, 13.24it/s] Iteration: 12% 118/958 [00:09&lt;01:04, 13.02it/s] Iteration: 13% 120/958 [00:09&lt;01:03, 13.21it/s] Iteration: 13% 122/958 [00:09&lt;01:01, 13.49it/s] Iteration: 13% 124/958 [00:09&lt;01:01, 13.63it/s] Iteration: 13% 126/958 [00:09&lt;01:01, 13.50it/s] Iteration: 13% 128/958 [00:09&lt;01:06, 12.41it/s] Iteration: 14% 130/958 [00:09&lt;01:07, 12.31it/s] Iteration: 14% 132/958 [00:10&lt;01:04, 12.83it/s] Iteration: 14% 134/958 [00:10&lt;01:03, 12.96it/s] Iteration: 14% 136/958 [00:10&lt;01:01, 13.34it/s] Iteration: 14% 138/958 [00:10&lt;01:00, 13.63it/s] Iteration: 15% 140/958 [00:10&lt;01:01, 13.21it/s] Iteration: 15% 142/958 [00:10&lt;01:05, 12.49it/s] Iteration: 15% 144/958 [00:11&lt;01:03, 12.80it/s] Iteration: 15% 146/958 [00:11&lt;01:02, 12.96it/s] Iteration: 15% 148/958 [00:11&lt;01:00, 13.28it/s] Iteration: 16% 150/958 [00:11&lt;01:00, 13.46it/s] Iteration: 16% 152/958 [00:11&lt;00:59, 13.57it/s] Iteration: 16% 154/958 [00:11&lt;01:01, 13.07it/s] Iteration: 16% 156/958 [00:11&lt;01:00, 13.28it/s] Iteration: 16% 158/958 [00:12&lt;01:01, 13.06it/s] Iteration: 17% 160/958 [00:12&lt;00:59, 13.51it/s] Iteration: 17% 162/958 [00:12&lt;00:58, 13.55it/s] Iteration: 17% 164/958 [00:12&lt;00:57, 13.75it/s] Iteration: 17% 166/958 [00:12&lt;00:59, 13.30it/s] Iteration: 18% 168/958 [00:12&lt;00:58, 13.51it/s] Iteration: 18% 170/958 [00:12&lt;00:58, 13.58it/s] Iteration: 18% 172/958 [00:13&lt;00:59, 13.10it/s] Iteration: 18% 174/958 [00:13&lt;00:58, 13.37it/s] Iteration: 18% 176/958 [00:13&lt;00:57, 13.66it/s] Iteration: 19% 178/958 [00:13&lt;00:56, 13.81it/s] Iteration: 19% 180/958 [00:13&lt;00:56, 13.69it/s] Iteration: 19% 182/958 [00:13&lt;00:57, 13.58it/s] Iteration: 19% 184/958 [00:14&lt;01:00, 12.71it/s] Iteration: 19% 186/958 [00:14&lt;01:00, 12.80it/s] Iteration: 20% 188/958 [00:14&lt;00:58, 13.07it/s] Iteration: 20% 190/958 [00:14&lt;00:57, 13.45it/s] Iteration: 20% 192/958 [00:14&lt;00:55, 13.70it/s] Iteration: 20% 194/958 [00:14&lt;00:55, 13.74it/s] Iteration: 20% 196/958 [00:14&lt;00:54, 13.98it/s] Iteration: 21% 198/958 [00:15&lt;00:53, 14.20it/s] Iteration: 21% 200/958 [00:15&lt;00:53, 14.17it/s] Iteration: 21% 202/958 [00:15&lt;00:53, 14.12it/s] Iteration: 21% 204/958 [00:15&lt;00:52, 14.42it/s] Iteration: 22% 206/958 [00:15&lt;00:51, 14.69it/s] Iteration: 22% 208/958 [00:15&lt;00:53, 13.98it/s] Iteration: 22% 210/958 [00:15&lt;00:54, 13.78it/s] Iteration: 22% 212/958 [00:16&lt;00:53, 13.91it/s] Iteration: 22% 214/958 [00:16&lt;00:54, 13.72it/s] Iteration: 23% 216/958 [00:16&lt;00:53, 13.86it/s] Iteration: 23% 218/958 [00:16&lt;00:53, 13.93it/s] Iteration: 23% 220/958 [00:16&lt;00:53, 13.69it/s] Iteration: 23% 222/958 [00:16&lt;00:53, 13.66it/s] Iteration: 23% 224/958 [00:16&lt;00:55, 13.18it/s] Iteration: 24% 226/958 [00:17&lt;00:55, 13.27it/s] Iteration: 24% 228/958 [00:17&lt;00:55, 13.22it/s] Iteration: 24% 230/958 [00:17&lt;00:59, 12.28it/s] Iteration: 24% 232/958 [00:17&lt;01:01, 11.78it/s] Iteration: 24% 234/958 [00:17&lt;01:02, 11.65it/s] Iteration: 25% 236/958 [00:17&lt;00:59, 12.11it/s] Iteration: 25% 238/958 [00:18&lt;00:56, 12.85it/s] Iteration: 25% 240/958 [00:18&lt;00:55, 12.89it/s] Iteration: 25% 242/958 [00:18&lt;00:54, 13.20it/s] Iteration: 25% 244/958 [00:18&lt;00:52, 13.55it/s] Iteration: 26% 246/958 [00:18&lt;00:51, 13.72it/s] Iteration: 26% 248/958 [00:18&lt;00:53, 13.36it/s] Iteration: 26% 250/958 [00:18&lt;00:51, 13.62it/s] Iteration: 26% 252/958 [00:19&lt;00:52, 13.57it/s] Iteration: 27% 254/958 [00:19&lt;00:52, 13.44it/s] Iteration: 27% 256/958 [00:19&lt;00:50, 13.85it/s] Iteration: 27% 258/958 [00:19&lt;00:49, 14.04it/s] Iteration: 27% 260/958 [00:19&lt;00:50, 13.93it/s] Iteration: 27% 262/958 [00:19&lt;00:50, 13.90it/s] Iteration: 28% 264/958 [00:19&lt;00:49, 13.95it/s] Iteration: 28% 266/958 [00:20&lt;00:49, 13.99it/s] Iteration: 28% 268/958 [00:20&lt;00:49, 13.83it/s] Iteration: 28% 270/958 [00:20&lt;00:49, 13.93it/s] Iteration: 28% 272/958 [00:20&lt;00:48, 14.05it/s] Iteration: 29% 274/958 [00:20&lt;00:48, 14.21it/s] Iteration: 29% 276/958 [00:20&lt;00:48, 13.99it/s] Iteration: 29% 278/958 [00:20&lt;00:53, 12.80it/s] Iteration: 29% 280/958 [00:21&lt;00:53, 12.70it/s] Iteration: 29% 282/958 [00:21&lt;00:52, 12.90it/s] Iteration: 30% 284/958 [00:21&lt;00:50, 13.30it/s] Iteration: 30% 286/958 [00:21&lt;00:49, 13.70it/s] Iteration: 30% 288/958 [00:21&lt;00:48, 13.83it/s] Iteration: 30% 290/958 [00:21&lt;00:52, 12.64it/s] Iteration: 30% 292/958 [00:22&lt;00:55, 12.08it/s] Iteration: 31% 294/958 [00:22&lt;00:52, 12.59it/s] Iteration: 31% 296/958 [00:22&lt;00:51, 12.89it/s] Iteration: 31% 298/958 [00:22&lt;00:50, 13.12it/s] Iteration: 31% 300/958 [00:22&lt;00:49, 13.41it/s] Iteration: 32% 302/958 [00:22&lt;00:48, 13.44it/s] Iteration: 32% 304/958 [00:22&lt;00:48, 13.60it/s] Iteration: 32% 306/958 [00:23&lt;00:47, 13.73it/s] Iteration: 32% 308/958 [00:23&lt;00:46, 13.98it/s] Iteration: 32% 310/958 [00:23&lt;00:46, 13.86it/s] Iteration: 33% 312/958 [00:23&lt;00:46, 13.97it/s] Iteration: 33% 314/958 [00:23&lt;00:46, 13.83it/s] Iteration: 33% 316/958 [00:23&lt;00:46, 13.81it/s] Iteration: 33% 318/958 [00:23&lt;00:46, 13.86it/s] Iteration: 33% 320/958 [00:24&lt;00:45, 13.89it/s] Iteration: 34% 322/958 [00:24&lt;00:45, 13.84it/s] Iteration: 34% 324/958 [00:24&lt;00:45, 13.80it/s] Iteration: 34% 326/958 [00:24&lt;00:45, 13.95it/s] Iteration: 34% 328/958 [00:24&lt;00:45, 13.88it/s] Iteration: 34% 330/958 [00:24&lt;00:45, 13.87it/s] Iteration: 35% 332/958 [00:24&lt;00:45, 13.83it/s] Iteration: 35% 334/958 [00:25&lt;00:44, 13.88it/s] Iteration: 35% 336/958 [00:25&lt;00:44, 13.91it/s] Iteration: 35% 338/958 [00:25&lt;00:45, 13.75it/s] Iteration: 35% 340/958 [00:25&lt;00:44, 13.90it/s] Iteration: 36% 342/958 [00:25&lt;00:43, 14.08it/s] Iteration: 36% 344/958 [00:25&lt;00:44, 13.83it/s] Iteration: 36% 346/958 [00:25&lt;00:44, 13.67it/s] Iteration: 36% 348/958 [00:26&lt;00:43, 13.87it/s] Iteration: 37% 350/958 [00:26&lt;00:43, 14.01it/s] Iteration: 37% 352/958 [00:26&lt;00:44, 13.47it/s] Iteration: 37% 354/958 [00:26&lt;00:44, 13.66it/s] Iteration: 37% 356/958 [00:26&lt;00:43, 13.91it/s] Iteration: 37% 358/958 [00:26&lt;00:42, 14.08it/s] Iteration: 38% 360/958 [00:26&lt;00:42, 14.04it/s] Iteration: 38% 362/958 [00:27&lt;00:42, 14.01it/s] Iteration: 38% 364/958 [00:27&lt;00:42, 14.04it/s] Iteration: 38% 366/958 [00:27&lt;00:42, 13.82it/s] Iteration: 38% 368/958 [00:27&lt;00:42, 14.02it/s] Iteration: 39% 370/958 [00:27&lt;00:41, 14.02it/s] Iteration: 39% 372/958 [00:27&lt;00:42, 13.94it/s] Iteration: 39% 374/958 [00:28&lt;00:43, 13.47it/s] Iteration: 39% 376/958 [00:28&lt;00:42, 13.83it/s] Iteration: 39% 378/958 [00:28&lt;00:41, 14.07it/s] Iteration: 40% 380/958 [00:28&lt;00:41, 13.92it/s] Iteration: 40% 382/958 [00:28&lt;00:41, 14.03it/s] Iteration: 40% 384/958 [00:28&lt;00:40, 14.11it/s] Iteration: 40% 386/958 [00:28&lt;00:44, 12.97it/s] Iteration: 41% 388/958 [00:29&lt;00:44, 12.85it/s] Iteration: 41% 390/958 [00:29&lt;00:42, 13.28it/s] Iteration: 41% 392/958 [00:29&lt;00:41, 13.54it/s] Iteration: 41% 394/958 [00:29&lt;00:41, 13.68it/s] Iteration: 41% 396/958 [00:29&lt;00:40, 13.92it/s] Iteration: 42% 398/958 [00:29&lt;00:39, 14.01it/s] Iteration: 42% 400/958 [00:29&lt;00:39, 14.08it/s] Iteration: 42% 402/958 [00:30&lt;00:39, 13.94it/s] Iteration: 42% 404/958 [00:30&lt;00:39, 13.97it/s] Iteration: 42% 406/958 [00:30&lt;00:40, 13.76it/s] Iteration: 43% 408/958 [00:30&lt;00:43, 12.63it/s] Iteration: 43% 410/958 [00:30&lt;00:45, 12.09it/s] Iteration: 43% 412/958 [00:30&lt;00:46, 11.69it/s] Iteration: 43% 414/958 [00:31&lt;00:44, 12.25it/s] Iteration: 43% 416/958 [00:31&lt;00:42, 12.80it/s] Iteration: 44% 418/958 [00:31&lt;00:40, 13.35it/s] Iteration: 44% 420/958 [00:31&lt;00:39, 13.60it/s] Iteration: 44% 422/958 [00:31&lt;00:39, 13.45it/s] Iteration: 44% 424/958 [00:31&lt;00:41, 12.86it/s] Iteration: 44% 426/958 [00:31&lt;00:43, 12.31it/s] Iteration: 45% 428/958 [00:32&lt;00:41, 12.85it/s] Iteration: 45% 430/958 [00:32&lt;00:39, 13.20it/s] Iteration: 45% 432/958 [00:32&lt;00:39, 13.44it/s] Iteration: 45% 434/958 [00:32&lt;00:38, 13.60it/s] Iteration: 46% 436/958 [00:32&lt;00:37, 13.90it/s] Iteration: 46% 438/958 [00:32&lt;00:37, 13.89it/s] Iteration: 46% 440/958 [00:32&lt;00:36, 14.09it/s] Iteration: 46% 442/958 [00:33&lt;00:36, 14.06it/s] Iteration: 46% 444/958 [00:33&lt;00:36, 14.22it/s] Iteration: 47% 446/958 [00:33&lt;00:35, 14.33it/s] Iteration: 47% 448/958 [00:33&lt;00:34, 14.61it/s] Iteration: 47% 450/958 [00:33&lt;00:34, 14.57it/s] Iteration: 47% 452/958 [00:33&lt;00:34, 14.49it/s] Iteration: 47% 454/958 [00:33&lt;00:35, 14.16it/s] Iteration: 48% 456/958 [00:34&lt;00:35, 13.96it/s] Iteration: 48% 458/958 [00:34&lt;00:35, 13.94it/s] Iteration: 48% 460/958 [00:34&lt;00:35, 14.15it/s] Iteration: 48% 462/958 [00:34&lt;00:34, 14.32it/s] Iteration: 48% 464/958 [00:34&lt;00:34, 14.32it/s] Iteration: 49% 466/958 [00:34&lt;00:34, 14.47it/s] Iteration: 49% 468/958 [00:34&lt;00:33, 14.43it/s] Iteration: 49% 470/958 [00:35&lt;00:34, 14.34it/s] Iteration: 49% 472/958 [00:35&lt;00:33, 14.35it/s] Iteration: 49% 474/958 [00:35&lt;00:33, 14.52it/s] Iteration: 50% 476/958 [00:35&lt;00:32, 14.71it/s] Iteration: 50% 478/958 [00:35&lt;00:32, 14.63it/s] Iteration: 50% 480/958 [00:35&lt;00:32, 14.75it/s] Iteration: 50% 482/958 [00:35&lt;00:33, 14.38it/s] Iteration: 51% 484/958 [00:36&lt;00:34, 13.85it/s] Iteration: 51% 486/958 [00:36&lt;00:33, 14.24it/s] Iteration: 51% 488/958 [00:36&lt;00:32, 14.31it/s] Iteration: 51% 490/958 [00:36&lt;00:32, 14.55it/s] Iteration: 51% 492/958 [00:36&lt;00:31, 14.75it/s] Iteration: 52% 494/958 [00:36&lt;00:31, 14.53it/s] Iteration: 52% 496/958 [00:36&lt;00:31, 14.68it/s] Iteration: 52% 498/958 [00:36&lt;00:31, 14.79it/s]{&#39;loss&#39;: 0.5845457153320313, &#39;learning_rate&#39;: 4.130132219902575e-05, &#39;epoch&#39;: 0.5219206680584552, &#39;total_flos&#39;: 382951541760000, &#39;step&#39;: 500} Iteration: 52% 500/958 [00:37&lt;00:31, 14.64it/s] Iteration: 52% 502/958 [00:37&lt;00:32, 14.20it/s] Iteration: 53% 504/958 [00:37&lt;00:31, 14.23it/s] Iteration: 53% 506/958 [00:37&lt;00:31, 14.53it/s] Iteration: 53% 508/958 [00:37&lt;00:30, 14.54it/s] Iteration: 53% 510/958 [00:37&lt;00:30, 14.52it/s] Iteration: 53% 512/958 [00:37&lt;00:30, 14.69it/s] Iteration: 54% 514/958 [00:38&lt;00:30, 14.65it/s] Iteration: 54% 516/958 [00:38&lt;00:30, 14.58it/s] Iteration: 54% 518/958 [00:38&lt;00:30, 14.63it/s] Iteration: 54% 520/958 [00:38&lt;00:29, 14.75it/s] Iteration: 54% 522/958 [00:38&lt;00:29, 14.56it/s] Iteration: 55% 524/958 [00:38&lt;00:30, 14.31it/s] Iteration: 55% 526/958 [00:38&lt;00:30, 14.37it/s] Iteration: 55% 528/958 [00:39&lt;00:30, 14.18it/s] Iteration: 55% 530/958 [00:39&lt;00:29, 14.33it/s] Iteration: 56% 532/958 [00:39&lt;00:29, 14.40it/s] Iteration: 56% 534/958 [00:39&lt;00:29, 14.44it/s] Iteration: 56% 536/958 [00:39&lt;00:29, 14.41it/s] Iteration: 56% 538/958 [00:39&lt;00:29, 14.20it/s] Iteration: 56% 540/958 [00:39&lt;00:30, 13.68it/s] Iteration: 57% 542/958 [00:40&lt;00:29, 13.96it/s] Iteration: 57% 544/958 [00:40&lt;00:29, 13.91it/s] Iteration: 57% 546/958 [00:40&lt;00:29, 14.03it/s] Iteration: 57% 548/958 [00:40&lt;00:28, 14.15it/s] Iteration: 57% 550/958 [00:40&lt;00:28, 14.32it/s] Iteration: 58% 552/958 [00:40&lt;00:28, 14.12it/s] Iteration: 58% 554/958 [00:40&lt;00:28, 14.25it/s] Iteration: 58% 556/958 [00:40&lt;00:27, 14.37it/s] Iteration: 58% 558/958 [00:41&lt;00:27, 14.40it/s] Iteration: 58% 560/958 [00:41&lt;00:27, 14.50it/s] Iteration: 59% 562/958 [00:41&lt;00:27, 14.40it/s] Iteration: 59% 564/958 [00:41&lt;00:27, 14.41it/s] Iteration: 59% 566/958 [00:41&lt;00:27, 14.46it/s] Iteration: 59% 568/958 [00:41&lt;00:27, 14.01it/s] Iteration: 59% 570/958 [00:41&lt;00:27, 13.94it/s] Iteration: 60% 572/958 [00:42&lt;00:27, 13.86it/s] Iteration: 60% 574/958 [00:42&lt;00:27, 14.17it/s] Iteration: 60% 576/958 [00:42&lt;00:26, 14.18it/s] Iteration: 60% 578/958 [00:42&lt;00:26, 14.09it/s] Iteration: 61% 580/958 [00:42&lt;00:26, 14.31it/s] Iteration: 61% 582/958 [00:42&lt;00:26, 14.09it/s] Iteration: 61% 584/958 [00:42&lt;00:26, 14.11it/s] Iteration: 61% 586/958 [00:43&lt;00:26, 13.99it/s] Iteration: 61% 588/958 [00:43&lt;00:26, 14.15it/s] Iteration: 62% 590/958 [00:43&lt;00:25, 14.17it/s] Iteration: 62% 592/958 [00:43&lt;00:25, 14.25it/s] Iteration: 62% 594/958 [00:43&lt;00:25, 14.38it/s] Iteration: 62% 596/958 [00:43&lt;00:27, 13.19it/s] Iteration: 62% 598/958 [00:43&lt;00:26, 13.34it/s] Iteration: 63% 600/958 [00:44&lt;00:26, 13.59it/s] Iteration: 63% 602/958 [00:44&lt;00:25, 14.02it/s] Iteration: 63% 604/958 [00:44&lt;00:25, 14.08it/s] Iteration: 63% 606/958 [00:44&lt;00:24, 14.20it/s] Iteration: 63% 608/958 [00:44&lt;00:24, 14.43it/s] Iteration: 64% 610/958 [00:44&lt;00:26, 13.00it/s] Iteration: 64% 612/958 [00:45&lt;00:25, 13.33it/s] Iteration: 64% 614/958 [00:45&lt;00:25, 13.48it/s] Iteration: 64% 616/958 [00:45&lt;00:25, 13.67it/s] Iteration: 65% 618/958 [00:45&lt;00:24, 13.70it/s] Iteration: 65% 620/958 [00:45&lt;00:25, 13.43it/s] Iteration: 65% 622/958 [00:45&lt;00:24, 13.45it/s] Iteration: 65% 624/958 [00:45&lt;00:25, 13.27it/s] Iteration: 65% 626/958 [00:46&lt;00:24, 13.47it/s] Iteration: 66% 628/958 [00:46&lt;00:24, 13.55it/s] Iteration: 66% 630/958 [00:46&lt;00:24, 13.55it/s] Iteration: 66% 632/958 [00:46&lt;00:23, 13.65it/s] Iteration: 66% 634/958 [00:46&lt;00:23, 13.86it/s] Iteration: 66% 636/958 [00:46&lt;00:23, 13.70it/s] Iteration: 67% 638/958 [00:46&lt;00:23, 13.41it/s] Iteration: 67% 640/958 [00:47&lt;00:23, 13.59it/s] Iteration: 67% 642/958 [00:47&lt;00:23, 13.50it/s] Iteration: 67% 644/958 [00:47&lt;00:22, 13.71it/s] Iteration: 67% 646/958 [00:47&lt;00:23, 13.38it/s] Iteration: 68% 648/958 [00:47&lt;00:23, 13.22it/s] Iteration: 68% 650/958 [00:47&lt;00:22, 13.50it/s] Iteration: 68% 652/958 [00:47&lt;00:22, 13.52it/s] Iteration: 68% 654/958 [00:48&lt;00:22, 13.81it/s] Iteration: 68% 656/958 [00:48&lt;00:22, 13.66it/s] Iteration: 69% 658/958 [00:48&lt;00:21, 13.74it/s] Iteration: 69% 660/958 [00:48&lt;00:23, 12.48it/s] Iteration: 69% 662/958 [00:48&lt;00:24, 12.01it/s] Iteration: 69% 664/958 [00:48&lt;00:24, 11.91it/s] Iteration: 70% 666/958 [00:49&lt;00:23, 12.45it/s] Iteration: 70% 668/958 [00:49&lt;00:22, 12.83it/s] Iteration: 70% 670/958 [00:49&lt;00:21, 13.30it/s] Iteration: 70% 672/958 [00:49&lt;00:21, 13.36it/s] Iteration: 70% 674/958 [00:49&lt;00:20, 13.70it/s] Iteration: 71% 676/958 [00:49&lt;00:20, 13.77it/s] Iteration: 71% 678/958 [00:49&lt;00:22, 12.45it/s] Iteration: 71% 680/958 [00:50&lt;00:24, 11.55it/s] Iteration: 71% 682/958 [00:50&lt;00:24, 11.23it/s] Iteration: 71% 684/958 [00:50&lt;00:24, 11.14it/s] Iteration: 72% 686/958 [00:50&lt;00:24, 11.11it/s] Iteration: 72% 688/958 [00:50&lt;00:24, 11.11it/s] Iteration: 72% 690/958 [00:51&lt;00:22, 12.02it/s] Iteration: 72% 692/958 [00:51&lt;00:21, 12.46it/s] Iteration: 72% 694/958 [00:51&lt;00:20, 13.02it/s] Iteration: 73% 696/958 [00:51&lt;00:19, 13.44it/s] Iteration: 73% 698/958 [00:51&lt;00:18, 13.81it/s] Iteration: 73% 700/958 [00:51&lt;00:18, 13.79it/s] Iteration: 73% 702/958 [00:51&lt;00:18, 13.98it/s] Iteration: 73% 704/958 [00:52&lt;00:18, 13.79it/s] Iteration: 74% 706/958 [00:52&lt;00:18, 13.72it/s] Iteration: 74% 708/958 [00:52&lt;00:18, 13.84it/s] Iteration: 74% 710/958 [00:52&lt;00:17, 14.01it/s] Iteration: 74% 712/958 [00:52&lt;00:17, 14.06it/s] Iteration: 75% 714/958 [00:52&lt;00:17, 13.63it/s] Iteration: 75% 716/958 [00:52&lt;00:18, 13.22it/s] Iteration: 75% 718/958 [00:53&lt;00:17, 13.37it/s] Iteration: 75% 720/958 [00:53&lt;00:17, 13.57it/s] Iteration: 75% 722/958 [00:53&lt;00:17, 13.82it/s] Iteration: 76% 724/958 [00:53&lt;00:16, 14.08it/s] Iteration: 76% 726/958 [00:53&lt;00:16, 14.16it/s] Iteration: 76% 728/958 [00:53&lt;00:16, 14.25it/s] Iteration: 76% 730/958 [00:53&lt;00:16, 14.09it/s] Iteration: 76% 732/958 [00:54&lt;00:16, 13.93it/s] Iteration: 77% 734/958 [00:54&lt;00:15, 14.07it/s] Iteration: 77% 736/958 [00:54&lt;00:15, 13.99it/s] Iteration: 77% 738/958 [00:54&lt;00:15, 13.81it/s] Iteration: 77% 740/958 [00:54&lt;00:15, 13.95it/s] Iteration: 77% 742/958 [00:54&lt;00:15, 14.13it/s] Iteration: 78% 744/958 [00:54&lt;00:15, 14.07it/s] Iteration: 78% 746/958 [00:55&lt;00:15, 13.96it/s] Iteration: 78% 748/958 [00:55&lt;00:14, 14.11it/s] Iteration: 78% 750/958 [00:55&lt;00:14, 13.89it/s] Iteration: 78% 752/958 [00:55&lt;00:14, 14.01it/s] Iteration: 79% 754/958 [00:55&lt;00:14, 14.18it/s] Iteration: 79% 756/958 [00:55&lt;00:14, 13.74it/s] Iteration: 79% 758/958 [00:55&lt;00:14, 13.34it/s] Iteration: 79% 760/958 [00:56&lt;00:14, 13.47it/s] Iteration: 80% 762/958 [00:56&lt;00:14, 13.79it/s] Iteration: 80% 764/958 [00:56&lt;00:13, 13.94it/s] Iteration: 80% 766/958 [00:56&lt;00:13, 14.18it/s] Iteration: 80% 768/958 [00:56&lt;00:13, 14.30it/s] Iteration: 80% 770/958 [00:56&lt;00:13, 14.31it/s] Iteration: 81% 772/958 [00:56&lt;00:13, 14.01it/s] Iteration: 81% 774/958 [00:57&lt;00:13, 13.15it/s] Iteration: 81% 776/958 [00:57&lt;00:13, 13.31it/s] Iteration: 81% 778/958 [00:57&lt;00:13, 13.45it/s] Iteration: 81% 780/958 [00:57&lt;00:12, 13.81it/s] Iteration: 82% 782/958 [00:57&lt;00:12, 13.95it/s] Iteration: 82% 784/958 [00:57&lt;00:12, 14.03it/s] Iteration: 82% 786/958 [00:57&lt;00:12, 14.11it/s] Iteration: 82% 788/958 [00:58&lt;00:12, 14.03it/s] Iteration: 82% 790/958 [00:58&lt;00:11, 14.07it/s] Iteration: 83% 792/958 [00:58&lt;00:11, 14.03it/s] Iteration: 83% 794/958 [00:58&lt;00:11, 14.20it/s] Iteration: 83% 796/958 [00:58&lt;00:11, 14.19it/s] Iteration: 83% 798/958 [00:58&lt;00:11, 14.36it/s] Iteration: 84% 800/958 [00:58&lt;00:11, 14.24it/s] Iteration: 84% 802/958 [00:59&lt;00:10, 14.29it/s] Iteration: 84% 804/958 [00:59&lt;00:11, 13.90it/s] Iteration: 84% 806/958 [00:59&lt;00:10, 13.92it/s] Iteration: 84% 808/958 [00:59&lt;00:10, 14.20it/s] Iteration: 85% 810/958 [00:59&lt;00:10, 14.34it/s] Iteration: 85% 812/958 [00:59&lt;00:10, 14.06it/s] Iteration: 85% 814/958 [00:59&lt;00:10, 14.14it/s] Iteration: 85% 816/958 [01:00&lt;00:09, 14.39it/s] Iteration: 85% 818/958 [01:00&lt;00:09, 14.16it/s] Iteration: 86% 820/958 [01:00&lt;00:09, 14.14it/s] Iteration: 86% 822/958 [01:00&lt;00:09, 14.35it/s] Iteration: 86% 824/958 [01:00&lt;00:09, 14.21it/s] Iteration: 86% 826/958 [01:00&lt;00:09, 14.36it/s] Iteration: 86% 828/958 [01:00&lt;00:09, 14.39it/s] Iteration: 87% 830/958 [01:01&lt;00:09, 14.20it/s] Iteration: 87% 832/958 [01:01&lt;00:08, 14.03it/s] Iteration: 87% 834/958 [01:01&lt;00:08, 13.97it/s] Iteration: 87% 836/958 [01:01&lt;00:08, 14.14it/s] Iteration: 87% 838/958 [01:01&lt;00:08, 14.28it/s] Iteration: 88% 840/958 [01:01&lt;00:08, 14.32it/s] Iteration: 88% 842/958 [01:01&lt;00:08, 14.34it/s] Iteration: 88% 844/958 [01:02&lt;00:08, 14.22it/s] Iteration: 88% 846/958 [01:02&lt;00:07, 14.10it/s] Iteration: 89% 848/958 [01:02&lt;00:07, 14.27it/s] Iteration: 89% 850/958 [01:02&lt;00:07, 14.10it/s] Iteration: 89% 852/958 [01:02&lt;00:07, 14.30it/s] Iteration: 89% 854/958 [01:02&lt;00:07, 14.42it/s] Iteration: 89% 856/958 [01:02&lt;00:07, 14.41it/s] Iteration: 90% 858/958 [01:03&lt;00:07, 14.06it/s] Iteration: 90% 860/958 [01:03&lt;00:06, 14.28it/s] Iteration: 90% 862/958 [01:03&lt;00:06, 14.04it/s] Iteration: 90% 864/958 [01:03&lt;00:06, 13.99it/s] Iteration: 90% 866/958 [01:03&lt;00:06, 14.28it/s] Iteration: 91% 868/958 [01:03&lt;00:06, 14.28it/s] Iteration: 91% 870/958 [01:03&lt;00:06, 14.17it/s] Iteration: 91% 872/958 [01:04&lt;00:06, 14.28it/s] Iteration: 91% 874/958 [01:04&lt;00:05, 14.27it/s] Iteration: 91% 876/958 [01:04&lt;00:05, 14.10it/s] Iteration: 92% 878/958 [01:04&lt;00:05, 14.12it/s] Iteration: 92% 880/958 [01:04&lt;00:05, 14.14it/s] Iteration: 92% 882/958 [01:04&lt;00:05, 14.27it/s] Iteration: 92% 884/958 [01:04&lt;00:05, 14.40it/s] Iteration: 92% 886/958 [01:05&lt;00:05, 14.32it/s] Iteration: 93% 888/958 [01:05&lt;00:04, 14.24it/s] Iteration: 93% 890/958 [01:05&lt;00:04, 14.09it/s] Iteration: 93% 892/958 [01:05&lt;00:05, 12.74it/s] Iteration: 93% 894/958 [01:05&lt;00:05, 12.11it/s] Iteration: 94% 896/958 [01:05&lt;00:04, 12.42it/s] Iteration: 94% 898/958 [01:05&lt;00:04, 13.04it/s] Iteration: 94% 900/958 [01:06&lt;00:04, 13.49it/s] Iteration: 94% 902/958 [01:06&lt;00:04, 13.53it/s] Iteration: 94% 904/958 [01:06&lt;00:03, 13.65it/s] Iteration: 95% 906/958 [01:06&lt;00:03, 13.79it/s] Iteration: 95% 908/958 [01:06&lt;00:03, 14.06it/s] Iteration: 95% 910/958 [01:06&lt;00:03, 14.23it/s] Iteration: 95% 912/958 [01:06&lt;00:03, 14.01it/s] Iteration: 95% 914/958 [01:07&lt;00:03, 14.05it/s] Iteration: 96% 916/958 [01:07&lt;00:02, 14.33it/s] Iteration: 96% 918/958 [01:07&lt;00:02, 14.18it/s] Iteration: 96% 920/958 [01:07&lt;00:02, 14.08it/s] Iteration: 96% 922/958 [01:07&lt;00:02, 14.35it/s] Iteration: 96% 924/958 [01:07&lt;00:02, 14.01it/s] Iteration: 97% 926/958 [01:07&lt;00:02, 13.96it/s] Iteration: 97% 928/958 [01:08&lt;00:02, 13.09it/s] Iteration: 97% 930/958 [01:08&lt;00:02, 13.51it/s] Iteration: 97% 932/958 [01:08&lt;00:01, 13.56it/s] Iteration: 97% 934/958 [01:08&lt;00:01, 12.84it/s] Iteration: 98% 936/958 [01:08&lt;00:01, 12.35it/s] Iteration: 98% 938/958 [01:08&lt;00:01, 12.79it/s] Iteration: 98% 940/958 [01:09&lt;00:01, 12.90it/s] Iteration: 98% 942/958 [01:09&lt;00:01, 13.44it/s] Iteration: 99% 944/958 [01:09&lt;00:01, 13.79it/s] Iteration: 99% 946/958 [01:09&lt;00:00, 13.57it/s] Iteration: 99% 948/958 [01:09&lt;00:00, 13.99it/s] Iteration: 99% 950/958 [01:09&lt;00:00, 13.55it/s] Iteration: 99% 952/958 [01:09&lt;00:00, 13.52it/s] Iteration: 100% 954/958 [01:10&lt;00:00, 13.71it/s] Iteration: 100% 956/958 [01:10&lt;00:00, 14.03it/s] Iteration: 100% 958/958 [01:10&lt;00:00, 13.62it/s] Epoch: 33% 1/3 [01:10&lt;02:20, 70.33s/it] Iteration: 0% 0/958 [00:00&lt;?, ?it/s] Iteration: 0% 2/958 [00:00&lt;01:18, 12.11it/s] Iteration: 0% 4/958 [00:00&lt;01:15, 12.65it/s] Iteration: 1% 6/958 [00:00&lt;01:12, 13.13it/s] Iteration: 1% 8/958 [00:00&lt;01:12, 13.03it/s] Iteration: 1% 10/958 [00:00&lt;01:11, 13.26it/s] Iteration: 1% 12/958 [00:00&lt;01:10, 13.42it/s] Iteration: 1% 14/958 [00:01&lt;01:09, 13.67it/s] Iteration: 2% 16/958 [00:01&lt;01:10, 13.43it/s] Iteration: 2% 18/958 [00:01&lt;01:08, 13.74it/s] Iteration: 2% 20/958 [00:01&lt;01:08, 13.65it/s] Iteration: 2% 22/958 [00:01&lt;01:07, 13.78it/s] Iteration: 3% 24/958 [00:01&lt;01:07, 13.91it/s] Iteration: 3% 26/958 [00:01&lt;01:05, 14.19it/s] Iteration: 3% 28/958 [00:02&lt;01:04, 14.32it/s] Iteration: 3% 30/958 [00:02&lt;01:06, 13.87it/s] Iteration: 3% 32/958 [00:02&lt;01:07, 13.81it/s] Iteration: 4% 34/958 [00:02&lt;01:12, 12.70it/s] Iteration: 4% 36/958 [00:02&lt;01:11, 12.88it/s] Iteration: 4% 38/958 [00:02&lt;01:09, 13.29it/s] Iteration: 4% 40/958 [00:02&lt;01:07, 13.65it/s]{&#39;loss&#39;: 0.339539306640625, &#39;learning_rate&#39;: 3.26026443980515e-05, &#39;epoch&#39;: 1.0438413361169103, &#39;total_flos&#39;: 765520131978240, &#39;step&#39;: 1000} Iteration: 4% 42/958 [00:03&lt;01:05, 13.90it/s] Iteration: 5% 44/958 [00:03&lt;01:07, 13.54it/s] Iteration: 5% 46/958 [00:03&lt;01:07, 13.52it/s] Iteration: 5% 48/958 [00:03&lt;01:07, 13.40it/s] Iteration: 5% 50/958 [00:03&lt;01:06, 13.69it/s] Iteration: 5% 52/958 [00:03&lt;01:04, 13.98it/s] Iteration: 6% 54/958 [00:03&lt;01:05, 13.78it/s] Iteration: 6% 56/958 [00:04&lt;01:04, 13.95it/s] Iteration: 6% 58/958 [00:04&lt;01:06, 13.58it/s] Iteration: 6% 60/958 [00:04&lt;01:04, 13.86it/s] Iteration: 6% 62/958 [00:04&lt;01:03, 14.00it/s] Iteration: 7% 64/958 [00:04&lt;01:02, 14.22it/s] Iteration: 7% 66/958 [00:04&lt;01:02, 14.25it/s] Iteration: 7% 68/958 [00:04&lt;01:01, 14.35it/s] Iteration: 7% 70/958 [00:05&lt;01:01, 14.41it/s] Iteration: 8% 72/958 [00:05&lt;01:03, 13.89it/s] Iteration: 8% 74/958 [00:05&lt;01:05, 13.55it/s] Iteration: 8% 76/958 [00:05&lt;01:03, 13.79it/s] Iteration: 8% 78/958 [00:05&lt;01:03, 13.96it/s] Iteration: 8% 80/958 [00:05&lt;01:02, 14.12it/s] Iteration: 9% 82/958 [00:05&lt;01:01, 14.16it/s] Iteration: 9% 84/958 [00:06&lt;01:01, 14.20it/s] Iteration: 9% 86/958 [00:06&lt;01:04, 13.57it/s] Iteration: 9% 88/958 [00:06&lt;01:02, 13.86it/s] Iteration: 9% 90/958 [00:06&lt;01:04, 13.40it/s] Iteration: 10% 92/958 [00:06&lt;01:03, 13.72it/s] Iteration: 10% 94/958 [00:06&lt;01:02, 13.85it/s] Iteration: 10% 96/958 [00:06&lt;01:01, 14.07it/s] Iteration: 10% 98/958 [00:07&lt;01:00, 14.12it/s] Iteration: 10% 100/958 [00:07&lt;01:02, 13.78it/s] Iteration: 11% 102/958 [00:07&lt;01:01, 13.97it/s] Iteration: 11% 104/958 [00:07&lt;01:02, 13.71it/s] Iteration: 11% 106/958 [00:07&lt;01:07, 12.72it/s] Iteration: 11% 108/958 [00:07&lt;01:04, 13.11it/s] Iteration: 11% 110/958 [00:08&lt;01:03, 13.42it/s] Iteration: 12% 112/958 [00:08&lt;01:01, 13.71it/s] Iteration: 12% 114/958 [00:08&lt;01:01, 13.64it/s] Iteration: 12% 116/958 [00:08&lt;01:02, 13.58it/s] Iteration: 12% 118/958 [00:08&lt;01:00, 13.88it/s] Iteration: 13% 120/958 [00:08&lt;00:59, 14.16it/s] Iteration: 13% 122/958 [00:08&lt;00:59, 14.10it/s] Iteration: 13% 124/958 [00:09&lt;00:58, 14.14it/s] Iteration: 13% 126/958 [00:09&lt;00:58, 14.14it/s] Iteration: 13% 128/958 [00:09&lt;01:00, 13.61it/s] Iteration: 14% 130/958 [00:09&lt;01:00, 13.70it/s] Iteration: 14% 132/958 [00:09&lt;01:00, 13.73it/s] Iteration: 14% 134/958 [00:09&lt;00:59, 13.77it/s] Iteration: 14% 136/958 [00:09&lt;00:59, 13.86it/s] Iteration: 14% 138/958 [00:10&lt;00:59, 13.86it/s] Iteration: 15% 140/958 [00:10&lt;00:59, 13.70it/s] Iteration: 15% 142/958 [00:10&lt;01:01, 13.34it/s] Iteration: 15% 144/958 [00:10&lt;01:01, 13.32it/s] Iteration: 15% 146/958 [00:10&lt;01:00, 13.52it/s] Iteration: 15% 148/958 [00:10&lt;00:58, 13.73it/s] Iteration: 16% 150/958 [00:10&lt;00:58, 13.70it/s] Iteration: 16% 152/958 [00:11&lt;00:59, 13.53it/s] Iteration: 16% 154/958 [00:11&lt;00:57, 13.88it/s] Iteration: 16% 156/958 [00:11&lt;00:59, 13.43it/s] Iteration: 16% 158/958 [00:11&lt;01:03, 12.66it/s] Iteration: 17% 160/958 [00:11&lt;01:00, 13.13it/s] Iteration: 17% 162/958 [00:11&lt;00:59, 13.48it/s] Iteration: 17% 164/958 [00:11&lt;00:57, 13.78it/s] Iteration: 17% 166/958 [00:12&lt;00:56, 14.09it/s] Iteration: 18% 168/958 [00:12&lt;00:55, 14.15it/s] Iteration: 18% 170/958 [00:12&lt;00:57, 13.66it/s] Iteration: 18% 172/958 [00:12&lt;00:56, 13.90it/s] Iteration: 18% 174/958 [00:12&lt;00:55, 14.05it/s] Iteration: 18% 176/958 [00:12&lt;00:55, 14.06it/s] Iteration: 19% 178/958 [00:12&lt;00:54, 14.29it/s] Iteration: 19% 180/958 [00:13&lt;00:54, 14.34it/s] Iteration: 19% 182/958 [00:13&lt;00:54, 14.23it/s] Iteration: 19% 184/958 [00:13&lt;00:55, 13.96it/s] Iteration: 19% 186/958 [00:13&lt;00:54, 14.12it/s] Iteration: 20% 188/958 [00:13&lt;00:55, 13.98it/s] Iteration: 20% 190/958 [00:13&lt;00:54, 14.07it/s] Iteration: 20% 192/958 [00:13&lt;00:53, 14.24it/s] Iteration: 20% 194/958 [00:14&lt;00:53, 14.31it/s] Iteration: 20% 196/958 [00:14&lt;00:53, 14.35it/s] Iteration: 21% 198/958 [00:14&lt;00:52, 14.50it/s] Iteration: 21% 200/958 [00:14&lt;00:53, 14.04it/s] Iteration: 21% 202/958 [00:14&lt;00:53, 14.03it/s] Iteration: 21% 204/958 [00:14&lt;00:52, 14.30it/s] Iteration: 22% 206/958 [00:14&lt;00:52, 14.32it/s] Iteration: 22% 208/958 [00:15&lt;00:52, 14.32it/s] Iteration: 22% 210/958 [00:15&lt;00:51, 14.48it/s] Iteration: 22% 212/958 [00:15&lt;00:51, 14.50it/s] Iteration: 22% 214/958 [00:15&lt;00:52, 14.20it/s] Iteration: 23% 216/958 [00:15&lt;00:52, 14.09it/s] Iteration: 23% 218/958 [00:15&lt;00:52, 14.14it/s] Iteration: 23% 220/958 [00:15&lt;00:51, 14.26it/s] Iteration: 23% 222/958 [00:16&lt;00:50, 14.51it/s] Iteration: 23% 224/958 [00:16&lt;00:50, 14.48it/s] Iteration: 24% 226/958 [00:16&lt;00:51, 14.13it/s] Iteration: 24% 228/958 [00:16&lt;00:52, 13.86it/s] Iteration: 24% 230/958 [00:16&lt;00:51, 14.06it/s] Iteration: 24% 232/958 [00:16&lt;00:51, 14.12it/s] Iteration: 24% 234/958 [00:16&lt;00:50, 14.42it/s] Iteration: 25% 236/958 [00:17&lt;00:54, 13.17it/s] Iteration: 25% 238/958 [00:17&lt;00:58, 12.38it/s] Iteration: 25% 240/958 [00:17&lt;00:59, 12.14it/s] Iteration: 25% 242/958 [00:17&lt;00:56, 12.59it/s] Iteration: 25% 244/958 [00:17&lt;00:54, 13.20it/s] Iteration: 26% 246/958 [00:17&lt;00:52, 13.50it/s] Iteration: 26% 248/958 [00:17&lt;00:51, 13.89it/s] Iteration: 26% 250/958 [00:18&lt;00:50, 14.13it/s] Iteration: 26% 252/958 [00:18&lt;00:49, 14.31it/s] Iteration: 27% 254/958 [00:18&lt;00:53, 13.06it/s] Iteration: 27% 256/958 [00:18&lt;00:55, 12.55it/s] Iteration: 27% 258/958 [00:18&lt;00:53, 13.01it/s] Iteration: 27% 260/958 [00:18&lt;00:51, 13.59it/s] Iteration: 27% 262/958 [00:19&lt;00:50, 13.89it/s] Iteration: 28% 264/958 [00:19&lt;00:49, 13.96it/s] Iteration: 28% 266/958 [00:19&lt;00:48, 14.15it/s] Iteration: 28% 268/958 [00:19&lt;00:49, 14.06it/s] Iteration: 28% 270/958 [00:19&lt;00:50, 13.65it/s] Iteration: 28% 272/958 [00:19&lt;00:53, 12.77it/s] Iteration: 29% 274/958 [00:19&lt;00:51, 13.31it/s] Iteration: 29% 276/958 [00:20&lt;00:49, 13.66it/s] Iteration: 29% 278/958 [00:20&lt;00:48, 14.02it/s] Iteration: 29% 280/958 [00:20&lt;00:49, 13.81it/s] Iteration: 29% 282/958 [00:20&lt;00:48, 13.84it/s] Iteration: 30% 284/958 [00:20&lt;00:49, 13.66it/s] Iteration: 30% 286/958 [00:20&lt;00:48, 13.90it/s] Iteration: 30% 288/958 [00:20&lt;00:47, 14.13it/s] Iteration: 30% 290/958 [00:21&lt;00:46, 14.31it/s] Iteration: 30% 292/958 [00:21&lt;00:46, 14.37it/s] Iteration: 31% 294/958 [00:21&lt;00:45, 14.46it/s] Iteration: 31% 296/958 [00:21&lt;00:45, 14.49it/s] Iteration: 31% 298/958 [00:21&lt;00:46, 14.24it/s] Iteration: 31% 300/958 [00:21&lt;00:46, 14.25it/s] Iteration: 32% 302/958 [00:21&lt;00:45, 14.45it/s] Iteration: 32% 304/958 [00:21&lt;00:44, 14.62it/s] Iteration: 32% 306/958 [00:22&lt;00:44, 14.63it/s] Iteration: 32% 308/958 [00:22&lt;00:44, 14.58it/s] Iteration: 32% 310/958 [00:22&lt;00:44, 14.58it/s] Iteration: 33% 312/958 [00:22&lt;00:46, 14.04it/s] Iteration: 33% 314/958 [00:22&lt;00:50, 12.74it/s] Iteration: 33% 316/958 [00:22&lt;00:48, 13.11it/s] Iteration: 33% 318/958 [00:23&lt;00:48, 13.27it/s] Iteration: 33% 320/958 [00:23&lt;00:48, 13.14it/s] Iteration: 34% 322/958 [00:23&lt;00:48, 13.24it/s] Iteration: 34% 324/958 [00:23&lt;00:48, 12.98it/s] Iteration: 34% 326/958 [00:23&lt;00:52, 12.10it/s] Iteration: 34% 328/958 [00:23&lt;00:51, 12.24it/s] Iteration: 34% 330/958 [00:24&lt;00:53, 11.72it/s] Iteration: 35% 332/958 [00:24&lt;00:55, 11.28it/s] Iteration: 35% 334/958 [00:24&lt;00:55, 11.33it/s] Iteration: 35% 336/958 [00:24&lt;00:52, 11.87it/s] Iteration: 35% 338/958 [00:24&lt;00:50, 12.38it/s] Iteration: 35% 340/958 [00:24&lt;00:47, 12.95it/s] Iteration: 36% 342/958 [00:24&lt;00:46, 13.33it/s] Iteration: 36% 344/958 [00:25&lt;00:45, 13.54it/s] Iteration: 36% 346/958 [00:25&lt;00:44, 13.81it/s] Iteration: 36% 348/958 [00:25&lt;00:43, 14.08it/s] Iteration: 37% 350/958 [00:25&lt;00:43, 13.85it/s] Iteration: 37% 352/958 [00:25&lt;00:43, 13.88it/s] Iteration: 37% 354/958 [00:25&lt;00:43, 14.04it/s] Iteration: 37% 356/958 [00:25&lt;00:42, 14.18it/s] Iteration: 37% 358/958 [00:26&lt;00:41, 14.41it/s] Iteration: 38% 360/958 [00:26&lt;00:42, 14.15it/s] Iteration: 38% 362/958 [00:26&lt;00:41, 14.21it/s] Iteration: 38% 364/958 [00:26&lt;00:41, 14.44it/s] Iteration: 38% 366/958 [00:26&lt;00:41, 14.18it/s] Iteration: 38% 368/958 [00:26&lt;00:42, 13.95it/s] Iteration: 39% 370/958 [00:26&lt;00:41, 14.13it/s] Iteration: 39% 372/958 [00:27&lt;00:41, 14.19it/s] Iteration: 39% 374/958 [00:27&lt;00:40, 14.27it/s] Iteration: 39% 376/958 [00:27&lt;00:40, 14.29it/s] Iteration: 39% 378/958 [00:27&lt;00:40, 14.44it/s] Iteration: 40% 380/958 [00:27&lt;00:40, 14.23it/s] Iteration: 40% 382/958 [00:27&lt;00:41, 13.99it/s] Iteration: 40% 384/958 [00:27&lt;00:40, 14.04it/s] Iteration: 40% 386/958 [00:28&lt;00:40, 14.22it/s] Iteration: 41% 388/958 [00:28&lt;00:39, 14.45it/s] Iteration: 41% 390/958 [00:28&lt;00:38, 14.59it/s] Iteration: 41% 392/958 [00:28&lt;00:39, 14.35it/s] Iteration: 41% 394/958 [00:28&lt;00:39, 14.23it/s] Iteration: 41% 396/958 [00:28&lt;00:40, 14.03it/s] Iteration: 42% 398/958 [00:28&lt;00:40, 13.92it/s] Iteration: 42% 400/958 [00:29&lt;00:40, 13.94it/s] Iteration: 42% 402/958 [00:29&lt;00:39, 14.18it/s] Iteration: 42% 404/958 [00:29&lt;00:38, 14.26it/s] Iteration: 42% 406/958 [00:29&lt;00:41, 13.20it/s] Iteration: 43% 408/958 [00:29&lt;00:40, 13.43it/s] Iteration: 43% 410/958 [00:29&lt;00:40, 13.48it/s] Iteration: 43% 412/958 [00:29&lt;00:39, 13.75it/s] Iteration: 43% 414/958 [00:30&lt;00:38, 14.03it/s] Iteration: 43% 416/958 [00:30&lt;00:38, 14.20it/s] Iteration: 44% 418/958 [00:30&lt;00:37, 14.54it/s] Iteration: 44% 420/958 [00:30&lt;00:37, 14.48it/s] Iteration: 44% 422/958 [00:30&lt;00:37, 14.26it/s] Iteration: 44% 424/958 [00:30&lt;00:37, 14.16it/s] Iteration: 44% 426/958 [00:30&lt;00:37, 14.32it/s] Iteration: 45% 428/958 [00:31&lt;00:37, 14.30it/s] Iteration: 45% 430/958 [00:31&lt;00:36, 14.46it/s] Iteration: 45% 432/958 [00:31&lt;00:36, 14.48it/s] Iteration: 45% 434/958 [00:31&lt;00:39, 13.25it/s] Iteration: 46% 436/958 [00:31&lt;00:41, 12.73it/s] Iteration: 46% 438/958 [00:31&lt;00:41, 12.68it/s] Iteration: 46% 440/958 [00:32&lt;00:40, 12.64it/s] Iteration: 46% 442/958 [00:32&lt;00:42, 12.01it/s] Iteration: 46% 444/958 [00:32&lt;00:44, 11.59it/s] Iteration: 47% 446/958 [00:32&lt;00:44, 11.55it/s] Iteration: 47% 448/958 [00:32&lt;00:41, 12.16it/s] Iteration: 47% 450/958 [00:32&lt;00:40, 12.62it/s] Iteration: 47% 452/958 [00:32&lt;00:38, 13.13it/s] Iteration: 47% 454/958 [00:33&lt;00:37, 13.46it/s] Iteration: 48% 456/958 [00:33&lt;00:36, 13.75it/s] Iteration: 48% 458/958 [00:33&lt;00:35, 14.12it/s] Iteration: 48% 460/958 [00:33&lt;00:35, 14.03it/s] Iteration: 48% 462/958 [00:33&lt;00:35, 13.78it/s] Iteration: 48% 464/958 [00:33&lt;00:35, 14.08it/s] Iteration: 49% 466/958 [00:33&lt;00:35, 14.02it/s] Iteration: 49% 468/958 [00:34&lt;00:34, 14.39it/s] Iteration: 49% 470/958 [00:34&lt;00:33, 14.46it/s] Iteration: 49% 472/958 [00:34&lt;00:33, 14.61it/s] Iteration: 49% 474/958 [00:34&lt;00:33, 14.54it/s] Iteration: 50% 476/958 [00:34&lt;00:35, 13.66it/s] Iteration: 50% 478/958 [00:34&lt;00:35, 13.59it/s] Iteration: 50% 480/958 [00:34&lt;00:35, 13.62it/s] Iteration: 50% 482/958 [00:35&lt;00:33, 14.03it/s] Iteration: 51% 484/958 [00:35&lt;00:33, 14.10it/s] Iteration: 51% 486/958 [00:35&lt;00:32, 14.38it/s] Iteration: 51% 488/958 [00:35&lt;00:32, 14.50it/s] Iteration: 51% 490/958 [00:35&lt;00:34, 13.47it/s] Iteration: 51% 492/958 [00:35&lt;00:36, 12.66it/s] Iteration: 52% 494/958 [00:36&lt;00:39, 11.89it/s] Iteration: 52% 496/958 [00:36&lt;00:40, 11.54it/s] Iteration: 52% 498/958 [00:36&lt;00:40, 11.49it/s] Iteration: 52% 500/958 [00:36&lt;00:40, 11.32it/s] Iteration: 52% 502/958 [00:36&lt;00:37, 12.01it/s] Iteration: 53% 504/958 [00:36&lt;00:35, 12.69it/s] Iteration: 53% 506/958 [00:37&lt;00:34, 13.08it/s] Iteration: 53% 508/958 [00:37&lt;00:33, 13.53it/s] Iteration: 53% 510/958 [00:37&lt;00:32, 13.92it/s] Iteration: 53% 512/958 [00:37&lt;00:32, 13.90it/s] Iteration: 54% 514/958 [00:37&lt;00:31, 14.03it/s] Iteration: 54% 516/958 [00:37&lt;00:33, 13.02it/s] Iteration: 54% 518/958 [00:37&lt;00:35, 12.36it/s] Iteration: 54% 520/958 [00:38&lt;00:37, 11.70it/s] Iteration: 54% 522/958 [00:38&lt;00:37, 11.51it/s] Iteration: 55% 524/958 [00:38&lt;00:38, 11.14it/s] Iteration: 55% 526/958 [00:38&lt;00:36, 11.80it/s] Iteration: 55% 528/958 [00:38&lt;00:34, 12.50it/s] Iteration: 55% 530/958 [00:38&lt;00:32, 13.04it/s] Iteration: 56% 532/958 [00:39&lt;00:31, 13.47it/s] Iteration: 56% 534/958 [00:39&lt;00:30, 13.80it/s] Iteration: 56% 536/958 [00:39&lt;00:30, 13.71it/s] Iteration: 56% 538/958 [00:39&lt;00:30, 13.89it/s] Iteration: 56% 540/958 [00:39&lt;00:31, 13.29it/s]{&#39;loss&#39;: 0.26624603271484376, &#39;learning_rate&#39;: 2.3903966597077245e-05, &#39;epoch&#39;: 1.5657620041753653, &#39;total_flos&#39;: 1148471673738240, &#39;step&#39;: 1500} Iteration: 57% 542/958 [00:39&lt;00:33, 12.29it/s] Iteration: 57% 544/958 [00:40&lt;00:35, 11.72it/s] Iteration: 57% 546/958 [00:40&lt;00:35, 11.62it/s] Iteration: 57% 548/958 [00:40&lt;00:35, 11.51it/s] Iteration: 57% 550/958 [00:40&lt;00:35, 11.51it/s] Iteration: 58% 552/958 [00:40&lt;00:33, 12.28it/s] Iteration: 58% 554/958 [00:40&lt;00:31, 12.80it/s] Iteration: 58% 556/958 [00:41&lt;00:32, 12.29it/s] Iteration: 58% 558/958 [00:41&lt;00:34, 11.55it/s] Iteration: 58% 560/958 [00:41&lt;00:34, 11.39it/s] Iteration: 59% 562/958 [00:41&lt;00:32, 12.26it/s] Iteration: 59% 564/958 [00:41&lt;00:33, 11.86it/s] Iteration: 59% 566/958 [00:41&lt;00:34, 11.50it/s] Iteration: 59% 568/958 [00:42&lt;00:35, 11.04it/s] Iteration: 59% 570/958 [00:42&lt;00:35, 10.81it/s] Iteration: 60% 572/958 [00:42&lt;00:36, 10.65it/s] Iteration: 60% 574/958 [00:42&lt;00:34, 11.17it/s] Iteration: 60% 576/958 [00:42&lt;00:32, 11.86it/s] Iteration: 60% 578/958 [00:42&lt;00:29, 12.68it/s] Iteration: 61% 580/958 [00:43&lt;00:28, 13.11it/s] Iteration: 61% 582/958 [00:43&lt;00:28, 13.36it/s] Iteration: 61% 584/958 [00:43&lt;00:27, 13.77it/s] Iteration: 61% 586/958 [00:43&lt;00:27, 13.78it/s] Iteration: 61% 588/958 [00:43&lt;00:26, 13.83it/s] Iteration: 62% 590/958 [00:43&lt;00:26, 13.87it/s] Iteration: 62% 592/958 [00:43&lt;00:26, 13.68it/s] Iteration: 62% 594/958 [00:44&lt;00:26, 13.72it/s] Iteration: 62% 596/958 [00:44&lt;00:26, 13.68it/s] Iteration: 62% 598/958 [00:44&lt;00:25, 13.93it/s] Iteration: 63% 600/958 [00:44&lt;00:25, 14.08it/s] Iteration: 63% 602/958 [00:44&lt;00:25, 14.15it/s] Iteration: 63% 604/958 [00:44&lt;00:24, 14.20it/s] Iteration: 63% 606/958 [00:44&lt;00:24, 14.28it/s] Iteration: 63% 608/958 [00:45&lt;00:24, 14.33it/s] Iteration: 64% 610/958 [00:45&lt;00:24, 14.14it/s] Iteration: 64% 612/958 [00:45&lt;00:24, 14.26it/s] Iteration: 64% 614/958 [00:45&lt;00:23, 14.45it/s] Iteration: 64% 616/958 [00:45&lt;00:24, 14.10it/s] Iteration: 65% 618/958 [00:45&lt;00:23, 14.29it/s] Iteration: 65% 620/958 [00:45&lt;00:23, 14.34it/s] Iteration: 65% 622/958 [00:46&lt;00:23, 14.47it/s] Iteration: 65% 624/958 [00:46&lt;00:23, 14.02it/s] Iteration: 65% 626/958 [00:46&lt;00:23, 14.31it/s] Iteration: 66% 628/958 [00:46&lt;00:22, 14.40it/s] Iteration: 66% 630/958 [00:46&lt;00:22, 14.56it/s] Iteration: 66% 632/958 [00:46&lt;00:22, 14.71it/s] Iteration: 66% 634/958 [00:46&lt;00:22, 14.56it/s] Iteration: 66% 636/958 [00:46&lt;00:22, 14.11it/s] Iteration: 67% 638/958 [00:47&lt;00:22, 14.40it/s] Iteration: 67% 640/958 [00:47&lt;00:22, 14.28it/s] Iteration: 67% 642/958 [00:47&lt;00:21, 14.54it/s] Iteration: 67% 644/958 [00:47&lt;00:21, 14.39it/s] Iteration: 67% 646/958 [00:47&lt;00:21, 14.57it/s] Iteration: 68% 648/958 [00:47&lt;00:21, 14.66it/s] Iteration: 68% 650/958 [00:47&lt;00:21, 14.41it/s] Iteration: 68% 652/958 [00:48&lt;00:20, 14.64it/s] Iteration: 68% 654/958 [00:48&lt;00:21, 14.24it/s] Iteration: 68% 656/958 [00:48&lt;00:22, 13.69it/s] Iteration: 69% 658/958 [00:48&lt;00:22, 13.08it/s] Iteration: 69% 660/958 [00:48&lt;00:23, 12.44it/s] Iteration: 69% 662/958 [00:48&lt;00:22, 12.92it/s] Iteration: 69% 664/958 [00:49&lt;00:21, 13.42it/s] Iteration: 70% 666/958 [00:49&lt;00:21, 13.77it/s] Iteration: 70% 668/958 [00:49&lt;00:20, 13.87it/s] Iteration: 70% 670/958 [00:49&lt;00:20, 13.72it/s] Iteration: 70% 672/958 [00:49&lt;00:20, 14.03it/s] Iteration: 70% 674/958 [00:49&lt;00:19, 14.34it/s] Iteration: 71% 676/958 [00:49&lt;00:19, 14.32it/s] Iteration: 71% 678/958 [00:49&lt;00:19, 14.31it/s] Iteration: 71% 680/958 [00:50&lt;00:19, 14.60it/s] Iteration: 71% 682/958 [00:50&lt;00:19, 14.24it/s] Iteration: 71% 684/958 [00:50&lt;00:19, 14.42it/s] Iteration: 72% 686/958 [00:50&lt;00:18, 14.46it/s] Iteration: 72% 688/958 [00:50&lt;00:18, 14.52it/s] Iteration: 72% 690/958 [00:50&lt;00:18, 14.61it/s] Iteration: 72% 692/958 [00:50&lt;00:18, 14.50it/s] Iteration: 72% 694/958 [00:51&lt;00:18, 14.49it/s] Iteration: 73% 696/958 [00:51&lt;00:18, 14.44it/s] Iteration: 73% 698/958 [00:51&lt;00:18, 14.41it/s] Iteration: 73% 700/958 [00:51&lt;00:17, 14.45it/s] Iteration: 73% 702/958 [00:51&lt;00:17, 14.45it/s] Iteration: 73% 704/958 [00:51&lt;00:17, 14.29it/s] Iteration: 74% 706/958 [00:51&lt;00:17, 14.07it/s] Iteration: 74% 708/958 [00:52&lt;00:17, 14.36it/s] Iteration: 74% 710/958 [00:52&lt;00:16, 14.63it/s] Iteration: 74% 712/958 [00:52&lt;00:17, 14.38it/s] Iteration: 75% 714/958 [00:52&lt;00:16, 14.41it/s] Iteration: 75% 716/958 [00:52&lt;00:16, 14.46it/s] Iteration: 75% 718/958 [00:52&lt;00:16, 14.40it/s] Iteration: 75% 720/958 [00:52&lt;00:16, 14.38it/s] Iteration: 75% 722/958 [00:53&lt;00:16, 14.60it/s] Iteration: 76% 724/958 [00:53&lt;00:16, 14.54it/s] Iteration: 76% 726/958 [00:53&lt;00:16, 14.46it/s] Iteration: 76% 728/958 [00:53&lt;00:16, 14.12it/s] Iteration: 76% 730/958 [00:53&lt;00:17, 13.23it/s] Iteration: 76% 732/958 [00:53&lt;00:17, 12.96it/s] Iteration: 77% 734/958 [00:53&lt;00:17, 13.02it/s] Iteration: 77% 736/958 [00:54&lt;00:16, 13.53it/s] Iteration: 77% 738/958 [00:54&lt;00:15, 13.94it/s] Iteration: 77% 740/958 [00:54&lt;00:15, 13.79it/s] Iteration: 77% 742/958 [00:54&lt;00:15, 14.09it/s] Iteration: 78% 744/958 [00:54&lt;00:14, 14.32it/s] Iteration: 78% 746/958 [00:54&lt;00:14, 14.44it/s] Iteration: 78% 748/958 [00:54&lt;00:14, 14.81it/s] Iteration: 78% 750/958 [00:55&lt;00:14, 14.56it/s] Iteration: 78% 752/958 [00:55&lt;00:14, 14.56it/s] Iteration: 79% 754/958 [00:55&lt;00:13, 14.68it/s] Iteration: 79% 756/958 [00:55&lt;00:13, 14.57it/s] Iteration: 79% 758/958 [00:55&lt;00:14, 13.92it/s] Iteration: 79% 760/958 [00:55&lt;00:14, 14.02it/s] Iteration: 80% 762/958 [00:55&lt;00:13, 14.36it/s] Iteration: 80% 764/958 [00:56&lt;00:13, 14.29it/s] Iteration: 80% 766/958 [00:56&lt;00:13, 14.71it/s] Iteration: 80% 768/958 [00:56&lt;00:12, 14.69it/s] Iteration: 80% 770/958 [00:56&lt;00:13, 14.12it/s] Iteration: 81% 772/958 [00:56&lt;00:13, 14.00it/s] Iteration: 81% 774/958 [00:56&lt;00:12, 14.32it/s] Iteration: 81% 776/958 [00:56&lt;00:12, 14.37it/s] Iteration: 81% 778/958 [00:57&lt;00:12, 14.09it/s] Iteration: 81% 780/958 [00:57&lt;00:12, 14.32it/s] Iteration: 82% 782/958 [00:57&lt;00:12, 14.42it/s] Iteration: 82% 784/958 [00:57&lt;00:12, 14.44it/s] Iteration: 82% 786/958 [00:57&lt;00:11, 14.48it/s] Iteration: 82% 788/958 [00:57&lt;00:12, 13.41it/s] Iteration: 82% 790/958 [00:57&lt;00:13, 12.59it/s] Iteration: 83% 792/958 [00:58&lt;00:13, 12.02it/s] Iteration: 83% 794/958 [00:58&lt;00:13, 11.72it/s] Iteration: 83% 796/958 [00:58&lt;00:14, 11.26it/s] Iteration: 83% 798/958 [00:58&lt;00:13, 11.71it/s] Iteration: 84% 800/958 [00:58&lt;00:12, 12.45it/s] Iteration: 84% 802/958 [00:58&lt;00:12, 12.72it/s] Iteration: 84% 804/958 [00:59&lt;00:11, 13.19it/s] Iteration: 84% 806/958 [00:59&lt;00:11, 13.81it/s] Iteration: 84% 808/958 [00:59&lt;00:10, 14.17it/s] Iteration: 85% 810/958 [00:59&lt;00:10, 14.02it/s] Iteration: 85% 812/958 [00:59&lt;00:10, 14.28it/s] Iteration: 85% 814/958 [00:59&lt;00:09, 14.46it/s] Iteration: 85% 816/958 [00:59&lt;00:09, 14.72it/s] Iteration: 85% 818/958 [00:59&lt;00:09, 14.62it/s] Iteration: 86% 820/958 [01:00&lt;00:09, 14.70it/s] Iteration: 86% 822/958 [01:00&lt;00:09, 14.75it/s] Iteration: 86% 824/958 [01:00&lt;00:08, 14.92it/s] Iteration: 86% 826/958 [01:00&lt;00:09, 14.63it/s] Iteration: 86% 828/958 [01:00&lt;00:08, 14.57it/s] Iteration: 87% 830/958 [01:00&lt;00:08, 14.55it/s] Iteration: 87% 832/958 [01:00&lt;00:08, 14.61it/s] Iteration: 87% 834/958 [01:01&lt;00:08, 14.71it/s] Iteration: 87% 836/958 [01:01&lt;00:08, 14.75it/s] Iteration: 87% 838/958 [01:01&lt;00:08, 13.98it/s] Iteration: 88% 840/958 [01:01&lt;00:08, 13.67it/s] Iteration: 88% 842/958 [01:01&lt;00:08, 13.73it/s] Iteration: 88% 844/958 [01:01&lt;00:08, 14.17it/s] Iteration: 88% 846/958 [01:01&lt;00:07, 14.31it/s] Iteration: 89% 848/958 [01:02&lt;00:07, 14.31it/s] Iteration: 89% 850/958 [01:02&lt;00:07, 14.58it/s] Iteration: 89% 852/958 [01:02&lt;00:07, 14.78it/s] Iteration: 89% 854/958 [01:02&lt;00:07, 14.80it/s] Iteration: 89% 856/958 [01:02&lt;00:07, 14.50it/s] Iteration: 90% 858/958 [01:02&lt;00:06, 14.68it/s] Iteration: 90% 860/958 [01:02&lt;00:06, 14.77it/s] Iteration: 90% 862/958 [01:03&lt;00:06, 14.65it/s] Iteration: 90% 864/958 [01:03&lt;00:06, 14.79it/s] Iteration: 90% 866/958 [01:03&lt;00:06, 14.90it/s] Iteration: 91% 868/958 [01:03&lt;00:06, 14.53it/s] Iteration: 91% 870/958 [01:03&lt;00:06, 14.51it/s] Iteration: 91% 872/958 [01:03&lt;00:05, 14.86it/s] Iteration: 91% 874/958 [01:03&lt;00:05, 14.56it/s] Iteration: 91% 876/958 [01:03&lt;00:05, 14.84it/s] Iteration: 92% 878/958 [01:04&lt;00:05, 13.52it/s] Iteration: 92% 880/958 [01:04&lt;00:05, 13.03it/s] Iteration: 92% 882/958 [01:04&lt;00:06, 12.63it/s] Iteration: 92% 884/958 [01:04&lt;00:05, 12.71it/s] Iteration: 92% 886/958 [01:04&lt;00:05, 13.33it/s] Iteration: 93% 888/958 [01:04&lt;00:05, 13.81it/s] Iteration: 93% 890/958 [01:05&lt;00:04, 13.86it/s] Iteration: 93% 892/958 [01:05&lt;00:04, 14.23it/s] Iteration: 93% 894/958 [01:05&lt;00:04, 14.48it/s] Iteration: 94% 896/958 [01:05&lt;00:04, 14.44it/s] Iteration: 94% 898/958 [01:05&lt;00:04, 14.37it/s] Iteration: 94% 900/958 [01:05&lt;00:03, 14.71it/s] Iteration: 94% 902/958 [01:05&lt;00:03, 14.77it/s] Iteration: 94% 904/958 [01:05&lt;00:03, 14.83it/s] Iteration: 95% 906/958 [01:06&lt;00:03, 14.83it/s] Iteration: 95% 908/958 [01:06&lt;00:03, 14.88it/s] Iteration: 95% 910/958 [01:06&lt;00:03, 13.83it/s] Iteration: 95% 912/958 [01:06&lt;00:03, 12.97it/s] Iteration: 95% 914/958 [01:06&lt;00:03, 12.53it/s] Iteration: 96% 916/958 [01:06&lt;00:03, 12.89it/s] Iteration: 96% 918/958 [01:07&lt;00:03, 12.18it/s] Iteration: 96% 920/958 [01:07&lt;00:03, 11.80it/s] Iteration: 96% 922/958 [01:07&lt;00:02, 12.08it/s] Iteration: 96% 924/958 [01:07&lt;00:02, 12.73it/s] Iteration: 97% 926/958 [01:07&lt;00:02, 13.20it/s] Iteration: 97% 928/958 [01:07&lt;00:02, 13.56it/s] Iteration: 97% 930/958 [01:07&lt;00:02, 13.96it/s] Iteration: 97% 932/958 [01:08&lt;00:01, 14.32it/s] Iteration: 97% 934/958 [01:08&lt;00:01, 14.50it/s] Iteration: 98% 936/958 [01:08&lt;00:01, 14.28it/s] Iteration: 98% 938/958 [01:08&lt;00:01, 13.41it/s] Iteration: 98% 940/958 [01:08&lt;00:01, 13.68it/s] Iteration: 98% 942/958 [01:08&lt;00:01, 14.01it/s] Iteration: 99% 944/958 [01:08&lt;00:00, 14.17it/s] Iteration: 99% 946/958 [01:09&lt;00:00, 14.28it/s] Iteration: 99% 948/958 [01:09&lt;00:00, 14.56it/s] Iteration: 99% 950/958 [01:09&lt;00:00, 14.46it/s] Iteration: 99% 952/958 [01:09&lt;00:00, 14.52it/s] Iteration: 100% 954/958 [01:09&lt;00:00, 14.15it/s] Iteration: 100% 956/958 [01:09&lt;00:00, 14.29it/s] Iteration: 100% 958/958 [01:09&lt;00:00, 13.70it/s] Epoch: 67% 2/3 [02:20&lt;01:10, 70.22s/it] Iteration: 0% 0/958 [00:00&lt;?, ?it/s] Iteration: 0% 2/958 [00:00&lt;01:04, 14.77it/s] Iteration: 0% 4/958 [00:00&lt;01:05, 14.54it/s] Iteration: 1% 6/958 [00:00&lt;01:04, 14.75it/s] Iteration: 1% 8/958 [00:00&lt;01:04, 14.84it/s] Iteration: 1% 10/958 [00:00&lt;01:03, 14.89it/s] Iteration: 1% 12/958 [00:00&lt;01:04, 14.69it/s] Iteration: 1% 14/958 [00:00&lt;01:03, 14.75it/s] Iteration: 2% 16/958 [00:01&lt;01:03, 14.78it/s] Iteration: 2% 18/958 [00:01&lt;01:03, 14.77it/s] Iteration: 2% 20/958 [00:01&lt;01:03, 14.76it/s] Iteration: 2% 22/958 [00:01&lt;01:03, 14.79it/s] Iteration: 3% 24/958 [00:01&lt;01:03, 14.82it/s] Iteration: 3% 26/958 [00:01&lt;01:03, 14.76it/s] Iteration: 3% 28/958 [00:01&lt;01:04, 14.41it/s] Iteration: 3% 30/958 [00:02&lt;01:04, 14.48it/s] Iteration: 3% 32/958 [00:02&lt;01:03, 14.62it/s] Iteration: 4% 34/958 [00:02&lt;01:03, 14.56it/s] Iteration: 4% 36/958 [00:02&lt;01:05, 14.15it/s] Iteration: 4% 38/958 [00:02&lt;01:08, 13.41it/s] Iteration: 4% 40/958 [00:02&lt;01:07, 13.69it/s] Iteration: 4% 42/958 [00:02&lt;01:07, 13.59it/s] Iteration: 5% 44/958 [00:03&lt;01:04, 14.15it/s] Iteration: 5% 46/958 [00:03&lt;01:03, 14.45it/s] Iteration: 5% 48/958 [00:03&lt;01:02, 14.47it/s] Iteration: 5% 50/958 [00:03&lt;01:02, 14.61it/s] Iteration: 5% 52/958 [00:03&lt;01:01, 14.69it/s] Iteration: 6% 54/958 [00:03&lt;01:01, 14.65it/s] Iteration: 6% 56/958 [00:03&lt;01:02, 14.38it/s] Iteration: 6% 58/958 [00:04&lt;01:02, 14.47it/s] Iteration: 6% 60/958 [00:04&lt;01:01, 14.66it/s] Iteration: 6% 62/958 [00:04&lt;01:00, 14.78it/s] Iteration: 7% 64/958 [00:04&lt;00:59, 15.05it/s] Iteration: 7% 66/958 [00:04&lt;01:00, 14.72it/s] Iteration: 7% 68/958 [00:04&lt;01:00, 14.79it/s] Iteration: 7% 70/958 [00:04&lt;00:59, 14.90it/s] Iteration: 8% 72/958 [00:04&lt;01:00, 14.62it/s] Iteration: 8% 74/958 [00:05&lt;01:00, 14.70it/s] Iteration: 8% 76/958 [00:05&lt;00:59, 14.90it/s] Iteration: 8% 78/958 [00:05&lt;00:59, 14.72it/s] Iteration: 8% 80/958 [00:05&lt;00:59, 14.80it/s] Iteration: 9% 82/958 [00:05&lt;00:58, 14.91it/s]{&#39;loss&#39;: 0.2389298095703125, &#39;learning_rate&#39;: 1.5205288796102993e-05, &#39;epoch&#39;: 2.0876826722338206, &#39;total_flos&#39;: 1531040263956480, &#39;step&#39;: 2000} Iteration: 9% 84/958 [00:05&lt;00:59, 14.79it/s] Iteration: 9% 86/958 [00:05&lt;00:59, 14.60it/s] Iteration: 9% 88/958 [00:06&lt;00:59, 14.67it/s] Iteration: 9% 90/958 [00:06&lt;00:59, 14.59it/s] Iteration: 10% 92/958 [00:06&lt;01:00, 14.40it/s] Iteration: 10% 94/958 [00:06&lt;00:59, 14.44it/s] Iteration: 10% 96/958 [00:06&lt;01:01, 14.02it/s] Iteration: 10% 98/958 [00:06&lt;01:00, 14.19it/s] Iteration: 10% 100/958 [00:06&lt;00:59, 14.36it/s] Iteration: 11% 102/958 [00:07&lt;01:00, 14.13it/s] Iteration: 11% 104/958 [00:07&lt;01:00, 14.12it/s] Iteration: 11% 106/958 [00:07&lt;01:00, 14.02it/s] Iteration: 11% 108/958 [00:07&lt;00:59, 14.31it/s] Iteration: 11% 110/958 [00:07&lt;00:59, 14.23it/s] Iteration: 12% 112/958 [00:07&lt;00:58, 14.40it/s] Iteration: 12% 114/958 [00:07&lt;00:58, 14.46it/s] Iteration: 12% 116/958 [00:08&lt;00:59, 14.14it/s] Iteration: 12% 118/958 [00:08&lt;00:57, 14.52it/s] Iteration: 13% 120/958 [00:08&lt;00:56, 14.71it/s] Iteration: 13% 122/958 [00:08&lt;00:57, 14.65it/s] Iteration: 13% 124/958 [00:08&lt;00:55, 14.91it/s] Iteration: 13% 126/958 [00:08&lt;00:57, 14.50it/s] Iteration: 13% 128/958 [00:08&lt;01:00, 13.78it/s] Iteration: 14% 130/958 [00:08&lt;00:59, 13.89it/s] Iteration: 14% 132/958 [00:09&lt;00:57, 14.29it/s] Iteration: 14% 134/958 [00:09&lt;00:56, 14.49it/s] Iteration: 14% 136/958 [00:09&lt;00:56, 14.58it/s] Iteration: 14% 138/958 [00:09&lt;00:56, 14.63it/s] Iteration: 15% 140/958 [00:09&lt;00:55, 14.66it/s] Iteration: 15% 142/958 [00:09&lt;00:55, 14.74it/s] Iteration: 15% 144/958 [00:09&lt;00:54, 14.81it/s] Iteration: 15% 146/958 [00:10&lt;00:55, 14.56it/s] Iteration: 15% 148/958 [00:10&lt;00:55, 14.53it/s] Iteration: 16% 150/958 [00:10&lt;00:56, 14.40it/s] Iteration: 16% 152/958 [00:10&lt;00:54, 14.80it/s] Iteration: 16% 154/958 [00:10&lt;00:54, 14.82it/s] Iteration: 16% 156/958 [00:10&lt;00:53, 14.96it/s] Iteration: 16% 158/958 [00:10&lt;00:53, 14.90it/s] Iteration: 17% 160/958 [00:11&lt;00:54, 14.62it/s] Iteration: 17% 162/958 [00:11&lt;00:54, 14.64it/s] Iteration: 17% 164/958 [00:11&lt;00:53, 14.84it/s] Iteration: 17% 166/958 [00:11&lt;00:53, 14.69it/s] Iteration: 18% 168/958 [00:11&lt;00:53, 14.65it/s] Iteration: 18% 170/958 [00:11&lt;00:54, 14.56it/s] Iteration: 18% 172/958 [00:11&lt;00:53, 14.73it/s] Iteration: 18% 174/958 [00:11&lt;00:52, 14.92it/s] Iteration: 18% 176/958 [00:12&lt;00:53, 14.62it/s] Iteration: 19% 178/958 [00:12&lt;00:53, 14.70it/s] Iteration: 19% 180/958 [00:12&lt;00:52, 14.69it/s] Iteration: 19% 182/958 [00:12&lt;00:53, 14.54it/s] Iteration: 19% 184/958 [00:12&lt;00:53, 14.35it/s] Iteration: 19% 186/958 [00:12&lt;00:53, 14.31it/s] Iteration: 20% 188/958 [00:12&lt;00:55, 13.78it/s] Iteration: 20% 190/958 [00:13&lt;00:55, 13.90it/s] Iteration: 20% 192/958 [00:13&lt;00:53, 14.20it/s] Iteration: 20% 194/958 [00:13&lt;00:54, 14.14it/s] Iteration: 20% 196/958 [00:13&lt;00:52, 14.40it/s] Iteration: 21% 198/958 [00:13&lt;00:54, 13.91it/s] Iteration: 21% 200/958 [00:13&lt;00:57, 13.10it/s] Iteration: 21% 202/958 [00:13&lt;00:55, 13.56it/s] Iteration: 21% 204/958 [00:14&lt;00:58, 12.84it/s] Iteration: 22% 206/958 [00:14&lt;01:01, 12.28it/s] Iteration: 22% 208/958 [00:14&lt;01:00, 12.39it/s] Iteration: 22% 210/958 [00:14&lt;00:56, 13.13it/s] Iteration: 22% 212/958 [00:14&lt;00:54, 13.72it/s] Iteration: 22% 214/958 [00:14&lt;00:52, 14.16it/s] Iteration: 23% 216/958 [00:15&lt;00:50, 14.59it/s] Iteration: 23% 218/958 [00:15&lt;00:51, 14.28it/s] Iteration: 23% 220/958 [00:15&lt;00:51, 14.36it/s] Iteration: 23% 222/958 [00:15&lt;00:51, 14.32it/s] Iteration: 23% 224/958 [00:15&lt;00:51, 14.36it/s] Iteration: 24% 226/958 [00:15&lt;00:50, 14.52it/s] Iteration: 24% 228/958 [00:15&lt;00:50, 14.38it/s] Iteration: 24% 230/958 [00:15&lt;00:50, 14.45it/s] Iteration: 24% 232/958 [00:16&lt;00:50, 14.47it/s] Iteration: 24% 234/958 [00:16&lt;00:49, 14.60it/s] Iteration: 25% 236/958 [00:16&lt;00:49, 14.52it/s] Iteration: 25% 238/958 [00:16&lt;00:49, 14.65it/s] Iteration: 25% 240/958 [00:16&lt;00:48, 14.67it/s] Iteration: 25% 242/958 [00:16&lt;00:48, 14.64it/s] Iteration: 25% 244/958 [00:16&lt;00:48, 14.80it/s] Iteration: 26% 246/958 [00:17&lt;00:48, 14.78it/s] Iteration: 26% 248/958 [00:17&lt;00:48, 14.63it/s] Iteration: 26% 250/958 [00:17&lt;00:48, 14.57it/s] Iteration: 26% 252/958 [00:17&lt;00:48, 14.51it/s] Iteration: 27% 254/958 [00:17&lt;00:47, 14.87it/s] Iteration: 27% 256/958 [00:17&lt;00:48, 14.49it/s] Iteration: 27% 258/958 [00:17&lt;00:48, 14.51it/s] Iteration: 27% 260/958 [00:18&lt;00:47, 14.75it/s] Iteration: 27% 262/958 [00:18&lt;00:47, 14.56it/s] Iteration: 28% 264/958 [00:18&lt;00:47, 14.60it/s] Iteration: 28% 266/958 [00:18&lt;00:47, 14.52it/s] Iteration: 28% 268/958 [00:18&lt;00:47, 14.44it/s] Iteration: 28% 270/958 [00:18&lt;00:47, 14.63it/s] Iteration: 28% 272/958 [00:18&lt;00:46, 14.79it/s] Iteration: 29% 274/958 [00:18&lt;00:45, 14.92it/s] Iteration: 29% 276/958 [00:19&lt;00:45, 14.83it/s] Iteration: 29% 278/958 [00:19&lt;00:46, 14.61it/s] Iteration: 29% 280/958 [00:19&lt;00:46, 14.69it/s] Iteration: 29% 282/958 [00:19&lt;00:46, 14.45it/s] Iteration: 30% 284/958 [00:19&lt;00:46, 14.50it/s] Iteration: 30% 286/958 [00:19&lt;00:45, 14.62it/s] Iteration: 30% 288/958 [00:19&lt;00:45, 14.76it/s] Iteration: 30% 290/958 [00:20&lt;00:46, 14.22it/s] Iteration: 30% 292/958 [00:20&lt;00:51, 12.89it/s] Iteration: 31% 294/958 [00:20&lt;00:54, 12.20it/s] Iteration: 31% 296/958 [00:20&lt;00:54, 12.04it/s] Iteration: 31% 298/958 [00:20&lt;00:51, 12.75it/s] Iteration: 31% 300/958 [00:20&lt;00:49, 13.22it/s] Iteration: 32% 302/958 [00:21&lt;00:47, 13.77it/s] Iteration: 32% 304/958 [00:21&lt;00:46, 14.07it/s] Iteration: 32% 306/958 [00:21&lt;00:46, 14.05it/s] Iteration: 32% 308/958 [00:21&lt;00:46, 14.10it/s] Iteration: 32% 310/958 [00:21&lt;00:45, 14.28it/s] Iteration: 33% 312/958 [00:21&lt;00:44, 14.52it/s] Iteration: 33% 314/958 [00:21&lt;00:44, 14.52it/s] Iteration: 33% 316/958 [00:21&lt;00:43, 14.61it/s] Iteration: 33% 318/958 [00:22&lt;00:43, 14.68it/s] Iteration: 33% 320/958 [00:22&lt;00:44, 14.46it/s] Iteration: 34% 322/958 [00:22&lt;00:43, 14.60it/s] Iteration: 34% 324/958 [00:22&lt;00:43, 14.72it/s] Iteration: 34% 326/958 [00:22&lt;00:42, 14.92it/s] Iteration: 34% 328/958 [00:22&lt;00:42, 14.66it/s] Iteration: 34% 330/958 [00:22&lt;00:42, 14.70it/s] Iteration: 35% 332/958 [00:23&lt;00:42, 14.86it/s] Iteration: 35% 334/958 [00:23&lt;00:42, 14.81it/s] Iteration: 35% 336/958 [00:23&lt;00:42, 14.51it/s] Iteration: 35% 338/958 [00:23&lt;00:43, 14.40it/s] Iteration: 35% 340/958 [00:23&lt;00:43, 14.28it/s] Iteration: 36% 342/958 [00:23&lt;00:42, 14.52it/s] Iteration: 36% 344/958 [00:23&lt;00:41, 14.68it/s] Iteration: 36% 346/958 [00:24&lt;00:41, 14.70it/s] Iteration: 36% 348/958 [00:24&lt;00:41, 14.79it/s] Iteration: 37% 350/958 [00:24&lt;00:41, 14.66it/s] Iteration: 37% 352/958 [00:24&lt;00:40, 14.85it/s] Iteration: 37% 354/958 [00:24&lt;00:41, 14.73it/s] Iteration: 37% 356/958 [00:24&lt;00:40, 14.78it/s] Iteration: 37% 358/958 [00:24&lt;00:40, 14.82it/s] Iteration: 38% 360/958 [00:24&lt;00:39, 15.07it/s] Iteration: 38% 362/958 [00:25&lt;00:39, 15.02it/s] Iteration: 38% 364/958 [00:25&lt;00:39, 15.07it/s] Iteration: 38% 366/958 [00:25&lt;00:39, 14.84it/s] Iteration: 38% 368/958 [00:25&lt;00:40, 14.52it/s] Iteration: 39% 370/958 [00:25&lt;00:40, 14.56it/s] Iteration: 39% 372/958 [00:25&lt;00:40, 14.64it/s] Iteration: 39% 374/958 [00:25&lt;00:39, 14.65it/s] Iteration: 39% 376/958 [00:26&lt;00:39, 14.78it/s] Iteration: 39% 378/958 [00:26&lt;00:38, 14.96it/s] Iteration: 40% 380/958 [00:26&lt;00:39, 14.46it/s] Iteration: 40% 382/958 [00:26&lt;00:39, 14.62it/s] Iteration: 40% 384/958 [00:26&lt;00:39, 14.54it/s] Iteration: 40% 386/958 [00:26&lt;00:39, 14.64it/s] Iteration: 41% 388/958 [00:26&lt;00:38, 14.84it/s] Iteration: 41% 390/958 [00:27&lt;00:38, 14.65it/s] Iteration: 41% 392/958 [00:27&lt;00:38, 14.67it/s] Iteration: 41% 394/958 [00:27&lt;00:38, 14.75it/s] Iteration: 41% 396/958 [00:27&lt;00:38, 14.57it/s] Iteration: 42% 398/958 [00:27&lt;00:38, 14.64it/s] Iteration: 42% 400/958 [00:27&lt;00:37, 14.79it/s] Iteration: 42% 402/958 [00:27&lt;00:37, 14.76it/s] Iteration: 42% 404/958 [00:27&lt;00:37, 14.75it/s] Iteration: 42% 406/958 [00:28&lt;00:37, 14.82it/s] Iteration: 43% 408/958 [00:28&lt;00:36, 14.96it/s] Iteration: 43% 410/958 [00:28&lt;00:37, 14.78it/s] Iteration: 43% 412/958 [00:28&lt;00:37, 14.41it/s] Iteration: 43% 414/958 [00:28&lt;00:38, 13.96it/s] Iteration: 43% 416/958 [00:28&lt;00:39, 13.65it/s] Iteration: 44% 418/958 [00:28&lt;00:38, 13.93it/s] Iteration: 44% 420/958 [00:29&lt;00:37, 14.39it/s] Iteration: 44% 422/958 [00:29&lt;00:36, 14.65it/s] Iteration: 44% 424/958 [00:29&lt;00:36, 14.53it/s] Iteration: 44% 426/958 [00:29&lt;00:39, 13.39it/s] Iteration: 45% 428/958 [00:29&lt;00:41, 12.65it/s] Iteration: 45% 430/958 [00:29&lt;00:41, 12.81it/s] Iteration: 45% 432/958 [00:30&lt;00:38, 13.55it/s] Iteration: 45% 434/958 [00:30&lt;00:37, 14.08it/s] Iteration: 46% 436/958 [00:30&lt;00:36, 14.42it/s] Iteration: 46% 438/958 [00:30&lt;00:35, 14.58it/s] Iteration: 46% 440/958 [00:30&lt;00:36, 14.35it/s] Iteration: 46% 442/958 [00:30&lt;00:36, 14.23it/s] Iteration: 46% 444/958 [00:30&lt;00:36, 14.27it/s] Iteration: 47% 446/958 [00:30&lt;00:35, 14.49it/s] Iteration: 47% 448/958 [00:31&lt;00:34, 14.75it/s] Iteration: 47% 450/958 [00:31&lt;00:33, 14.98it/s] Iteration: 47% 452/958 [00:31&lt;00:33, 15.10it/s] Iteration: 47% 454/958 [00:31&lt;00:35, 14.14it/s] Iteration: 48% 456/958 [00:31&lt;00:37, 13.33it/s] Iteration: 48% 458/958 [00:31&lt;00:36, 13.87it/s] Iteration: 48% 460/958 [00:31&lt;00:35, 14.23it/s] Iteration: 48% 462/958 [00:32&lt;00:34, 14.46it/s] Iteration: 48% 464/958 [00:32&lt;00:33, 14.59it/s] Iteration: 49% 466/958 [00:32&lt;00:33, 14.61it/s] Iteration: 49% 468/958 [00:32&lt;00:33, 14.68it/s] Iteration: 49% 470/958 [00:32&lt;00:33, 14.67it/s] Iteration: 49% 472/958 [00:32&lt;00:32, 14.74it/s] Iteration: 49% 474/958 [00:32&lt;00:32, 14.76it/s] Iteration: 50% 476/958 [00:33&lt;00:32, 14.85it/s] Iteration: 50% 478/958 [00:33&lt;00:32, 14.93it/s] Iteration: 50% 480/958 [00:33&lt;00:31, 14.94it/s] Iteration: 50% 482/958 [00:33&lt;00:32, 14.63it/s] Iteration: 51% 484/958 [00:33&lt;00:32, 14.46it/s] Iteration: 51% 486/958 [00:33&lt;00:32, 14.57it/s] Iteration: 51% 488/958 [00:33&lt;00:31, 14.74it/s] Iteration: 51% 490/958 [00:33&lt;00:31, 14.72it/s] Iteration: 51% 492/958 [00:34&lt;00:33, 13.90it/s] Iteration: 52% 494/958 [00:34&lt;00:36, 12.88it/s] Iteration: 52% 496/958 [00:34&lt;00:37, 12.30it/s] Iteration: 52% 498/958 [00:34&lt;00:38, 12.05it/s] Iteration: 52% 500/958 [00:34&lt;00:35, 12.85it/s] Iteration: 52% 502/958 [00:34&lt;00:33, 13.53it/s] Iteration: 53% 504/958 [00:35&lt;00:32, 13.91it/s] Iteration: 53% 506/958 [00:35&lt;00:31, 14.18it/s] Iteration: 53% 508/958 [00:35&lt;00:31, 14.45it/s] Iteration: 53% 510/958 [00:35&lt;00:30, 14.52it/s] Iteration: 53% 512/958 [00:35&lt;00:30, 14.49it/s] Iteration: 54% 514/958 [00:35&lt;00:30, 14.42it/s] Iteration: 54% 516/958 [00:35&lt;00:30, 14.51it/s] Iteration: 54% 518/958 [00:36&lt;00:30, 14.39it/s] Iteration: 54% 520/958 [00:36&lt;00:30, 14.15it/s] Iteration: 54% 522/958 [00:36&lt;00:30, 14.33it/s] Iteration: 55% 524/958 [00:36&lt;00:30, 14.26it/s] Iteration: 55% 526/958 [00:36&lt;00:30, 14.00it/s] Iteration: 55% 528/958 [00:36&lt;00:30, 13.96it/s] Iteration: 55% 530/958 [00:36&lt;00:30, 13.95it/s] Iteration: 56% 532/958 [00:37&lt;00:31, 13.55it/s] Iteration: 56% 534/958 [00:37&lt;00:32, 12.89it/s] Iteration: 56% 536/958 [00:37&lt;00:34, 12.37it/s] Iteration: 56% 538/958 [00:37&lt;00:32, 12.93it/s] Iteration: 56% 540/958 [00:37&lt;00:31, 13.08it/s] Iteration: 57% 542/958 [00:37&lt;00:30, 13.58it/s] Iteration: 57% 544/958 [00:37&lt;00:29, 14.01it/s] Iteration: 57% 546/958 [00:38&lt;00:28, 14.35it/s] Iteration: 57% 548/958 [00:38&lt;00:28, 14.59it/s] Iteration: 57% 550/958 [00:38&lt;00:27, 14.88it/s] Iteration: 58% 552/958 [00:38&lt;00:26, 15.16it/s] Iteration: 58% 554/958 [00:38&lt;00:26, 15.30it/s] Iteration: 58% 556/958 [00:38&lt;00:27, 14.61it/s] Iteration: 58% 558/958 [00:38&lt;00:26, 14.83it/s] Iteration: 58% 560/958 [00:39&lt;00:26, 14.81it/s] Iteration: 59% 562/958 [00:39&lt;00:26, 14.79it/s] Iteration: 59% 564/958 [00:39&lt;00:26, 14.96it/s] Iteration: 59% 566/958 [00:39&lt;00:26, 14.94it/s] Iteration: 59% 568/958 [00:39&lt;00:27, 14.30it/s] Iteration: 59% 570/958 [00:39&lt;00:27, 13.97it/s] Iteration: 60% 572/958 [00:39&lt;00:26, 14.31it/s] Iteration: 60% 574/958 [00:39&lt;00:26, 14.49it/s] Iteration: 60% 576/958 [00:40&lt;00:25, 14.80it/s] Iteration: 60% 578/958 [00:40&lt;00:25, 14.80it/s] Iteration: 61% 580/958 [00:40&lt;00:25, 14.89it/s] Iteration: 61% 582/958 [00:40&lt;00:24, 15.16it/s]{&#39;loss&#39;: 0.1878497314453125, &#39;learning_rate&#39;: 6.506610995128741e-06, &#39;epoch&#39;: 2.6096033402922756, &#39;total_flos&#39;: 1913991805716480, &#39;step&#39;: 2500} Iteration: 61% 584/958 [00:40&lt;00:24, 15.26it/s] Iteration: 61% 586/958 [00:40&lt;00:25, 14.72it/s] Iteration: 61% 588/958 [00:40&lt;00:24, 14.92it/s] Iteration: 62% 590/958 [00:41&lt;00:24, 15.12it/s] Iteration: 62% 592/958 [00:41&lt;00:23, 15.28it/s] Iteration: 62% 594/958 [00:41&lt;00:24, 14.89it/s] Iteration: 62% 596/958 [00:41&lt;00:24, 14.97it/s] Iteration: 62% 598/958 [00:41&lt;00:23, 15.23it/s] Iteration: 63% 600/958 [00:41&lt;00:24, 14.72it/s] Iteration: 63% 602/958 [00:41&lt;00:24, 14.80it/s] Iteration: 63% 604/958 [00:41&lt;00:23, 15.08it/s] Iteration: 63% 606/958 [00:42&lt;00:23, 14.99it/s] Iteration: 63% 608/958 [00:42&lt;00:23, 14.93it/s] Iteration: 64% 610/958 [00:42&lt;00:23, 14.85it/s] Iteration: 64% 612/958 [00:42&lt;00:23, 14.84it/s] Iteration: 64% 614/958 [00:42&lt;00:22, 15.08it/s] Iteration: 64% 616/958 [00:42&lt;00:23, 14.75it/s] Iteration: 65% 618/958 [00:42&lt;00:23, 14.68it/s] Iteration: 65% 620/958 [00:43&lt;00:22, 14.74it/s] Iteration: 65% 622/958 [00:43&lt;00:22, 14.90it/s] Iteration: 65% 624/958 [00:43&lt;00:22, 14.53it/s] Iteration: 65% 626/958 [00:43&lt;00:22, 14.78it/s] Iteration: 66% 628/958 [00:43&lt;00:22, 14.95it/s] Iteration: 66% 630/958 [00:43&lt;00:22, 14.56it/s] Iteration: 66% 632/958 [00:43&lt;00:22, 14.41it/s] Iteration: 66% 634/958 [00:44&lt;00:22, 14.66it/s] Iteration: 66% 636/958 [00:44&lt;00:21, 14.75it/s] Iteration: 67% 638/958 [00:44&lt;00:21, 14.60it/s] Iteration: 67% 640/958 [00:44&lt;00:22, 14.45it/s] Iteration: 67% 642/958 [00:44&lt;00:21, 14.52it/s] Iteration: 67% 644/958 [00:44&lt;00:21, 14.66it/s] Iteration: 67% 646/958 [00:44&lt;00:21, 14.42it/s] Iteration: 68% 648/958 [00:44&lt;00:21, 14.47it/s] Iteration: 68% 650/958 [00:45&lt;00:20, 14.67it/s] Iteration: 68% 652/958 [00:45&lt;00:20, 14.82it/s] Iteration: 68% 654/958 [00:45&lt;00:20, 14.92it/s] Iteration: 68% 656/958 [00:45&lt;00:20, 14.87it/s] Iteration: 69% 658/958 [00:45&lt;00:20, 14.74it/s] Iteration: 69% 660/958 [00:45&lt;00:20, 14.47it/s] Iteration: 69% 662/958 [00:45&lt;00:20, 14.17it/s] Iteration: 69% 664/958 [00:46&lt;00:20, 14.43it/s] Iteration: 70% 666/958 [00:46&lt;00:19, 14.66it/s] Iteration: 70% 668/958 [00:46&lt;00:19, 14.68it/s] Iteration: 70% 670/958 [00:46&lt;00:19, 14.63it/s] Iteration: 70% 672/958 [00:46&lt;00:19, 14.82it/s] Iteration: 70% 674/958 [00:46&lt;00:19, 14.67it/s] Iteration: 71% 676/958 [00:46&lt;00:19, 14.24it/s] Iteration: 71% 678/958 [00:47&lt;00:19, 14.41it/s] Iteration: 71% 680/958 [00:47&lt;00:19, 14.51it/s] Iteration: 71% 682/958 [00:47&lt;00:18, 14.71it/s] Iteration: 71% 684/958 [00:47&lt;00:18, 14.79it/s] Iteration: 72% 686/958 [00:47&lt;00:18, 14.77it/s] Iteration: 72% 688/958 [00:47&lt;00:18, 14.87it/s] Iteration: 72% 690/958 [00:47&lt;00:18, 14.51it/s] Iteration: 72% 692/958 [00:47&lt;00:18, 14.62it/s] Iteration: 72% 694/958 [00:48&lt;00:17, 14.90it/s] Iteration: 73% 696/958 [00:48&lt;00:17, 14.97it/s] Iteration: 73% 698/958 [00:48&lt;00:17, 14.61it/s] Iteration: 73% 700/958 [00:48&lt;00:17, 14.83it/s] Iteration: 73% 702/958 [00:48&lt;00:17, 15.01it/s] Iteration: 73% 704/958 [00:48&lt;00:17, 14.92it/s] Iteration: 74% 706/958 [00:48&lt;00:17, 14.51it/s] Iteration: 74% 708/958 [00:49&lt;00:17, 14.62it/s] Iteration: 74% 710/958 [00:49&lt;00:16, 14.75it/s] Iteration: 74% 712/958 [00:49&lt;00:16, 15.00it/s] Iteration: 75% 714/958 [00:49&lt;00:16, 14.93it/s] Iteration: 75% 716/958 [00:49&lt;00:16, 14.33it/s] Iteration: 75% 718/958 [00:49&lt;00:18, 13.23it/s] Iteration: 75% 720/958 [00:49&lt;00:17, 13.28it/s] Iteration: 75% 722/958 [00:50&lt;00:17, 13.76it/s] Iteration: 76% 724/958 [00:50&lt;00:16, 14.20it/s] Iteration: 76% 726/958 [00:50&lt;00:16, 14.50it/s] Iteration: 76% 728/958 [00:50&lt;00:15, 14.50it/s] Iteration: 76% 730/958 [00:50&lt;00:15, 14.65it/s] Iteration: 76% 732/958 [00:50&lt;00:15, 14.78it/s] Iteration: 77% 734/958 [00:50&lt;00:15, 14.33it/s] Iteration: 77% 736/958 [00:51&lt;00:15, 14.44it/s] Iteration: 77% 738/958 [00:51&lt;00:15, 14.57it/s] Iteration: 77% 740/958 [00:51&lt;00:14, 14.57it/s] Iteration: 77% 742/958 [00:51&lt;00:14, 14.80it/s] Iteration: 78% 744/958 [00:51&lt;00:14, 14.93it/s] Iteration: 78% 746/958 [00:51&lt;00:14, 14.81it/s] Iteration: 78% 748/958 [00:51&lt;00:14, 14.80it/s] Iteration: 78% 750/958 [00:51&lt;00:14, 14.38it/s] Iteration: 78% 752/958 [00:52&lt;00:14, 14.58it/s] Iteration: 79% 754/958 [00:52&lt;00:14, 14.29it/s] Iteration: 79% 756/958 [00:52&lt;00:14, 14.38it/s] Iteration: 79% 758/958 [00:52&lt;00:13, 14.56it/s] Iteration: 79% 760/958 [00:52&lt;00:13, 14.82it/s] Iteration: 80% 762/958 [00:52&lt;00:13, 14.74it/s] Iteration: 80% 764/958 [00:52&lt;00:13, 14.29it/s] Iteration: 80% 766/958 [00:53&lt;00:13, 14.52it/s] Iteration: 80% 768/958 [00:53&lt;00:12, 14.70it/s] Iteration: 80% 770/958 [00:53&lt;00:12, 14.93it/s] Iteration: 81% 772/958 [00:53&lt;00:12, 15.09it/s] Iteration: 81% 774/958 [00:53&lt;00:12, 15.02it/s] Iteration: 81% 776/958 [00:53&lt;00:12, 15.02it/s] Iteration: 81% 778/958 [00:53&lt;00:11, 15.08it/s] Iteration: 81% 780/958 [00:54&lt;00:12, 14.31it/s] Iteration: 82% 782/958 [00:54&lt;00:12, 14.55it/s] Iteration: 82% 784/958 [00:54&lt;00:11, 14.91it/s] Iteration: 82% 786/958 [00:54&lt;00:11, 15.00it/s] Iteration: 82% 788/958 [00:54&lt;00:11, 15.03it/s] Iteration: 82% 790/958 [00:54&lt;00:11, 15.13it/s] Iteration: 83% 792/958 [00:54&lt;00:11, 15.05it/s] Iteration: 83% 794/958 [00:54&lt;00:11, 14.68it/s] Iteration: 83% 796/958 [00:55&lt;00:11, 14.64it/s] Iteration: 83% 798/958 [00:55&lt;00:10, 14.79it/s] Iteration: 84% 800/958 [00:55&lt;00:10, 14.80it/s] Iteration: 84% 802/958 [00:55&lt;00:10, 14.85it/s] Iteration: 84% 804/958 [00:55&lt;00:10, 14.90it/s] Iteration: 84% 806/958 [00:55&lt;00:10, 14.95it/s] Iteration: 84% 808/958 [00:55&lt;00:09, 15.08it/s] Iteration: 85% 810/958 [00:56&lt;00:10, 14.66it/s] Iteration: 85% 812/958 [00:56&lt;00:09, 14.84it/s] Iteration: 85% 814/958 [00:56&lt;00:09, 15.13it/s] Iteration: 85% 816/958 [00:56&lt;00:09, 15.15it/s] Iteration: 85% 818/958 [00:56&lt;00:09, 14.50it/s] Iteration: 86% 820/958 [00:56&lt;00:09, 14.65it/s] Iteration: 86% 822/958 [00:56&lt;00:09, 14.74it/s] Iteration: 86% 824/958 [00:56&lt;00:09, 14.21it/s] Iteration: 86% 826/958 [00:57&lt;00:10, 12.83it/s] Iteration: 86% 828/958 [00:57&lt;00:10, 12.19it/s] Iteration: 87% 830/958 [00:57&lt;00:10, 12.70it/s] Iteration: 87% 832/958 [00:57&lt;00:09, 13.29it/s] Iteration: 87% 834/958 [00:57&lt;00:09, 13.74it/s] Iteration: 87% 836/958 [00:57&lt;00:08, 14.19it/s] Iteration: 87% 838/958 [00:58&lt;00:08, 14.23it/s] Iteration: 88% 840/958 [00:58&lt;00:08, 14.26it/s] Iteration: 88% 842/958 [00:58&lt;00:07, 14.63it/s] Iteration: 88% 844/958 [00:58&lt;00:07, 14.74it/s] Iteration: 88% 846/958 [00:58&lt;00:07, 14.75it/s] Iteration: 89% 848/958 [00:58&lt;00:07, 14.98it/s] Iteration: 89% 850/958 [00:58&lt;00:07, 14.97it/s] Iteration: 89% 852/958 [00:58&lt;00:07, 14.75it/s] Iteration: 89% 854/958 [00:59&lt;00:07, 14.64it/s] Iteration: 89% 856/958 [00:59&lt;00:06, 14.85it/s] Iteration: 90% 858/958 [00:59&lt;00:06, 14.86it/s] Iteration: 90% 860/958 [00:59&lt;00:07, 13.82it/s] Iteration: 90% 862/958 [00:59&lt;00:06, 14.04it/s] Iteration: 90% 864/958 [00:59&lt;00:06, 13.98it/s] Iteration: 90% 866/958 [00:59&lt;00:06, 14.36it/s] Iteration: 91% 868/958 [01:00&lt;00:06, 14.41it/s] Iteration: 91% 870/958 [01:00&lt;00:06, 14.42it/s] Iteration: 91% 872/958 [01:00&lt;00:05, 14.57it/s] Iteration: 91% 874/958 [01:00&lt;00:06, 13.49it/s] Iteration: 91% 876/958 [01:00&lt;00:06, 13.59it/s] Iteration: 92% 878/958 [01:00&lt;00:05, 14.08it/s] Iteration: 92% 880/958 [01:00&lt;00:05, 14.26it/s] Iteration: 92% 882/958 [01:01&lt;00:05, 14.30it/s] Iteration: 92% 884/958 [01:01&lt;00:05, 14.26it/s] Iteration: 92% 886/958 [01:01&lt;00:05, 14.25it/s] Iteration: 93% 888/958 [01:01&lt;00:04, 14.47it/s] Iteration: 93% 890/958 [01:01&lt;00:04, 14.48it/s] Iteration: 93% 892/958 [01:01&lt;00:04, 14.44it/s] Iteration: 93% 894/958 [01:01&lt;00:04, 14.59it/s] Iteration: 94% 896/958 [01:02&lt;00:04, 14.40it/s] Iteration: 94% 898/958 [01:02&lt;00:04, 14.08it/s] Iteration: 94% 900/958 [01:02&lt;00:04, 14.22it/s] Iteration: 94% 902/958 [01:02&lt;00:03, 14.27it/s] Iteration: 94% 904/958 [01:02&lt;00:03, 13.97it/s] Iteration: 95% 906/958 [01:02&lt;00:03, 14.14it/s] Iteration: 95% 908/958 [01:02&lt;00:03, 14.21it/s] Iteration: 95% 910/958 [01:03&lt;00:03, 14.31it/s] Iteration: 95% 912/958 [01:03&lt;00:03, 14.27it/s] Iteration: 95% 914/958 [01:03&lt;00:03, 14.62it/s] Iteration: 96% 916/958 [01:03&lt;00:02, 14.66it/s] Iteration: 96% 918/958 [01:03&lt;00:02, 14.69it/s] Iteration: 96% 920/958 [01:03&lt;00:02, 14.95it/s] Iteration: 96% 922/958 [01:03&lt;00:02, 14.99it/s] Iteration: 96% 924/958 [01:04&lt;00:02, 15.08it/s] Iteration: 97% 926/958 [01:04&lt;00:02, 15.00it/s] Iteration: 97% 928/958 [01:04&lt;00:02, 14.74it/s] Iteration: 97% 930/958 [01:04&lt;00:01, 14.79it/s] Iteration: 97% 932/958 [01:04&lt;00:01, 15.02it/s] Iteration: 97% 934/958 [01:04&lt;00:01, 15.13it/s] Iteration: 98% 936/958 [01:04&lt;00:01, 14.95it/s] Iteration: 98% 938/958 [01:04&lt;00:01, 15.06it/s] Iteration: 98% 940/958 [01:05&lt;00:01, 15.00it/s] Iteration: 98% 942/958 [01:05&lt;00:01, 14.99it/s] Iteration: 99% 944/958 [01:05&lt;00:00, 14.58it/s] Iteration: 99% 946/958 [01:05&lt;00:00, 14.58it/s] Iteration: 99% 948/958 [01:05&lt;00:00, 14.40it/s] Iteration: 99% 950/958 [01:05&lt;00:00, 14.42it/s] Iteration: 99% 952/958 [01:05&lt;00:00, 14.29it/s] Iteration: 100% 954/958 [01:06&lt;00:00, 14.48it/s] Iteration: 100% 956/958 [01:06&lt;00:00, 14.41it/s] Iteration: 100% 958/958 [01:06&lt;00:00, 14.44it/s] Epoch: 100% 3/3 [03:26&lt;00:00, 68.87s/it] /usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1101: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead. warnings.warn(&#34;This method is deprecated, use `Trainer.is_world_process_zero()` instead.&#34;, FutureWarning) 09/19/2020 21:19:22 - INFO - filelock - Lock 140460075897632 acquired on /content/drive/My Drive/transformers-ner/mit-restaurant-data/cached_test_RobertaTokenizer_128.lock 09/19/2020 21:19:22 - INFO - utils_ner - Creating features from dataset file at /content/drive/My Drive/transformers-ner/mit-restaurant-data/ 09/19/2020 21:19:22 - INFO - utils_ner - Writing example 0 of 1521 09/19/2020 21:19:22 - INFO - utils_ner - *** Example *** 09/19/2020 21:19:22 - INFO - utils_ner - guid: test-1 09/19/2020 21:19:22 - INFO - utils_ner - tokens: &lt;s&gt; a four star rest aur ant with a bar &lt;/s&gt; 09/19/2020 21:19:22 - INFO - utils_ner - input_ids: 0 102 10231 3641 7110 8616 927 5632 102 4901 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:19:22 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - label_ids: -100 2 0 1 2 -100 -100 5 6 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:19:22 - INFO - utils_ner - *** Example *** 09/19/2020 21:19:22 - INFO - utils_ner - guid: test-2 09/19/2020 21:19:22 - INFO - utils_ner - tokens: &lt;s&gt; any as ian cu isine around &lt;/s&gt; 09/19/2020 21:19:22 - INFO - utils_ner - input_ids: 0 3785 281 811 16312 40116 13837 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:19:22 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - label_ids: -100 2 14 -100 2 -100 5 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:19:22 - INFO - utils_ner - *** Example *** 09/19/2020 21:19:22 - INFO - utils_ner - guid: test-3 09/19/2020 21:19:22 - INFO - utils_ner - tokens: &lt;s&gt; any bb q places open before 5 near by &lt;/s&gt; 09/19/2020 21:19:22 - INFO - utils_ner - input_ids: 0 3785 14141 1343 23454 12592 23033 245 34664 1409 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:19:22 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - label_ids: -100 2 14 -100 2 10 11 11 5 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:19:22 - INFO - utils_ner - *** Example *** 09/19/2020 21:19:22 - INFO - utils_ner - guid: test-4 09/19/2020 21:19:22 - INFO - utils_ner - tokens: &lt;s&gt; any d ancing establish ments with reasonable pr icing &lt;/s&gt; 09/19/2020 21:19:22 - INFO - utils_ner - input_ids: 0 3785 417 7710 30248 2963 5632 33739 4862 16483 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:19:22 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - label_ids: -100 2 5 -100 6 -100 2 9 2 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:19:22 - INFO - utils_ner - *** Example *** 09/19/2020 21:19:22 - INFO - utils_ner - guid: test-5 09/19/2020 21:19:22 - INFO - utils_ner - tokens: &lt;s&gt; any good che ap ger man rest aur ants near by &lt;/s&gt; 09/19/2020 21:19:22 - INFO - utils_ner - input_ids: 0 3785 8396 2871 1115 2403 397 7110 8616 3277 34664 1409 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 09/19/2020 21:19:22 - INFO - utils_ner - input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 09/19/2020 21:19:22 - INFO - utils_ner - label_ids: -100 2 2 9 -100 14 -100 2 -100 -100 5 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 09/19/2020 21:19:23 - INFO - utils_ner - Saving features into cached file /content/drive/My Drive/transformers-ner/mit-restaurant-data/cached_test_RobertaTokenizer_128 09/19/2020 21:19:23 - INFO - filelock - Lock 140460075897632 released on /content/drive/My Drive/transformers-ner/mit-restaurant-data/cached_test_RobertaTokenizer_128.lock Prediction: 100% 191/191 [00:03&lt;00:00, 51.02it/s] 09/19/2020 21:19:34 - INFO - __main__ - eval_loss = 0.29688322173849724 09/19/2020 21:19:34 - INFO - __main__ - eval_accuracy_score = 0.9196127946127947 09/19/2020 21:19:34 - INFO - __main__ - eval_precision = 0.7854288349216109 09/19/2020 21:19:34 - INFO - __main__ - eval_recall = 0.8108536972389717 09/19/2020 21:19:34 - INFO - __main__ - eval_f1 = 0.7979387882573391 Traceback (most recent call last): File &#34;run_ner.py&#34;, line 308, in &lt;module&gt; main() File &#34;run_ner.py&#34;, line 297, in main token_classification_task.write_predictions_to_file(writer, f, preds_list) File &#34;/content/tasks.py&#34;, line 51, in write_predictions_to_file if not preds_list[example_id]: IndexError: list index out of range . from transformers import pipeline model_name = &#39;/content/restaurant-model&#39; nlp = pipeline(task=&quot;ner&quot;, model=model_name, tokenizer=model_name, framework=&quot;pt&quot;,grouped_entities=False) . sequence =&quot;&quot;&quot;how many 5 star restaurants are near me.&quot;&quot;&quot; nlp(sequence) . [{&#39;entity&#39;: &#39;B-Rating&#39;, &#39;index&#39;: 3, &#39;score&#39;: 0.9885362982749939, &#39;word&#39;: &#39;Ġ5&#39;}, {&#39;entity&#39;: &#39;I-Rating&#39;, &#39;index&#39;: 4, &#39;score&#39;: 0.9926660060882568, &#39;word&#39;: &#39;Ġstar&#39;}, {&#39;entity&#39;: &#39;B-Location&#39;, &#39;index&#39;: 7, &#39;score&#39;: 0.956812858581543, &#39;word&#39;: &#39;Ġnear&#39;}, {&#39;entity&#39;: &#39;I-Location&#39;, &#39;index&#39;: 8, &#39;score&#39;: 0.8987305760383606, &#39;word&#39;: &#39;Ġme&#39;}] .",
            "url": "http://forgetfulcrow.com/huggingface/transformers/ner/token%20classification/sequence%20labelling/2020/09/19/_09_20_Entity_Extraction_Transformers_Part_1.html",
            "relUrl": "/huggingface/transformers/ner/token%20classification/sequence%20labelling/2020/09/19/_09_20_Entity_Extraction_Transformers_Part_1.html",
            "date": " • Sep 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "A Neural Probabilistic Language Model",
            "content": "We will go through the paper by Yoshua Bengio - A Neural Probabilistic Language Model . Language Modeling . In language modeling task we try to learn the joint probability function of word sequences p(w1 w2 .. wm). . p(w1 w2 .. wm) = count(w1 w2 .. wm) / count(all m-gram word seq) . Unseen Word sequences . Calculating joint probability is difficult becuase it involves counting all the m-word sequences and also, it doesn&#39;t generalize well on unseen word sequences. . What will happen to the word sequences we have not seen during the modeling time? We will assign a zero probability to all such sequences. . We use probability chain formula to frame the joint probability as a product of conditional probabilities. We define joint probability as product of conditional probabilities as follows . p(w1 w2 ..wm) = p(w1) p(w2|w1) p(w3|w2w1) .. p(wm|w1..wm-1) . Calculating conditional probability is comparatively easy. p(w3|w2 w1) = count(w1 w2 w2)/count(w1 w2) . But still we have the same problem when calculating conditional probability of long word sequences - it is highly likely that we won&#39;t have seen such long sequence of exact words during the modeling time. How do we handle this? . N-gram assumption . Linguists have observed that a word depends only n previous words and not all the words before it. This is the n-gram or Markov assumption. This simplifies the function p(wm|w1 w2..wm-1) into p(wm| wm-n ..wm-1). If we make a trigram(n=3) assumption, which is popular in statistical language modeling, it becomes p(wm| wm-1 wm-2), i.e. each word is assumed to be dependent on only last two words in the sequence. . Back-off and Smoothed Models . To further improve the generalization on unseen word sequences, there a few more bag of tricks like back-off trigram model where if p(w4|w3 w2) is not known to us, we consider futher smaller sequences until we find the required probability. We look at p(w4|w3) if it is available, else we look at p(w4). Also, we use smoothed trigram model to distribute the probability mass. . Bengio&#39;s NNLM paper . It addresses two problems with the traditional statistical language modelling . Curse of Dimensionality To model a joint distribution of 10-gram word sequence of 10,000 words vocabulary, there are 10,000^10 -1 free parameters required. When modeling continous variables, we obtain generalization more easily because function to be learned is expected to have some local smoothness, i.e. if we have smoothness property for language modeling task, we should be able to use some local smoothness to generalize the model to unseen word sequences. If we have seen a word sequence &quot;The cat is walking in the bedroom&quot;, it should be able to generalize to simliar word sequence like &quot;A dog was running in the room&quot; as &quot;dog&quot; and &quot;cat&quot; have similar semantic and grammatic roles. The generalization in the statistical language modeling is obtained by gluing together(product of conditional probabilities) the short subsequneces. Typically, trigrams were considered becuase of the curse of dimensionality. It doesn&#39;t take in to account more than 1-2 previous words. | Not taking into account similarity between words The big problem in language modeling is generalization. If the model understands the semantic simliary between words, it will generalize better. | Previous Work . Neural Networks for Modeling Joint Probability Distribution- Neural networks have been used to model the joint probability distribution of random variables and also for lanugage modeling before this paper. Each output node spits the conditional probability of a word give the sequence of words as input. . | Word Similarities Discovering the word simliarties to obtain generalization has been tried as well, by clustering similar words together but the model proposed by Bengio learns the distributed feature vector to represent word similarty. . | Vector space representation of words -The previous works in information retrieval has explored the vector-space representation of words(LSI) but this paper explores the reprsentation of words which helps in representing the probability distribution of the word sequences. The paper mentions that learning the word representation and probability distribution jointly is very useful. . | . . Implementation . import torch import torch.nn as nn import torch.optim as optim . corpus = [&#39;i like dog&#39;, &#39;i love coffee&#39;, &#39;i hate milk&#39;] # list of all the sentences vocabulary = list(set(&#39; &#39;.join(corpus).split())) # |v| list of all the uwords word2int = {w:i for i, w in enumerate(vocabulary)} corpus = [&#39;i like dog&#39;, &#39;i love coffee&#39;, &#39;i hate milk&#39;] # list of all the sentences vocabulary = list(set(&#39; &#39;.join(corpus).split())) # |v| list of all the uwords n_vocab = len(vocabulary) class NNLM(nn.Module): def __init__(self): super(NNLM, self).__init__() # projection layer self.C = nn.Embedding(n_vocab, m) # hidden layer # tanh(XH + d) self.H = nn.Parameter(torch.randn(n_step * m, n_hidden).type(torch.FloatTensor)) self.d = nn.Parameter(torch.randn(n_hidden).type(torch.FloatTensor)) # hidden layer # WX + b self.W = nn.Parameter(torch.randn(n_step * m, n_vocab).type(torch.FloatTensor)) self.b = nn.Parameter(torch.randn(n_vocab).type(torch.FloatTensor)) # tanh U self.U = nn.Parameter(torch.randn(n_hidden, n_vocab).type(torch.FloatTensor)) def forward(self, X): X = self.C(X) # (batch_size, n_step, m) X = X.view(-1, n_step * m) # (batch_size, n_step * m) tanh = torch.tanh(self.d + torch.mm(X, self.H)) output = torch.mm(tanh, self.U) + torch.mm(X, self.W) + self.b return output def prepare_input(sentences): X = [] Y = [] for sent in sentences: sent = sent.split() X.append([word2int[word] for word in sent[:-1]]) Y.append(word2int[sent[-1]]) return (X, Y) . X, Y = prepare_input(corpus) m = 2 n_step = 2 n_hidden =2 model = NNLM() criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=0.001) X = torch.LongTensor(X) Y = torch.LongTensor(Y) for epoch in range(5000): optimizer.zero_grad() output = model(X) # output : [batch_size, n_class], target_batch : [batch_size] (LongTensor, not one-hot) loss = criterion(output, Y) if (epoch + 1)%1000 == 0: print(&#39;Epoch:&#39;, &#39;%04d&#39; % (epoch + 1), &#39;cost =&#39;, &#39;{:.6f}&#39;.format(loss)) loss.backward() optimizer.step() . Epoch: 1000 cost = 0.207585 Epoch: 2000 cost = 0.035829 Epoch: 3000 cost = 0.012441 Epoch: 4000 cost = 0.005522 Epoch: 5000 cost = 0.002739 . input = [] for w in X: input.append([vocabulary[i] for i in w]) predictions = model(X).data.max(1, keepdim=True)[1].squeeze() for i,o in zip(input, [vocabulary[pred] for pred in predictions]): print(&#39;Input: &#39;+&#39; &#39;.join(i)) print(&#39;Output:&#39;,o) print(&#39; n&#39;) . Input: i like Output: dog Input: i love Output: coffee Input: i hate Output: milk . How word2vec improves on the NNLM model? . The embedding to hidden and hidden to softmax layers are expensive in NNLM network. The softmax layers has 300*|V| weights. For a vocabulary of 10,000 words, the number of weights will be approx 3 million. Word2vec simplifies the NNLM model in few ways. . use negative sampling to update only a fraction of 3M weights in output layer. | modeling a binary classification instead of next word prediction model | .",
            "url": "http://forgetfulcrow.com/nnlm/torch/language%20model/nlp/2020/05/03/A-Neural-Probabilistic-Language-Model-(1).html",
            "relUrl": "/nnlm/torch/language%20model/nlp/2020/05/03/A-Neural-Probabilistic-Language-Model-(1).html",
            "date": " • May 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Detecting Handwritten text in Documents",
            "content": "Problem Statement . We wish to detect the handwritten text in the scanned/pdf document. It could be for number of reasons like . to identify if the document has been signed | to process handwritten text in the document in a different way | to mask the handwritten text | . Take following document image for an example. We wish to detect the text highlighted in the red bounding boxes. . Training Dataset . We will use the annotated dataset available in the following github repo https://github.com/CatalystCode/Handwriting/tree/master/Data/labelledcontracttrainingdata/trainingjpg_output_99/ . The dataset is part of the Microsoft blog available here https://devblogs.microsoft.com/cse/2018/05/07/handwriting-detection-and-recognition-in-scanned-documents-using-azure-ml-package-computer-vision-azure-cognitive-services-ocr/ . Each image is annotated in Pascal VOC Annotation format using Microsoft Vott Annotation tool. The directory structure of the annotated dataset looks like this . data/Annotations_99 data/JPEGImages_99 . There are 99 annotated images in the dataset. The images are present in JPEGImages_99 folder and corresponding xml annotations are available under Annotations_99. . An XML annotation file looks like . &lt;annotation verified=&quot;yes&quot;&gt; &lt;folder&gt;Annotation&lt;/folder&gt; &lt;filename&gt;07653e58-24d1-4b3f-9b4a-76057efe5c09-1&lt;/filename&gt; &lt;path&gt;C: data JPEGImages 07653e58-24d1-4b3f-9b4a-76057efe5c09-1.jpg&lt;/path&gt; &lt;source&gt; &lt;database&gt;Unknown&lt;/database&gt; &lt;/source&gt; &lt;size&gt; &lt;width&gt;1700&lt;/width&gt; &lt;height&gt;2200&lt;/height&gt; &lt;depth&gt;3&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; &lt;object&gt; &lt;name&gt;signature&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;bndbox&gt; &lt;xmin&gt;192&lt;/xmin&gt; &lt;ymin&gt;1188&lt;/ymin&gt; &lt;xmax&gt;738&lt;/xmax&gt; &lt;ymax&gt;1320&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; ... . In Pascal VOC annotation, there is a seperate annotation file for each image. The data we are interested in the xml file is . image filename - 07653e58-24d1-4b3f-9b4a-76057efe5c09-1 | object attribute for each annotation in the image category/class of the marked annotation | bounding box coordinates of top left and right bottom position | . | . About Detectron2 Framework . We will use pytorch detectron2 framework because it is simple and easy to extend. There are simple Training, Visualization, and Prediction modules available in the detectron2 which handles most of the stuff and we can use it as is, or if required, we can extend the functionality. . Simple steps to train a vision model in Detectron2 . Convert dataset in the detectron2 format | Register the dataset and metadata information like class labels | Update the config with registered dataset (DATASETS.{TRAIN,TEST}), model weight (MODEL.WEIGHT), learning rate, Number of output classes (MODEL.ROI_HEADS.NUM_CLASSES), and other training and test parameters | Train the model using DefaultTrainer class | Dataset Preparation(step 1 &amp; 2) . Detectron2 expects the dataset as list[dict] in the following format. So for training with detectron2 we will have to convert our dataset in the following format. . [{&#39;file_name&#39;: &#39;datasets/JPEGImages/1.jpg&#39;, &#39;image_id&#39;: &#39;1&#39;, &#39;height&#39;: 3300, &#39;width&#39;: 2550, &#39;annotations&#39;: [{&#39;category_id&#39;: 1, &#39;bbox&#39;: [1050.1000264270613, 457.33333333333337, 1406.9139799154334, 587.7450980392157], &#39;bbox_mode&#39;: &lt;BoxMode.XYXY_ABS: 0&gt;}, {&#39;category_id&#39;: 1, &#39;bbox&#39;: [1529.9097515856238, 473.5098039215687, 1617.167679704017, 555.3921568627452], &#39;bbox_mode&#39;: &lt;BoxMode.XYXY_ABS: 0&gt;}]}] . Detectron registers this list of dict as torch dataset and uses the default dataloader and datasampler for training. We can register the list[dict] with detectron2 using following code . def get_dicts(): ... return list[dict] in the above format from detectron2.data import DatasetCatalog DatasetCatalog.register(&quot;my_dataset&quot;, get_dicts) . And to register the metadata information related to dataset like category mapping to id&#39;s, the type of dataset, we have to set the keyvalue pair using . MetadataCatalog.get(&quot;my_dataset&quot;).thing_classes = [&quot;person&quot;, &quot;dog&quot;] . Choosing a Model and Initializing Configuration (step 3) . Detectron2 has lot of pretrained model available in the model zoo. For handwritten text detection, we will choose Faster RCNN with FPN backbone. . We have to initialize the parameters and weights for model we want to train. . cfg = get_cfg() cfg.merge_from_file(&#39;&lt;pretrained model config&#39;&gt;) cfg.MODEL.WEIGHTS = &#39;&lt;path to pretrained model weight&gt; #custom config for training cfg.DATASETS.TRAIN = (&quot;&lt;registered training dataset name&gt;&quot;,) cfg.SOLVER.MAX_ITER = &#39;&lt;number of training iterations&gt;&#39; cfg.MODEL.ROI_HEADS.NUM_CLASSES = &#39;&lt;number of classes&gt;&#39; . All the model configs are available in cfg object. If we want to replicate the training later, we can save the cfg object and load it back to resume training. . Model Training (step 4) . We will use the DefaultTrainer for now. There are simple modules available which only accept the minimal parameters and make assumptions about lot of things. . The DefaultTrainer Module . builds the model | builds the optimizer | builds the dataloader | loads the model weights, and | register common hooks | . trainer = DefaultTrainer(cfg) trainer.resume_or_load(resume=False) trainer.train() . Now, we can train our Instance Detection model using Detectron2. We will try FasterRCNN-FPN-50 Model and see how it performs . 1. Install Detectron2 . !pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html !pip install cython pyyaml==5.1 !pip install -U &#39;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#39; import torch, torchvision torch.__version__ !gcc --version # opencv is pre-installed on colab # install detectron2: !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html . 2. Download the Dataset . We will create following directory structure . datasets/ Annotations/ JPEGImages/ train.txt . train.txt and test.txt have a filename(without extension) per row . !git clone https://github.com/CatalystCode/Handwriting !mv Handwriting/Data/labelledcontracttrainingdata/trainingjpg_output_99/ datasets !mv datasets/Annotations_99 datasets/Annotations !mv datasets/JPEGImages_99 datasets/JPEGImages !ls -l datasets/Annotations | awk &#39;{print $9}&#39; | sed &#39;s/ .[^.]*$//&#39; &gt; datasets/train.txt . 1. Prepare &amp; Visualize the Dataset . To visualize the labeled dataset in detectron2, we need to convert the xml annotations in the detectron2 dataset format as explained above. . We will use the custom function register_pascal_voc() which will convert the dataset into detectron2 format and register it with DatasetCatalog. . Visualizer Class . To draw the annotations on the images, we will use the Detectron2 Visualizer class which takes the image in rgb format, the metadata which has ordered label names and the scale parameter. . Visualizer.draw_instance_predictions() function to visualize prediction results Visualizer.draw_dataset_dict() function to draw the annotated dataset . Download the dataset . %matplotlib inline . import numpy as np import os import xml.etree.ElementTree as ET from detectron2.data import DatasetCatalog, MetadataCatalog from detectron2.structures import BoxMode from fvcore.common.file_io import PathManager import random import cv2 from detectron2.utils.visualizer import Visualizer from matplotlib.pyplot import figure from matplotlib import pyplot as plt def load_voc_instances(dirname, split, CLASS_NAMES): &quot;&quot;&quot; Load Pascal VOC detection annotations to Detectron2 format. Args: dirname: Contain &quot;Annotations&quot;, &quot;JPEGImages&quot; split (str): one of &quot;train&quot;, &quot;test&quot;, &quot;val&quot;, &quot;trainval&quot; &quot;&quot;&quot; with PathManager.open(os.path.join(dirname, split+&quot;.txt&quot;)) as f: fileids = np.loadtxt(f, dtype=np.str) dicts = [] for fileid in fileids: anno_file = os.path.join(dirname, &quot;Annotations&quot;, fileid + &quot;.xml&quot;) jpeg_file = os.path.join(dirname, &quot;JPEGImages&quot;, fileid + &quot;.jpg&quot;) tree = ET.parse(anno_file) r = { &quot;file_name&quot;: jpeg_file, &quot;image_id&quot;: fileid, &quot;height&quot;: int(tree.findall(&quot;./size/height&quot;)[0].text), &quot;width&quot;: int(tree.findall(&quot;./size/width&quot;)[0].text), } instances = [] for obj in tree.findall(&quot;object&quot;): cls = obj.find(&quot;name&quot;).text # We include &quot;difficult&quot; samples in training. # Based on limited experiments, they don&#39;t hurt accuracy. # difficult = int(obj.find(&quot;difficult&quot;).text) # if difficult == 1: # continue bbox = obj.find(&quot;bndbox&quot;) bbox = [float(bbox.find(x).text) for x in [&quot;xmin&quot;, &quot;ymin&quot;, &quot;xmax&quot;, &quot;ymax&quot;]] # Original annotations are integers in the range [1, W or H] # Assuming they mean 1-based pixel indices (inclusive), # a box with annotation (xmin=1, xmax=W) covers the whole image. # In coordinate space this is represented by (xmin=0, xmax=W) bbox[0] -= 1.0 bbox[1] -= 1.0 instances.append( {&quot;category_id&quot;: CLASS_NAMES.index(cls), &quot;bbox&quot;: bbox, &quot;bbox_mode&quot;: BoxMode.XYXY_ABS} ) r[&quot;annotations&quot;] = instances dicts.append(r) return dicts def visualize_dataset(datasetname, n_samples=10): dataset_dicts = DatasetCatalog.get(datasetname) metadata = MetadataCatalog.get(datasetname) for d in random.sample(dataset_dicts,n_samples): print(d[&#39;file_name&#39;]) img = cv2.imread(d[&quot;file_name&quot;]) visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5) vis = visualizer.draw_dataset_dict(d) figure(num=None, figsize=(15, 15), dpi=100, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.axis(&quot;off&quot;) plt.imshow(vis.get_image()[:, :, ::-1]) plt.show() def register_pascal_voc(name, dirname, split, CLASS_NAMES): if name not in DatasetCatalog.list(): DatasetCatalog.register(name, lambda: load_voc_instances(dirname, split, CLASS_NAMES)) MetadataCatalog.get(name).set( thing_classes=CLASS_NAMES, split=split, dirname= dirname, year=2012 ) . #register pascal voc dataset in detectron2 register_pascal_voc(&#39;signature_dataset_train&#39;, dirname=&#39;datasets&#39;, split=&#39;train&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) . visualize_dataset(&#39;signature_dataset_train&#39;,n_samples = 4) . datasets/JPEGImages/07653e58-24d1-4b3f-9b4a-76057efe5c09-3.jpg . datasets/JPEGImages/7674b81e-aa42-4891-856d-8938620d6fa0-1.jpg . datasets/JPEGImages/84cce561-1ee5-4201-9dfe-13da1711ca75-2.jpg . datasets/JPEGImages/9854ded9-3bd7-437c-83c4-b05d409c5872-2.jpg . 2. Model Training . from detectron2.engine import default_argument_parser from detectron2.engine import DefaultTrainer from detectron2.engine import default_setup from detectron2.config import get_cfg def setup_cfg(args): &quot;&quot;&quot; Create configs and perform basic setups. &quot;&quot;&quot; cfg = get_cfg() cfg.merge_from_file(args.config_file) cfg.merge_from_list(args.opts) cfg.freeze() default_setup(cfg, args) return cfg parser = default_argument_parser() args = parser.parse_args(&quot;--config-file sign_config/sign_faster_rcnn_R_50_FPN_3x.yaml OUTPUT_DIR sign_model &quot;.split()) . We have copied the config file for Faster RCNN R50 FPN from the model zoo as sign_faster_rcnn_R_50_FPN_3x.yaml and updated the configuration parameters. We have set the MODEL.ROI_HEADS(classes) to 2, Max Number of iterations to 4000, and training dataset name to the one we registered earlier. . config.setup_cfg function will load the configuration from the --config-file path, and will update the configration with other parameters passed as arguments . Here, we have passed the OUTPUT_DIR parameter to update the cfg.OUTPUT_DIR parameter value . cfg = setup_cfg(args) . Now that we have all the configurations, we can start training the model. . As I explained earlier, DefaultTrainer will build the model(without weights), optimizer, learning rate scheduler and then load weights from the checkpoint file specified in the cfg.MODEL.WEIGHTS parameter. . trainer = DefaultTrainer(cfg) trainer.resume_or_load(resume=False) trainer.train() . 3. Model Prediction . Now that the model has been trained and saved in the output directory. The config saved during the model training has all the parameters except model weight. We pass the model weight path as paramter to load the trained model weight. . The DefaultPredictor does image translation and takes only single image for prediction. But we can easily modify the DefaultPredictor class to accept batch of input images for prediction . from detectron2.engine import default_argument_parser from detectron2.engine import DefaultPredictor import config parser = default_argument_parser() args = parser.parse_args(&quot;--config-file sign_model/config.yaml MODEL.WEIGHTS sign_model/model_final.pth&quot;.split()) cfg = config.setup_cfg(args) predictor = DefaultPredictor(cfg) . import glob import time import os from matplotlib.pyplot import figure from matplotlib import pyplot as plt import cv2 from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog files = glob.glob(&quot;test_images/*.jpg&quot;) sample_size = 5 for file,_ in zip(files,range(sample_size)): im = cv2.imread(file) MetadataCatalog.get(&quot;signature_dataset_train&quot;).thing_classes = [&quot;signature&quot;,&quot;others&quot;] start_time = time.time() outputs = predictor(im) print(time.time()- start_time) v = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(&quot;signature_dataset_train&quot;), scale=0.5) v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) print(file) figure(num=None, figsize=(15, 15), dpi=100, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.axis(&quot;off&quot;) plt.imshow(v.get_image()[:, :, ::-1]) plt.show() . 0.4223031997680664 test_images/image_11.jpg . 0.18352723121643066 test_images/0d0eddfc-731b-44de-b84d-d265afc7d996-1.jpg . 0.17967772483825684 test_images/07653e58-24d1-4b3f-9b4a-76057efe5c09-6.jpg . 0.14139199256896973 test_images/0d0eddfc-731b-44de-b84d-d265afc7d996-2.jpg . 0.15801262855529785 test_images/image_10.jpg . 4. Evaluate . The DefaultTrainer class doesn&#39;t have a evaluator method implemented. I have created a new Trainer class and added the build_evaluator method. We could have used this new Trainer class in the first step instead of DefaultTrainer but I wanted to show how easy it is to train the model without writing more code. . from detectron2.engine import default_argument_parser import config import trainer import dataset_utils dataset_utils.register_pascal_voc(&#39;signature_dataset_train&#39;, dirname=&#39;datasets&#39;, split=&#39;train&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) #dataset_utils.register_pascal_voc(&#39;signature_dataset_test&#39;, dirname=&#39;datasets&#39;, split=&#39;test&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) parser = default_argument_parser() args = parser.parse_args(&quot;--config-file sign_model/config.yaml MODEL.WEIGHTS sign_model/model_final.pth&quot;.split()) trainer.eval(args) . OrderedDict([(&#39;bbox&#39;, {&#39;AP&#39;: 69.67298193268152, &#39;AP50&#39;: 98.10585383880746, &#39;AP75&#39;: 88.13255308840152})]) . . Training on a Custom Dataset . Let us say we have got some more annotated dataset which is not in PASCAL VOC xml format. To train the above model, we have write a custom function get_dicts() which returns data in detectron2 format. . To improve the accuracy of handwriting detection, I found one more dataset which is of annotated french documents. The annotations are in json format for each image. The dataset is available in the following github repo https://github.com/hyperlex/Signature-detection-Practical-guide/tree/master/data/dataset. Download and save it in french_dataset directory . I have added all the functions in the library files dataset_utils.py and trainer.py. We will use these abstractions to quickly train and evaluate new models . import json import glob import os import cv2 from detectron2.structures import BoxMode def get_french_dicts(annot_dir): json_files = glob.glob(os.path.join(annot_dir,&#39;*.json&#39;)) dataset_dicts = [] for f in json_files: record={} img_ann = json.load(open(f)) filename = img_ann[&#39;asset&#39;][&#39;name&#39;] height, width = cv2.imread(os.path.join(annot_dir,&#39;..&#39;,filename)).shape[:2] record[&quot;file_name&quot;] = os.path.join(annot_dir,&#39;..&#39;,filename) record[&quot;image_id&quot;] = img_ann[&#39;asset&#39;][&#39;id&#39;] record[&quot;height&quot;] = height record[&quot;width&quot;] = width annos = img_ann[&quot;regions&quot;] objs =[] for ann in annos: px = ann[&#39;boundingBox&#39;][&#39;left&#39;] py = ann[&#39;boundingBox&#39;][&#39;top&#39;] px1 = ann[&#39;boundingBox&#39;][&#39;left&#39;] + ann[&#39;boundingBox&#39;][&#39;width&#39;] py1 = ann[&#39;boundingBox&#39;][&#39;top&#39;] + ann[&#39;boundingBox&#39;][&#39;height&#39;] obj = { &quot;bbox&quot;: [px, py, px1, py1], &quot;bbox_mode&quot;: BoxMode.XYXY_ABS, &quot;category_id&quot;: {&#39;signature&#39;:0,&#39;paraphe&#39;:1,&#39;date&#39;:1}[ann[&#39;tags&#39;][0]], &quot;iscrowd&quot;: 0 } objs.append(obj) record[&quot;annotations&quot;] = objs dataset_dicts.append(record) return dataset_dicts . from detectron2.utils.visualizer import Visualizer from detectron2.data import DatasetCatalog, MetadataCatalog import dataset_utils def get_img_dicts(): ann1 = dataset_utils.load_voc_instances(dirname = &#39;datasets&#39;, split = &#39;train&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) ann2 = get_french_dicts(&#39;french_dataset/per_img_labels&#39;) return ann1 + ann2 dataset_name = &#39;signature_dataset_train&#39; DatasetCatalog.register(dataset_name, lambda: get_img_dicts()) MetadataCatalog.get(dataset_name).set(thing_classes=[&quot;signature&quot;,&quot;others&quot;], split=&#39;train&#39;, dirname= dirname, year=2012) . Metadata(name=&#39;signature_dataset_train&#39;, thing_classes=[&#39;signature&#39;, &#39;others&#39;]) . len(DatasetCatalog.get(dataset_name)) . 139 . dataset_utils.visualize_dataset(&#39;signature_dataset_train&#39;, n_samples=2) . datasets/JPEGImages/63348ad3-b0cc-45d0-bc85-bf2c865744ec-2.jpg . datasets/JPEGImages/4c85bb9b-1d8d-45b0-8f4a-664d77ee4b83-4.jpg . We can run the remaining steps as we did before for training the model and prediction . from detectron2.engine import default_argument_parser from detectron2.engine import DefaultTrainer import trainer parser = default_argument_parser() args = parser.parse_args(&quot;--config-file sign_config/chk_faster_rcnn_R_50_FPN_3x.yaml --num-gpus 3 OUTPUT_DIR french_sign_model SOLVER.MAX_ITER 4000&quot;.split()) trainer.train(args) . from detectron2.engine import default_argument_parser import config import trainer import dataset_utils import dataset_utils dataset_utils.register_pascal_voc(&#39;signature_dataset_test&#39;, dirname=&#39;datasets&#39;, split=&#39;train&#39;, CLASS_NAMES=[&quot;signature&quot;,&quot;others&quot;]) parser = default_argument_parser() args = parser.parse_args(&#39;--config-file french_sign_model/config.yaml MODEL.WEIGHTS french_sign_model/model_final.pth DATASETS.TEST (&quot;signature_dataset_test&quot;,)&#39;.split()) trainer.eval(args) . OrderedDict([(&#39;bbox&#39;, {&#39;AP&#39;: 67.82590895115814, &#39;AP50&#39;: 96.77629667360196, &#39;AP75&#39;: 84.18387912626437})]) . The Average Precision has reduced compared to the previous model. Let us check the prediction results. . from detectron2.engine import default_argument_parser from detectron2.engine import DefaultPredictor import config parser = default_argument_parser() args = parser.parse_args(&quot;--config-file french_sign_model/config.yaml MODEL.WEIGHTS french_sign_model/model_final.pth&quot;.split()) cfg = config.setup_cfg(args) predictor = DefaultPredictor(cfg) . import glob import time import os from matplotlib.pyplot import figure from matplotlib import pyplot as plt import cv2 from detectron2.utils.visualizer import Visualizer from detectron2.data import MetadataCatalog files = glob.glob(&quot;test_images/*.jpg&quot;) sample_size = 5 for file,_ in zip(files,range(sample_size)): im = cv2.imread(file) MetadataCatalog.get(&quot;signature_dataset_train&quot;).thing_classes = [&quot;signature&quot;,&quot;others&quot;] start_time = time.time() outputs = predictor(im) print(time.time()- start_time) v = Visualizer(im[:, :, ::-1], metadata=MetadataCatalog.get(&quot;signature_dataset_train&quot;), scale=0.5) v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;)) print(file) figure(num=None, figsize=(15, 15), dpi=100, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.imshow(v.get_image()[:, :, ::-1]) plt.show() . 0.4847722053527832 test_images/image_11.jpg . 0.18241333961486816 test_images/0d0eddfc-731b-44de-b84d-d265afc7d996-1.jpg . 0.17200803756713867 test_images/07653e58-24d1-4b3f-9b4a-76057efe5c09-6.jpg . 0.18814849853515625 test_images/0d0eddfc-731b-44de-b84d-d265afc7d996-2.jpg . 0.19216012954711914 test_images/image_10.jpg .",
            "url": "http://forgetfulcrow.com/detectron2/fasterrcnn/vision/2020/04/09/Handwritten-Text-Detection-in-Detectron2.html",
            "relUrl": "/detectron2/fasterrcnn/vision/2020/04/09/Handwritten-Text-Detection-in-Detectron2.html",
            "date": " • Apr 9, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Musing on the AI, Life and apparently Trivial matters. .",
          "url": "http://forgetfulcrow.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://forgetfulcrow.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}